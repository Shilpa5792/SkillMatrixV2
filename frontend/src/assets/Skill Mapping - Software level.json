[
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Reporting",
    "Tools": "Qlik",
    "L1": "* Navigate Qlik Sense apps and dashboards\n* Use basic visualizations: bar, pie, line charts\n* Apply filters and selections to explore data\n* Understand Qlik\u2019s associative data model\n* Load simple datasets using the Data Manager\n* Export charts and tables for reporting",
    "L2": "* Create interactive dashboards with multiple sheets\n* Use advanced visualizations: combo charts, KPI objects, maps\n* Build and manage data models using joins and concatenations\n* Write basic Qlik script for data transformation\n* Use set analysis for dynamic filtering\n* Schedule data reloads and manage app publishing",
    "L3": "* Design scalable and optimized dashboards for large datasets\n* Implement section access for data security\n* Use advanced set analysis and variables\n* Build custom extensions and visualizations using JavaScript\n* Automate report generation and distribution via NPrinting\n* Integrate Qlik dashboards into portals or third-party apps.",
    "hashId": "bf36c112e06d1268ceacfef23f5b8a4cac9de6f24ce291a9000b4df7c31f79c0"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Reporting",
    "Tools": "Power BI",
    "L1": "* Import data from Excel or CSV into Power BI\n* Create basic visuals: bar, line, pie charts\n* Use slicers and filters to explore data\n* Format visuals with titles, labels, and colors\n* Build simple dashboards with 2\u20133 visuals\n* Publish reports to Power BI Service",
    "L2": "* Create interactive dashboards with bookmarks and drill-through\n* Use advanced visuals: combo charts, KPI cards, maps\n* Build relationships between tables and manage data models\n* Write basic DAX formulas for calculated columns and measures\n* Schedule data refreshes and manage report access\n* Use themes and templates for consistent design",
    "L3": "* Optimize data models for performance and scalability\n* Implement row-level security and role-based access\n* Use advanced DAX for time intelligence and complex calculations\n* Create paginated reports for printable formats\n* Automate report distribution using Power Automate\n* Integrate Power BI with other tools (e.g., Teams, SharePoint, custom apps)",
    "hashId": "5a4d2f39d470d0490130d4b9f57dc5a836579b4d4831c6a7d2a3ccf37a712fb3"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Reporting",
    "Tools": "Tableau",
    "L1": "* Connect to simple data sources (Excel, CSV)\n* Create basic charts: bar, line, pie\n* Use filters and parameters to explore data\n* Format visuals with titles, labels, and colors\n* Build simple dashboards with basic interactivity\n* Publish to Tableau Public or Tableau Server",
    "L2": "* Create dashboards with multiple sheets and actions (filter, highlight)\n* Use advanced chart types: scatter plots, maps, bullet charts\n* Build relationships and joins between multiple data sources\n* Create calculated fields and use basic Level of Detail (LOD) expressions\n* Apply dashboard layout best practices for usability\n* Schedule data refreshes and manage workbook versions",
    "L3": "* Optimize dashboards for performance and scalability\n* Implement row-level security and user filters\n* Use advanced LOD expressions for complex aggregations\n* Build predictive visuals and statistical charts\n* Automate report distribution and embed dashboards in portals\n* Use Tableau JavaScript API for custom integrations",
    "hashId": "75d6e7c81c2859ff6014c669238fee1a0a153bf24b7fa793d363b9b27ef46939"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Reporting",
    "Tools": "Looker",
    "L1": "* Connect to simple data sources (Google Sheets, CSV)\n* Create basic charts: bar, line, pie\n* Use filters and date range controls\n* Format charts with titles, labels, and colors\n* Build simple dashboards with 2\u20133 visuals\n* Share reports via link or embed",
    "L2": "* Create dashboards with multiple pages and controls\n* Use advanced charts: scorecards, combo charts, tables with conditional formatting\n* Blend data from multiple sources\n* Create calculated fields and custom metrics\n* Use filters, controls, and drill-down features\n* Schedule report refreshes and manage access",
    "L3": "* Optimize dashboards for performance and scalability\n* Implement row-level security and user-based views\n* Use advanced calculated fields and regular expressions\n* Automate report generation and distribution\n* Build custom visualizations using community components or APIs\n* Integrate Looker dashboards into portals or third-party apps",
    "hashId": "4140632a2f2392ace0d4dfbedeca4eac9dea0169367ea29278587c4a34332904"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Reporting",
    "Tools": "Excel",
    "L1": "* Create basic charts: bar, line, pie\n* Use cell formatting and conditional formatting\n* Apply basic formulas (SUM, AVERAGE, COUNT)\n* Use filters and sorting in tables\n* Insert simple tables and visuals into reports\n* Save and share Excel files for reporting",
    "L2": "* Create Pivot Tables and Pivot Charts\n* Use slicers and timelines for interactivity\n* Apply advanced formulas (IF, VLOOKUP, INDEX-MATCH)\n* Clean and transform data using Power Query\n* Build dashboards with multiple visuals and dynamic ranges\n* Use conditional formatting for data-driven visuals",
    "L3": "* Create data models using Power Pivot\n* Write DAX formulas for calculated fields\n* Automate reporting tasks using Macros and VBA\n* Connect Excel to external data sources (SQL, Power BI, SharePoint)\n* Build scalable dashboards with dynamic controls\n* Optimize file performance and manage large datasets",
    "hashId": "c2b88a6a2531b80ce2ed88ef540492d176c8df54b854d9f58918eb330a7946fc"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Dashboarding",
    "Tools": "Power BI",
    "L1": "* Import data and create basic visuals (bar, line, pie)\n* Use slicers and filters for interactivity\n* Format visuals with titles, labels, and colors\n* Build simple dashboards with 2\u20133 visuals\n* Publish dashboards to Power BI Service\n* Share dashboards with stakeholders",
    "L2": "* Create dashboards with multiple pages and drill-through actions\n* Use advanced visuals: KPI cards, combo charts, maps\n* Build relationships between tables and manage data models\n* Write basic DAX formulas for calculated columns and measures\n* Apply bookmarks, tooltips, and dynamic titles\n* Schedule data refreshes and manage access permissions",
    "L3": "* Optimize dashboards for performance and large datasets\n* Implement row-level security and role-based access\n* Use advanced DAX for time intelligence and complex calculations\n* Create paginated reports for printable formats\n* Automate dashboard updates and report distribution\n* Embed dashboards into portals or third-party applications\n* Apply design best practices for executive-level storytelling",
    "hashId": "9262674e82e362b46bebe7a933635203f1766c4a020ae0e21e799a60b416d19f"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Dashboarding",
    "Tools": "Tableau",
    "L1": "* Connect to simple data sources (Excel, CSV)\n* Create basic charts: bar, line, pie\n* Use filters and parameters for interactivity\n* Format visuals with titles, labels, and colors\n* Build simple dashboards with 2\u20133 visuals\n* Publish dashboards to Tableau Public",
    "L2": "* Create dashboards with multiple sheets and dashboard actions (filter, highlight, URL)\n* Use advanced visuals: scatter plots, maps, bullet charts\n* Build relationships and joins between multiple data sources\n* Create calculated fields and use basic Level of Detail (LOD) expressions\n* Apply dashboard layout best practices for usability\n* Schedule data refreshes and manage workbook versions",
    "L3": "* Optimize dashboards for performance and scalability\n* Implement row-level security and user filters\n* Use advanced LOD expressions for complex aggregations\n* Build predictive visuals and statistical charts\n* Automate dashboard updates and embed dashboards in portals\n* Use Tableau JavaScript API for custom integrations\n* Apply design best practices for executive-level storytelling",
    "hashId": "24f6a21823b517dcdf3567c5ec69959d016cf09f0d6a9aa837cf669703a753aa"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Dashboarding",
    "Tools": "Looker",
    "L1": "* Connect to simple data sources (Google Sheets, CSV)\n* Create basic charts: bar, line, pie\n* Use filters and date range controls\n* Format charts with titles, labels, and colors\n* Build simple dashboards with 2\u20133 visuals\n* Share dashboards via link or embed",
    "L2": "* Create dashboards with multiple pages and controls\n* Use advanced charts: scorecards, combo charts, tables with conditional formatting\n* Blend data from multiple sources\n* Create calculated fields and custom metrics\n* Use filters, controls, and drill-down features\n* Schedule data refreshes and manage access",
    "L3": "* Optimize dashboards for performance and scalability\n* Implement row-level security and user-based views\n* Use advanced calculated fields and regular expressions\n* Automate dashboard updates and embed dashboards in portals\n* Build custom visualizations using community components or APIs\n* Apply design best practices for executive-level storytelling",
    "hashId": "f768437a620804c7f04994bdec23051c2c960187bb5f9a301330afda73e02b35"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Dashboarding",
    "Tools": "Google Sheets",
    "L1": "* Create basic charts: bar, line, pie\n* Use cell formatting and conditional formatting\n* Apply basic formulas (SUM, AVERAGE, COUNT)\n* Use filters and sorting in tables\n* Create simple dashboards with static visuals\n* Share dashboards via link or email",
    "L2": "* Create Pivot Tables and Pivot Charts\n* Use slicers and dropdowns for interactivity\n* Apply advanced formulas (IF, VLOOKUP, INDEX-MATCH)\n* Use conditional formatting for data-driven visuals\n* Build dashboards with multiple sheets and dynamic ranges\n* Link charts to dynamic data sources",
    "L3": "* Automate data updates using Google Apps Script\n* Connect Google Sheets to external data sources (APIs, Google Analytics, BigQuery)\n* Use QUERY function for advanced data manipulation\n* Build scalable dashboards with real-time data\n* Embed dashboards in websites or internal portals\n* Apply design best practices for executive-level storytelling",
    "hashId": "9dcf8ceb75d8b2de82198d8f0c6afac783317bcffdb234b58a2f93617403604e"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Dashboarding",
    "Tools": "Qlik Sense/Qlik view",
    "L1": "* Navigate Qlik dashboards and sheets\n* Use basic chart types: bar, pie, line\n* Apply filters and selections to explore data\n* Understand Qlik\u2019s associative data model\n* Load simple datasets using Data Manager (Qlik Sense)\n* Export visuals and tables for reporting",
    "L2": "* Create dashboards with multiple sheets and interactive objects\n* Use advanced visualizations: combo charts, KPI objects, maps\n* Build and manage data models using joins and concatenations\n* Write basic Qlik script for data transformation\n* Use set analysis for dynamic filtering\n* Schedule data reloads and manage app publishing",
    "L3": "* Design scalable dashboards for large datasets\n* Implement section access for data security\n* Use advanced set analysis and variables\n* Build custom extensions and visualizations using JavaScript\n* Automate dashboard distribution via NPrinting\n* Integrate dashboards into portals or third-party apps\n* Apply design best practices for executive-level storytelling",
    "hashId": "1f51645a7e23acc8112e2e590e95b76c2afc80fff5c05752cc07d5fa14bda949"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Charts and Graphs",
    "Tools": "D3.js",
    "L1": "* Understand the purpose and structure of D3.js\n* Set up a basic D3 environment (HTML + JS)\n* Create simple static charts (bar, line, pie)\n* Bind data to DOM elements using .data() and .enter()\n* Apply basic styling with CSS\n* Load data from CSV or JSON files",
    "L2": "* Create interactive charts with tooltips, hover effects, and transitions\n* Use scales (d3.scaleLinear, d3.scaleBand, etc.) and axes\n* Handle dynamic data updates and transitions\n* Build responsive visualizations that adapt to screen size\n* Use modular code and reusable chart components\n* Integrate D3 with external data sources (APIs, JSON)",
    "L3": "* Build complex visualizations (hierarchical, network, radial, force-directed graphs)\n* Optimize performance for large datasets\n* Implement zooming, panning, and brushing interactions\n* Integrate D3 with frontend frameworks (React + D3, Vue + D3)\n* Create custom reusable chart libraries\n* Apply advanced animation and storytelling techniques",
    "hashId": "aa2c9e9e719caaeb769f17de51481010b076d370f23e6e6c416c2deff5185b7c"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Charts and Graphs",
    "Tools": "MatPlotLib",
    "L1": "* Import and use matplotlib.pyplot\n* Create basic charts: line, bar, pie\n* Label axes and add titles\n* Customize colors and styles\n* Save plots as image files\n* Display plots inline in notebooks",
    "L2": "* Create multiple plots using subplot() and subplots()\n* Customize ticks, legends, and gridlines\n* Use data from Pandas DataFrames\n* Apply styles and themes (plt.style.use)\n* Annotate charts with text and arrows\n* Handle categorical and time series data",
    "L3": "* Create complex visualizations (stacked plots, histograms, scatter matrices)\n* Build animated plots using FuncAnimation\n* Use 3D plotting with Axes3D\n* Integrate Matplotlib with interactive tools (e.g., widgets in Jupyter)\n* Optimize plots for publication-quality visuals\n* Customize backends for rendering (e.g., Agg, TkAgg)",
    "hashId": "e8d3eb6d894e2f461d21b077f3f1f0b451ee110ca0b01a6a261050b787e1d0ea"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Data Story telling",
    "Tools": "Power BI bookmarks and Narratives",
    "L1": "* Create basic visuals (bar, line, pie) with clear titles and labels\n* Use slicers and filters to guide user exploration\n* Add text boxes and shapes to annotate dashboards\n* Understand the purpose of bookmarks and storytelling in dashboards\n* Share dashboards with basic narrative flow",
    "L2": "* Create bookmarks to capture specific views or filter states\n* Use buttons to navigate between bookmarks (e.g., \u201cNext\u201d, \u201cBack\u201d, \u201cDetails\u201d)\n* Organize visuals using the selection pane for layered storytelling\n* Use tooltips and drill-through pages to add depth to the story\n* Apply themes and layout consistency for visual clarity\n* Tailor dashboards for different audiences (e.g., executives vs analysts)",
    "L3": "* Build dynamic narratives using conditional formatting and DAX-driven visuals\n* Automate storytelling elements (e.g., dynamic headlines, alerts)\n* Create paginated reports for printable storytelling formats\n* Embed dashboards with guided navigation in portals or presentations\n* Implement row-level security for personalized storytelling\n* Apply storytelling frameworks (e.g., SCQA, Pyramid Principle) in dashboard design",
    "hashId": "feee5b8fa4c1e83541636f8f50eaee5b585edccb17e4f2f17109a0122758e4b0"
  },
  {
    "Category": "Analytics",
    "Sub-Category": "Visualization",
    "Sub-Sub-Category": "Data Story telling",
    "Tools": "D3.js",
    "L1": "* Understand the purpose of data storytelling and D3.js\n* Set up a basic D3 environment (HTML + JS)\n* Create simple static charts (bar, line, pie)\n* Bind data to DOM elements using .data() and .enter()\n* Add basic labels, titles, and tooltips\n* Load data from CSV or JSON files",
    "L2": "* Create interactive charts with hover effects, transitions, and tooltips\n* Use scales and axes to structure visual narratives\n* Build multi-step visual stories using scroll or click interactions\n* Annotate visuals with dynamic text and highlights\n* Integrate external datasets and APIs for real-time storytelling\n* Apply storytelling principles (e.g., context, contrast, clarity)",
    "L3": "* Build scroll-driven or animated data stories (scrollytelling)\n* Create complex visualizations (e.g., network graphs, timelines, radial plots)\n* Implement zooming, panning, and brushing for deep data exploration\n* Integrate D3 with frontend frameworks for modular storytelling\n* Optimize performance for large datasets and mobile responsiveness\n* Apply advanced storytelling frameworks (e.g., narrative arcs, tension/resolution)",
    "hashId": "2b8ac82bda175a8d9883e8ba94604d34dea9a2a3f60d2856b87913f1f3ae827c"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Database - MySQL",
    "L1": "* Understand relational database concepts (tables, rows, columns)\n* Connect to a MySQL database using GUI or CLI\n* Run basic SQL queries: SELECT, WHERE, ORDER BY, LIMIT\n* Export and import data using CSV files\n* Understand basic data types (INT, VARCHAR, DATE)\n* Perform simple filtering and sorting",
    "L2": "* Write complex SQL queries with JOIN, GROUP BY, HAVING, UNION\n* Create and manage views for reporting\n* Use stored procedures and functions for data transformation\n* Schedule data extraction jobs\n* Connect MySQL to BI tools (Power BI, Tableau, Looker)\n* Handle data quality issues (nulls, duplicates, type mismatches)",
    "L3": "* Design normalized and denormalized schemas for analytics\n* Optimize queries using indexing and execution plans\n* Implement data security (user roles, access control)\n* Automate data pipelines using orchestration tools\n* Integrate MySQL with real-time data sources and APIs\n* Monitor and troubleshoot performance issues",
    "hashId": "ed37a0b313742fc6dbdbf24d9cfdf6a53438e71d6de1bee2b909217327a3f379"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Database - Cloud SQL",
    "L1": "* Understand what Cloud SQL is and its role in data engineering\n* Connect to Cloud SQL using GUI tools or command line\n* Run basic SQL queries (SELECT, WHERE, ORDER BY)\n* Import/export data using CSV files\n* Understand basic database structure (tables, schemas, data types)\n* Set up basic Cloud SQL instance and user access",
    "L2": "* Write complex SQL queries (JOIN, GROUP BY, UNION)\n* Connect Cloud SQL to BI tools (Power BI, Tableau, Looker)\n* Use Python/R to extract and transform data from Cloud SQL\n* Schedule data ingestion jobs using Cloud Scheduler or workflows\n* Manage access control using IAM roles\n* Monitor query performance and optimize data retrieval",
    "L3": "* Design normalized and denormalized schemas for analytics\n* Automate data pipelines using orchestration tools (e.g., Airflow, Cloud Composer)\n* Implement encryption, backups, and failover strategies\n* Optimize queries and indexing for performance\n* Integrate Cloud SQL with real-time data sources and APIs\n* Apply security best practices (SSL, private IP, IAM policies)",
    "hashId": "31f5d29090bed012f84ee9a90b5099f155fc29ce4149079c47353bd808343d50"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Database - Oracle Database",
    "L1": "* Understand Oracle database structure (schemas, tables, columns)\n* Connect to Oracle DB using GUI tools\n* Run basic SQL queries: SELECT, WHERE, ORDER BY, LIMIT\n* Export/import data using CSV or SQL scripts\n* Understand basic data types (NUMBER, VARCHAR2, DATE)\n* Use DESCRIBE and SELECT * for data exploration",
    "L2": "* Write complex SQL queries (JOIN, GROUP BY, HAVING, UNION)\n* Create and manage views, indexes, and sequences\n* Use PL/SQL for stored procedures and functions\n* Schedule and monitor data extraction jobs\n* Integrate Oracle with BI tools (Power BI, Tableau, Looker)\n* Handle data quality issues (nulls, duplicates, type mismatches)",
    "L3": "* Design normalized and denormalized schemas for analytics\n* Optimize queries using indexing, partitions, and execution plans\n* Implement data security (roles, privileges, VPD)\n* Automate data pipelines using PL/SQL and orchestration tools\n* Integrate Oracle with real-time data sources and APIs\n* Monitor performance using AWR, ASH, and Oracle Enterprise Manager",
    "hashId": "32a457858fab92b807481afeb0f0f222c8b0dcbaaa0004ead61a216c996c535b"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Database - Maria DB",
    "L1": "* Understand relational database concepts (tables, rows, columns)\n* Connect to MariaDB using GUI or CLI\n* Run basic SQL queries: SELECT, WHERE, ORDER BY, LIMIT\n* Import/export data using CSV or SQL dump\n* Understand basic data types (INT, VARCHAR, DATE)\n* Perform simple filtering and sorting",
    "L2": "* Write complex SQL queries (JOIN, GROUP BY, HAVING, UNION)\n* Create and manage views, indexes, and stored procedures\n* Use triggers for basic automation\n* Connect MariaDB to BI tools (Power BI, Tableau, Looker)\n* Schedule data extraction jobs\n* Handle data quality issues (nulls, duplicates, type mismatches)",
    "L3": "* Design normalized and denormalized schemas for analytics\n* Optimize queries using indexing, partitioning, and EXPLAIN plans\n* Implement user roles, privileges, and SSL-based security\n* Automate data pipelines using orchestration tools\n* Integrate MariaDB with real-time data sources and APIs\n* Monitor performance and manage replication or clustering",
    "hashId": "ec2494d0f9ee2d0511b25509b0b8c1ca08d2fa62c845901329e99847265d906c"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Database -SQLite",
    "L1": "* Understand SQLite as a file-based relational database\n* Create and connect to .sqlite or .db files\n* Run basic SQL queries: SELECT, WHERE, ORDER BY, LIMIT\n* Create simple tables and insert data\n* Export/import data using CSV\n* Use SQLite in local development environments",
    "L2": "* Write complex SQL queries (JOIN, GROUP BY, HAVING, UNION)\n* Use SQLite with Pandas or R for data analysis\n* Create and manage indexes, views, and triggers\n* Integrate SQLite with lightweight applications or dashboards\n* Handle data quality issues (nulls, duplicates, type mismatches)\n* Automate data extraction and transformation scripts",
    "L3": "* Design normalized schemas for embedded or mobile applications\n* Use SQLite extensions for full-text search, JSON, or geospatial data\n* Optimize performance with indexing and query tuning\n* Implement data encryption and access control (e.g., SQLCipher)\n* Sync SQLite with cloud or remote databases\n* Embed SQLite in production-grade applications (mobile, IoT, desktop)",
    "hashId": "82a330bc3f2e29a7b4aa0b4f1b4647a6d49d383f10290cdd42c77e5ae4296acf"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "ERP Systems - Oracle financials",
    "L1": "* Understand Oracle Financials modules (GL, AP, AR, PO, etc.)\n* Navigate standard reports and dashboards\n* Export data from Oracle reports to Excel/CSV\n* Run basic SQL queries on ERP views (read-only)\n* Understand ERP table structures and naming conventions\n* Identify key financial data entities (e.g., invoices, journals, ledgers)",
    "L2": "* Write SQL queries to extract data from ERP base tables and views\n* Use BI Publisher to create custom reports and data models\n* Integrate Oracle ERP data into data warehouses or BI tools\n* Schedule and automate data extraction jobs\n* Handle joins across modules (e.g., GL to AP, PO to Inventory)\n* Understand data security and access roles in Oracle ERP",
    "L3": "* Design scalable data pipelines from Oracle ERP to cloud data platforms\n* Use Oracle APIs and FBDI (File-Based Data Import) for data exchange\n* Implement real-time data replication using GoldenGate\n* Apply data governance and lineage tracking for ERP data\n* Automate data workflows using orchestration tools\n* Optimize performance of ERP data extraction and transformation",
    "hashId": "f1be77b1ca2239ee111d9d28671a10be72684575473b2432e5c034751934528e"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "ERP Systems - SAP",
    "L1": "* Understand SAP modules (FI, CO, MM, SD, HR, etc.)\n* Navigate SAP GUI to access standard reports and tables\n* Export data from SAP to Excel or CSV\n* Use transaction codes (e.g., SE16N, SQ01, SQVI) to view data\n* Identify key SAP tables and fields (e.g., BKPF, BSEG, MARA)\n* Understand basic SAP data structures and relationships",
    "L2": "* Extract data from SAP tables using custom queries or views\n* Use SAP BW InfoProviders or CDS Views for reporting\n* Integrate SAP data into data lakes or warehouses\n* Schedule and automate data extraction jobs\n* Handle SAP-specific data types and formats (e.g., ALPHA conversion)\n* Understand SAP authorization concepts for data access",
    "L3": "* Design scalable pipelines from SAP to cloud platforms (e.g., BigQuery, Snowflake)\n* Use SAP OData/BAPI services for real-time data integration\n* Implement data governance and lineage for SAP data\n* Optimize performance of SAP data extraction (e.g., delta loads, parallel processing)\n* Automate workflows using orchestration tools (e.g., Airflow, SAP CPI)\n* Secure and audit SAP data pipelines with compliance standards",
    "hashId": "f752a699f2c0ad6d8ead2e5125c2c421dda083dd3705c3db7616251515535aa0"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "ERP Systems - Microsoft ERP",
    "L1": "* Understand core Dynamics 365 modules (Finance, SCM, Sales, HR)\n* Navigate standard reports and dashboards\n* Export data from Dynamics to Excel or CSV\n* Use filters and views to explore transactional data\n* Identify key entities (e.g., customers, vendors, invoices, journals)",
    "L2": "* Use OData endpoints to extract data from Dynamics 365\n* Connect Dynamics data to Power BI or Excel using Power Query\n* Schedule data refreshes and automate data pulls using Power Automate\n* Understand and use standard and custom data entities\n* Perform joins and transformations using Power Query or SQL\n* Handle data quality and mapping issues across modules",
    "L3": "* Design scalable pipelines from Dynamics 365 to cloud data platforms\n* Use APIs and Data Export Service for near real-time integration\n* Implement data governance, lineage, and security policies\n* Optimize performance of data extraction and transformation\n* Automate workflows across ERP, CRM, and external systems\n* Integrate Dynamics data with enterprise data lakes and warehouses",
    "hashId": "352ffe5831deb9616a342d3b083b1c94d9c44984f8b3fd423ad72b2064abe44d"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "ERP Systems - NetSuite ERP",
    "L1": "* Navigate NetSuite modules (Finance, CRM, Inventory, etc.)\n* Run and export standard reports and saved searches\n* Understand key NetSuite record types (e.g., transactions, customers, vendors)\n* Use filters and criteria in saved searches\n* Export data to Excel or CSV for analysis\n* Understand NetSuite\u2019s role in the data ecosystem",
    "L2": "* Create custom saved searches and summary reports\n* Use SuiteAnalytics Workbooks for data exploration\n* Connect NetSuite to external tools using ODBC/JDBC\n* Schedule and automate data exports\n* Integrate NetSuite data into data warehouses or BI tools\n* Handle data joins across modules (e.g., Sales Orders to Invoices)",
    "L3": "* Build custom data extraction scripts using SuiteScript\n* Use SuiteTalk APIs for real-time or batch data integration\n* Automate data pipelines from NetSuite to cloud platforms (e.g., Snowflake, BigQuery)\n* Implement data governance and security (role-based access, audit trails)\n* Optimize performance of saved searches and API calls\n* Monitor and troubleshoot integration workflows",
    "hashId": "2dafcdb105e2339bf2c97088f68b45a09e56518dad5fb1dc925f49f5c183376b"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "ERP Systems - Tally ERP",
    "L1": "* Understands basic Tally ERP modules (Accounting, Inventory).\n* Can manually export data (Excel, PDF).\n* Performs basic data cleaning using Excel.\n* Knows simple reporting techniques.\n* Aware of basic data privacy principles.\n* Follows instructions and documents basic steps.",
    "L2": "* Understands data relationships within Tally (e.g., ledgers, vouchers).\n* Uses ODBC/XML interfaces and connector tools for data extraction.\n* Applies ETL tools for structured data transformation.\n* Loads data into staging tables in data warehouses.\n* Builds dashboards using BI tools (Power BI, Looker Studio).\n* Implements role-based access and collaborates with teams.",
    "L3": "* Deep knowledge of Tally\u2019s internal schema and customization.\n* Automates data extraction using APIs and scripting (e.g., Python).\n* Designs scalable data pipelines with error handling.\n* Maintains full ETL workflows into enterprise data warehouses.\n* Creates dynamic, automated reporting systems.\n* Ensures compliance with security standards and leads data initiatives.",
    "hashId": "bb129c10976d32eb0de4961baede449d529dc2e0ec7a5ae19f0d5abdd51e5a11"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Unstructured Data - Microsoft Documents",
    "L1": "* Understands basic types of unstructured data (Word, Excel, PDFs).\n* Can manually locate and open documents for data review.\n* Performs basic copy-paste or manual extraction of data.\n* Uses simple Excel functions for organizing extracted data.\n* Aware of file formats and basic storage practices.",
    "L2": "* Identifies relevant data within unstructured documents using search and filters.\n* Uses tools like Power Query, Python (pandas), or Excel macros for semi-automated extraction.\n* Cleans and structures data for analysis (e.g., tabular format).\n* Understands metadata and document properties.\n* Collaborates with teams to standardize document formats and naming conventions.",
    "L3": "* Automates data extraction using NLP or document parsing libraries (e.g., Python with docx, pdfplumber, openpyxl).\n* Designs workflows to convert unstructured data into structured formats for databases or dashboards.\n* Implements document classification and tagging systems.\n* Ensures data quality, version control, and compliance in document handling.\n* Leads initiatives to integrate unstructured data into enterprise data platforms.",
    "hashId": "0a00a9df0e5386c84e36fccd9c97f761504553e974a1cfab5ef79ea00f5adf1e"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Unstructured Data - Image Data",
    "L1": "* Recognizes basic image file formats (JPEG, PNG, TIFF).\n* Manually reviews images for visible data (e.g., scanned invoices, receipts).\n* Uses basic tools (e.g., Snipping Tool, Paint) to extract or annotate data.\n* Understands limitations of image data in analysis.\n* Aware of basic image storage and organization practices.",
    "L2": "* Uses OCR tools (e.g., Tesseract, Adobe Acrobat) to extract text from images.\n* Applies image pre-processing techniques (e.g., resizing, contrast adjustment).\n* Cleans and structures extracted data for use in spreadsheets or databases.\n* Understands image metadata (e.g., resolution, timestamps).\n* Collaborates with teams to standardize image formats and naming conventions.",
    "L3": "* Automates image data extraction using advanced OCR and computer vision libraries (e.g., OpenCV, EasyOCR).\n* Designs pipelines to process large volumes of image data efficiently.\n* Integrates image data into structured databases or analytics platforms.\n* Implements quality checks, version control, and compliance for image data.\n* Leads initiatives involving image classification, tagging, and AI-based recognition.",
    "hashId": "d1cec76473f6a219a6009814440f9b29ccf337f70c7df2d5611bbc5801e891f5"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Unstructured Data - Azure Blob Storage",
    "L1": "* Understands what Azure Blob Storage is and its role in storing unstructured data.\n* Can manually upload/download files using Azure Portal.\n* Recognizes basic blob types (Block, Append, Page).\n* Aware of file formats commonly stored (e.g., images, documents, logs).\n* Follows basic naming and folder conventions.",
    "L2": "* Uses Azure Storage Explorer or CLI to manage blobs.\n* Applies filters and metadata to organize and retrieve blobs.\n* Extracts data from blobs using scripts or tools (e.g., Python with azure-storage-blob).\n* Understands access tiers and lifecycle management.\n* Collaborates with teams to define blob structure and access policies.",
    "L3": "* Automates blob data ingestion into data pipelines using Azure Functions, Logic Apps, or Data Factory.\n* Implements secure access using SAS tokens, RBAC, and encryption.\n* Designs scalable workflows for processing large volumes of unstructured blob data.\n* Integrates blob data with analytics platforms (e.g., Synapse, Databricks).\n* Leads governance, compliance, and optimization strategies for blob storage usage.",
    "hashId": "3e029a80a7ff65fe27edb0a92d8c8123097dc29b35697dbb0b1ef499aa276bd5"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "Unstructured Data -  PDF",
    "L1": "* Understands what PDF files are and their common use cases (e.g., invoices, reports).\n* Can manually open and review PDF documents.\n* Performs basic copy-paste operations to extract data.\n* Uses simple tools like Adobe Reader or browser-based viewers.\n* Aware of file organization and naming conventions.",
    "L2": "* Uses PDF extraction tools (e.g., Adobe Acrobat, Tabula, Power BI PDF connector).\n* Applies semi-automated methods to extract tabular or textual data.\n* Cleans and structures extracted data using Excel or scripting tools.\n* Understands PDF metadata and document properties.\n* Collaborates with teams to standardize PDF formats for easier processing.",
    "L3": "* Automates PDF data extraction using libraries like pdfplumber, PyMuPDF, or PDFMiner.\n* Designs workflows to process large volumes of PDFs efficiently.\n* Integrates extracted data into structured databases or analytics platforms.\n* Implements document classification, tagging, and version control.\n* Ensures compliance with data governance and leads initiatives for intelligent document processing (IDP).",
    "hashId": "6a93699e4b88772e898988e42d2d9e8dcff934cdf837b2513194ba772a64230d"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "API PostMan -  Postman",
    "L1": "* Understands what APIs are and their role in data exchange.\n* Can use Postman to send basic GET requests.\n* Reads simple JSON responses and identifies key-value pairs.\n* Manually copies response data for analysis.\n* Aware of basic API documentation and endpoints.",
    "L2": "* Uses Postman to send various request types (GET, POST, PUT, DELETE).\n* Adds headers, query parameters, and body payloads to requests.\n* Extracts and saves response data using Postman collections and environments.\n* Understands authentication methods (e.g., API keys, Bearer tokens).\n* Collaborates with developers to test and refine API endpoints.",
    "L3": "* Automates API workflows using Postman scripts (Pre-request & Tests).\n* Integrates Postman with CI/CD tools or data pipelines.\n* Handles complex authentication (OAuth 2.0, JWT).\n* Parses and transforms API responses for direct integration into databases or dashboards.\n* Leads API documentation reviews and contributes to API design for data engineering needs.",
    "hashId": "0fb883ed6985b2a401645453ea0d9ccc8a3639882009381cb91da34cef612e5f"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "API PostMan -  Swagger",
    "L1": "* Understands basic concepts of APIs and RESTful services.\n* Can use Postman to send simple GET requests and view responses.\n* Opens Swagger UI to explore available endpoints.\n* Reads basic JSON responses and identifies key fields.\n* Aware of API documentation and its purpose.",
    "L2": "* Uses Postman to send various request types (GET, POST, PUT, DELETE) with headers and payloads.\n* Navigates Swagger UI to test endpoints and understand parameters.\n* Extracts and saves response data for analysis.\n* Understands authentication methods (API keys, Bearer tokens).\n* Collaborates with developers to troubleshoot and refine API usage.",
    "L3": "* Automates API workflows using Postman scripting and environments.\n* Integrates Swagger-generated client code into data pipelines.\n* Handles complex authentication (OAuth 2.0, JWT) and error handling.\n* Parses and transforms API responses for direct integration into databases or dashboards.\n* Leads API integration strategies and contributes to API design and documentation.",
    "hashId": "5b43f3520e5751eb8dc9e81d4007c5d68c5c466f4b35833a6cc6388a78751cdc"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "API PostMan -  GIThub/ Jenkins",
    "L1": "* Understands basic API concepts and how Postman is used for testing.\n* Can send simple requests (GET/POST) and view responses in Postman.\n* Aware of GitHub as a code repository and Jenkins as an automation tool.\n* Manually saves and shares Postman collections.\n* Understands basic version control principles.",
    "L2": "* Uses Postman to test authenticated APIs and manage environments.\n* Stores and versions Postman collections in GitHub.\n* Triggers Jenkins jobs manually for testing or deployment.\n* Collaborates with developers to integrate API testing into workflows.\n* Understands CI/CD concepts and how Postman fits into automated pipelines.",
    "L3": "* Automates API testing using Postman scripts and integrates with Jenkins pipelines.\n* Uses GitHub Actions or Jenkins to run Postman tests on code commits or pull requests.\n* Implements dynamic environments and data-driven testing in Postman.\n* Designs CI/CD workflows that include API validation and monitoring.\n* Leads DevOps integration for data engineering pipelines involving APIs.",
    "hashId": "8220cc83d51998c497114d446e554e152750e1363f50c8c6678b05eddff6d7b0"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "DataSources",
    "Tools": "API PostMan -  AWS API Gateway / Azure API Management / Apigee",
    "L1": "* Understands basic API concepts and the role of API gateways.\n* Can send simple requests using Postman to public endpoints.\n* Aware of what AWS API Gateway, Azure API Management, and Apigee are.\n* Reads basic JSON responses and identifies key fields.\n* Understands basic API documentation and endpoint structure.",
    "L2": "* Uses Postman to test authenticated APIs deployed on AWS, Azure, or Apigee.\n* Understands how to configure headers, tokens, and parameters for gateway-managed APIs.\n* Navigates Swagger/OpenAPI documentation linked to gateway endpoints.\n* Collaborates with developers to troubleshoot gateway-related issues.\n* Understands rate limiting, throttling, and basic security policies.",
    "L3": "* Automates API testing and integrates Postman with CI/CD pipelines for gateway-managed APIs.\n* Designs and manages APIs using AWS API Gateway, Azure API Management, or Apigee.\n* Implements advanced security (OAuth 2.0, JWT, API keys) and traffic control policies.\n* Monitors API performance and logs using gateway analytics tools.\n* Leads API lifecycle management and contributes to enterprise API strategy.",
    "hashId": "101f64ad50bee31db74c18405d7c627203c2d004c06ae124816494f325f56653"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "ETL",
    "Tools": "Talend",
    "L1": "* Understands basic ETL concepts (Extract, Transform, Load).\n* Can navigate Talend Studio interface and open sample jobs.\n* Uses built-in components for simple data extraction (e.g., CSV, Excel).\n* Performs basic transformations like filtering and mapping.\n* Loads data into flat files or basic databases (e.g., MySQL).",
    "L2": "* Designs ETL workflows using Talend components for multiple data sources.\n* Connects to structured and semi-structured sources (e.g., APIs, XML, JSON).\n* Implements error handling and logging in ETL jobs.\n* Schedules jobs using Talend scheduler or external tools.\n* Collaborates with teams to optimize data flow and performance.",
    "L3": "* Develops complex, scalable ETL pipelines integrating cloud and on-premise systems.\n* Uses Talend for real-time data processing and big data integration (e.g., Hadoop, Spark).\n* Implements advanced transformations, joins, and lookups.\n* Integrates Talend with CI/CD tools (e.g., Jenkins, GitHub).\n* Leads data governance, metadata management, and performance tuning initiatives.",
    "hashId": "15ac4675555a097fd64dfc11ccb27d5d36ecc267cce9bd5f7c6048b2e2e0b3ba"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "ETL",
    "Tools": "Informatica Power Centre",
    "L1": "* Understands basic ETL concepts and the role of Informatica PowerCenter.\n* Can navigate the PowerCenter Designer and Workflow Manager interfaces.\n* Creates simple mappings for data extraction from flat files or basic databases.\n* Performs basic transformations like filtering and sorting.\n* Loads data into target tables with minimal logic.",
    "L2": "* Designs ETL workflows with multiple sources and targets.\n* Uses transformation components like Joiner, Lookup, Aggregator, and Expression.\n* Implements error handling, logging, and reusable mappings.\n* Schedules and monitors workflows using Workflow Monitor.\n* Collaborates with DBAs and analysts to optimize performance and data quality.",
    "L3": "* Develops complex ETL pipelines integrating enterprise systems and cloud platforms.\n* Implements parameterization, dynamic mappings, and advanced performance tuning.\n* Integrates PowerCenter with version control systems and CI/CD tools.\n* Designs metadata-driven frameworks and reusable components.\n* Leads data governance, lineage tracking, and compliance initiatives.",
    "hashId": "2b750f0238803d3392a7fd642e00af58d63146bd9d60dff1007d462ca832c645"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "ETL",
    "Tools": "Apache NiFi",
    "L1": "* Understands basic ETL concepts and Apache NiFi\u2019s role in data flow automation.\n* Can navigate the NiFi UI and create simple data flows using drag-and-drop processors.\n* Uses basic processors like GetFile, PutFile, and LogAttribute.\n* Understands flowfile structure and basic configuration settings.\n* Aware of NiFi\u2019s capabilities for handling batch data.",
    "L2": "* Designs data flows using multiple processors for ingestion, transformation, and routing.\n* Connects to external sources (e.g., APIs, databases, cloud storage).\n* Implements flowfile attributes, custom properties, and processor relationships.\n* Uses NiFi templates and version control for flow management.\n* Monitors and troubleshoots data flows using provenance and bulletin boards.",
    "L3": "* Builds scalable, fault-tolerant data pipelines with dynamic routing and prioritization.\n* Integrates NiFi with other tools (Kafka, Spark, Hive, cloud services).\n* Implements custom processors and scripting (e.g., ExecuteScript with Python/Groovy).\n* Manages cluster deployments and configures high availability.\n* Leads data governance, security (SSL, access policies), and performance optimization.",
    "hashId": "81ee149e326faf4f64a77f62dde9ba79d14db3d53efec5c33171850c9f4fe73c"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "ETL",
    "Tools": "Pentaho Data Integration (kettle)",
    "L1": "* Understands basic ETL concepts and the role of Pentaho (Kettle).\n* Can navigate the Spoon UI and create simple transformations.\n* Uses basic input/output steps (e.g., CSV, Excel, database).\n* Performs simple data filtering and mapping.\n* Loads data into flat files or basic databases.",
    "L2": "* Designs ETL workflows with multiple steps and conditional logic.\n* Connects to various data sources (e.g., APIs, cloud storage, relational databases).\n* Implements error handling, logging, and reusable transformations.\n* Schedules jobs using the Pentaho scheduler or external tools.\n* Collaborates with teams to optimize performance and data quality.",
    "L3": "* Builds complex, scalable ETL pipelines integrating cloud and big data platforms.\n* Uses scripting (JavaScript, SQL) within transformations for advanced logic.\n* Integrates Pentaho with CI/CD tools and version control systems.\n* Implements metadata-driven ETL frameworks and dynamic job execution.\n* Leads data governance, lineage tracking, and performance tuning initiatives.",
    "hashId": "42fab15f5b2f7cceacc1f21d256eea7f330c957ab1ea3cf0e275379b29f8793a"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Collection Acquisition",
    "Sub-Sub-Category": "ETL",
    "Tools": "Azure Data Factory",
    "L1": "* Understands basic ETL concepts and Azure Data Factory\u2019s role in cloud-based data integration.\n* Can navigate the ADF UI and create simple pipelines using the Copy Data tool.\n* Connects to basic data sources (e.g., Azure Blob Storage, SQL Database).\n* Uses default settings for data movement and transformation.\n* Aware of pipeline triggers and basic monitoring features.",
    "L2": "* Designs multi-step pipelines with activities like Lookup, Filter, and ForEach.\n* Connects to diverse data sources (on-prem, cloud, REST APIs).\n* Implements data transformations using Mapping Data Flows.\n* Configures pipeline parameters, variables, and expressions.\n* Monitors pipeline performance and handles errors using retry policies and alerts.",
    "L3": "* Builds scalable, dynamic pipelines with reusable components and modular design.\n* Integrates ADF with other Azure services (e.g., Synapse, Databricks, Key Vault).\n* Implements CI/CD using Git integration and Azure DevOps.\n* Optimizes performance with partitioning, staging, and parallelism.\n* Leads enterprise-level data orchestration strategies and ensures compliance and governance.",
    "hashId": "a63139c25be9ab7dc01a87f0b56a0368b2be2b38ee741d6aa3722f32b3d587fb"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "Databricks",
    "L1": "* Understands basic concepts of data lakes and Databricks.\n* Can navigate the Databricks workspace and run simple notebooks.\n* Reads and writes data to/from cloud storage (e.g., Azure Data Lake, AWS S3).\n* Uses basic Spark commands for data exploration.\n* Aware of notebook structure and cluster setup.",
    "L2": "* Builds data pipelines using PySpark or SQL in Databricks notebooks.\n* Connects to various data sources (e.g., APIs, databases, files).\n* Implements data transformations and joins using Spark DataFrames.\n* Uses Delta Lake for versioned and reliable data storage.\n* Collaborates with teams to schedule jobs and monitor performance.",
    "L3": "* Designs scalable, production-grade data pipelines using Databricks workflows.\n* Optimizes Spark jobs for performance and cost-efficiency.\n* Integrates Databricks with CI/CD tools and enterprise data platforms.\n* Implements advanced features like streaming, ML model deployment, and data governance.\n* Leads architecture design for lakehouse implementations and cross-functional data initiatives.",
    "hashId": "b27556882a265a7cf70bee66fb082441c5f9787eeb10a76fcc2d370eb3c7e455"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "Azure Data Factory",
    "L1": "* Understands the concept of data lakes and Azure Data Factory\u2019s role in cloud-based data movement.\n* Can create simple pipelines to move data into Azure Data Lake Storage (ADLS).\n* Uses the Copy Data tool to ingest files (e.g., CSV, JSON) into ADLS.\n* Aware of basic pipeline triggers and monitoring features.\n* Understands folder structure and naming conventions in ADLS.",
    "L2": "* Designs pipelines to ingest data from multiple sources (e.g., databases, APIs) into ADLS.\n* Implements Mapping Data Flows for transformations before storing in the lake.\n* Uses parameters, variables, and expressions to make pipelines dynamic.\n* Applies data partitioning and organizes data in hierarchical folder structures.\n* Collaborates with teams to manage access and optimize performance.",
    "L3": "* Builds scalable, metadata-driven pipelines for enterprise-grade lakehouse architectures.\n* Integrates ADF with other Azure services (e.g., Synapse, Databricks, Key Vault).\n* Implements CI/CD using Git integration and Azure DevOps.\n* Applies advanced security (RBAC, managed identities, encryption) and governance practices.\n* Leads data lake strategy, including lifecycle management, cost optimization, and compliance.",
    "hashId": "bcb3411881d1684731fcc07c28bca6ba3aae2cd9629820c38fd3e0806cba7549"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "GCP DataFlow",
    "L1": "* Understands basic concepts of data lakes and GCP Dataflow.\n* Can navigate the GCP Console and launch simple Dataflow jobs using templates.\n* Uses basic connectors to ingest data from sources like Cloud Storage or Pub/Sub.\n* Aware of pipeline structure and basic monitoring tools.\n* Understands the role of Apache Beam in Dataflow.",
    "L2": "* Designs custom pipelines using Apache Beam (Python/Java) for batch and streaming data.\n* Connects to various data sources (e.g., BigQuery, Cloud Storage, APIs).\n* Implements transformations, windowing, and filtering logic.\n* Monitors job performance and handles errors using retry strategies.\n* Collaborates with teams to optimize pipeline design and resource usage.",
    "L3": "* Builds scalable, production-grade pipelines with dynamic configurations and parameterization.\n* Integrates Dataflow with other GCP services (e.g., Dataform, Vertex AI, Dataproc).\n* Implements CI/CD workflows for pipeline deployment and versioning.\n* Applies advanced performance tuning, autoscaling, and cost optimization.\n* Leads data lake architecture strategy, governance, and real-time analytics initiatives.",
    "hashId": "2c4055419783b7b257c63af4dbdcef78dce9bfaca9044cbaa99db930ff39083f"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "Data Factory",
    "L1": "* Understands the concept of data lakes and Azure Data Factory\u2019s role in data movement.\n* Can create basic pipelines to ingest data into Azure Data Lake Storage (ADLS).\n* Uses the Copy Data tool for simple file transfers (e.g., CSV, JSON).\n* Aware of pipeline triggers and basic monitoring features.\n* Understands folder structures and naming conventions in ADLS",
    "L2": "* Designs pipelines to ingest and transform data from multiple sources into ADLS.\n* Uses Mapping Data Flows for data transformation before lake storage.\n* Implements dynamic pipelines using parameters, variables, and expressions.\n* Applies partitioning and organizes data in hierarchical folder structures.\n* Collaborates with teams to manage access, optimize performance, and ensure data quality.",
    "L3": "* Builds scalable, metadata-driven pipelines for enterprise-grade lakehouse architectures.\n* Integrates ADF with other Azure services (e.g., Synapse, Databricks, Key Vault).\n* Implements CI/CD workflows using Git integration and Azure DevOps.\n* Applies advanced security (RBAC, managed identities, encryption) and governance practices.\n* Leads data lake strategy including lifecycle management, cost optimization, and compliance.",
    "hashId": "74298f118a215bc3725bd5ae9bc7b2bedf013254cde587866950641d812370dc"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "Amazon Glue",
    "L1": "* Understands the concept of data lakes and AWS Glue\u2019s role in ETL and data cataloging.\n* Can navigate the AWS Glue Console and run basic jobs using built-in crawlers.\n* Uses Glue to ingest data from simple sources (e.g., S3 buckets).\n* Aware of Glue Data Catalog and its purpose in organizing metadata.\n* Understands basic job configurations and scheduling.",
    "L2": "* Designs ETL jobs using Glue Studio or PySpark scripts.\n* Connects to multiple data sources (e.g., RDS, Redshift, S3, APIs).\n* Implements transformations using Spark DataFrames and Glue DynamicFrames.\n* Manages Glue crawlers for schema discovery and updates.\n* Collaborates with teams to optimize job performance and data organization.",
    "L3": "* Builds scalable, serverless ETL pipelines using Glue workflows and triggers.\n* Integrates Glue with other AWS services (e.g., Athena, Lake Formation, Step Functions).\n* Implements advanced data governance, partitioning, and schema evolution strategies.\n* Optimizes Spark jobs for performance and cost-efficiency.\n* Leads enterprise data lake architecture and automation initiatives using Glue.",
    "hashId": "445955c8c2cbd035236ca7b9158182bfffc92332a20a5a317e94f2ea6de0c9ed"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "Delta Lake",
    "L1": "* Understands the concept of Delta Lake and its role in enhancing data lakes.\n* Can read and write Delta tables using basic Spark or Databricks commands.\n* Aware of features like ACID transactions and schema enforcement.\n* Works with small datasets stored in Delta format.\n* Understands basic file organization and versioning.",
    "L2": "* Uses Delta Lake for structured data storage and incremental data loads.\n* Implements schema evolution and time travel for data recovery.\n* Optimizes Delta tables using partitioning and Z-ordering.\n* Integrates Delta Lake with BI tools and other data platforms.\n* Collaborates with teams to manage data quality and performance.",
    "L3": "* Designs enterprise-grade lakehouse architectures using Delta Lake.\n* Implements complex data pipelines with streaming and batch processing.\n* Applies advanced features like Change Data Capture (CDC) and data compaction.\n* Integrates Delta Lake with ML workflows and governance frameworks.\n* Leads initiatives for scalable, secure, and compliant data lake operations.",
    "hashId": "97b59c73209f3f6a9ee8d435bae82449bf33286e8e55ddfc57fb13e900bfe0ac"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Lakes",
    "Tools": "Talend",
    "L1": "* Understands the concept of data lakes and Talend\u2019s role in ETL and data integration.\n* Can use Talend Studio to create simple jobs that move data into data lakes (e.g., HDFS, S3, ADLS).\n* Uses basic Talend components for file ingestion (CSV, Excel) into lake storage.\n* Aware of Talend\u2019s drag-and-drop interface and job execution flow.\n* Understands basic folder structure and naming conventions in data lakes.",
    "L2": "* Designs Talend jobs to ingest and transform data from multiple sources into data lakes.\n* Connects to cloud storage platforms (e.g., AWS S3, Azure Data Lake, GCP Cloud Storage).\n* Implements data cleansing, mapping, and enrichment before lake storage.\n* Uses Talend metadata and context variables for dynamic job configuration.\n* Collaborates with teams to manage data quality and optimize performance.",
    "L3": "* Builds scalable, reusable ETL frameworks for enterprise-grade lakehouse architectures.\n* Integrates Talend with big data platforms (e.g., Hadoop, Spark) and cloud services.\n* Implements advanced features like partitioning, schema evolution, and incremental loads.\n* Applies CI/CD practices using Git, Jenkins, and Talend Management Console.\n* Leads data governance, lineage tracking, and compliance initiatives for lake environments.",
    "hashId": "0432319f3ed9b57e06cc61dc0f1a02ddae2da01b5052ae8f6deee6c57eb23878"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Warehouse",
    "Tools": "SnowFlake",
    "L1": "* Understands basic data warehousing concepts and Snowflake\u2019s architecture.\n* Can connect to Snowflake and run simple SQL queries.\n* Loads small datasets into Snowflake using UI or basic scripts.\n* Aware of Snowflake\u2019s cloud-native features like scalability and separation of storage/compute.\n* Understands basic table types (permanent, transient, temporary).",
    "L2": "* Designs data pipelines to ingest structured and semi-structured data into Snowflake.\n* Uses Snowflake features like stages, file formats, and COPY commands.\n* Implements data transformations using SQL and Snowflake Tasks.\n* Manages roles, warehouses, and resource monitoring.\n* Collaborates with teams to optimize query performance and data modeling.",
    "L3": "* Builds scalable, automated data integration workflows using Snowflake Streams, Tasks, and Procedures.\n* Integrates Snowflake with ETL tools (e.g., Talend, ADF, dbt) and cloud platforms.\n* Implements advanced features like Time Travel, Zero-Copy Cloning, and Data Sharing.\n* Applies performance tuning, cost optimization, and security best practices.\n* Leads enterprise data warehouse strategy, governance, and compliance initiatives.",
    "hashId": "416c3367f89d3755dc0e93cf56fe165c4dc30354b4fa54fb5ecc9446358c7ab5"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Warehouse",
    "Tools": "BigQuery",
    "L1": "* Understands basic data warehousing concepts and BigQuery\u2019s role in GCP.\n* Can run simple SQL queries in the BigQuery console.\n* Loads small datasets manually (e.g., CSV, JSON) into BigQuery tables.\n* Aware of BigQuery\u2019s serverless architecture and pricing model.\n* Understands basic table types and dataset organization.",
    "L2": "* Designs data pipelines to ingest structured and semi-structured data into BigQuery.\n* Uses scheduled queries and Dataflow/Cloud Functions for automated ingestion.\n* Implements data transformations using SQL and temporary tables.\n* Manages access controls, partitions, and clustering for performance.\n* Collaborates with teams to optimize query cost and data modeling.",
    "L3": "* Builds scalable, production-grade data integration workflows using BigQuery with tools like dbt, Apache Beam, or Airflow.\n* Implements advanced features like materialized views, BI Engine, and federated queries.\n* Integrates BigQuery with ML models, dashboards, and real-time analytics.\n* Applies performance tuning, cost optimization, and governance best practices.\n* Leads enterprise data warehouse strategy, including security, compliance, and cross-platform integration.",
    "hashId": "fc3cc67506631404ba3a095b0625aafe3d158f92570ac5766172801b46a8f159"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Warehouse",
    "Tools": "RedShift",
    "L1": "* Understands basic data warehousing concepts and Redshift\u2019s architecture.\n* Can connect to Redshift and run simple SQL queries.\n* Loads small datasets using basic tools (e.g., Redshift UI, COPY command from S3).\n* Aware of Redshift\u2019s columnar storage and cluster setup.\n* Understands basic table creation and data types.",
    "L2": "* Designs data pipelines to ingest structured data into Redshift from various sources.\n* Uses COPY command with optimized file formats (e.g., Parquet, CSV) from S3.\n* Implements data transformations using SQL and staging tables.\n* Manages performance using distribution styles, sort keys, and vacuum operations.\n* Collaborates with teams to optimize query performance and data modeling.",
    "L3": "* Builds scalable, automated ETL workflows integrating Redshift with tools like Glue, Airflow, or dbt.\n* Implements advanced features like Redshift Spectrum for querying external data.\n* Applies performance tuning, workload management, and concurrency scaling.\n* Integrates Redshift with BI tools and real-time analytics platforms.\n* Leads enterprise data warehouse strategy, including governance, security, and cost optimization.",
    "hashId": "f04ca2fb176b081dda3916ea5d9368e65e646a9fd1bf05c946832b12379df3a6"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Data Warehouse",
    "Tools": "Python (sqlalchemy, pandas, boto3, google-cloud-bigquery)",
    "L1": "* Understands basic Python syntax and data manipulation using pandas.\n* Can connect to databases using sqlalchemy for simple queries.\n* Reads and writes data from/to CSV, Excel, or local files.\n* Aware of cloud SDKs like boto3 (AWS) and google-cloud-bigquery (GCP).\n* Performs basic data cleaning and formatting tasks.",
    "L2": "* Uses sqlalchemy for dynamic SQL queries and database transactions.\n* Automates data ingestion and transformation using pandas pipelines.\n* Connects to cloud data warehouses (e.g., BigQuery) using Python SDKs.\n* Uploads/downloads data from cloud storage using boto3 or GCP libraries.\n* Implements error handling, logging, and modular code practices.",
    "L3": "* Builds scalable, production-grade ETL workflows using Python and cloud-native libraries.\n* Integrates Python scripts with orchestration tools (e.g., Airflow, Prefect).\n* Optimizes data processing performance using vectorized operations and parallelism.\n* Implements secure authentication and access control for cloud services.\n* Leads development of reusable Python frameworks for data integration across platforms.",
    "hashId": "28b3ab5f72a5ca7bab1e0c1fc955c31645cbfd20ffb2ade50e5cfb1f591a6742"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Integration",
    "Tools": "MuleSoft",
    "L1": "* Understands basic integration concepts and MuleSoft\u2019s role in connecting systems.\n* Can navigate Anypoint Studio and create simple flows using drag-and-drop components.\n* Uses basic connectors (e.g., HTTP, File, Database) to move data between sources.\n* Aware of MuleSoft\u2019s API-led connectivity approach.\n* Understands basic deployment options and runtime behavior.",
    "L2": "* Designs integration flows using multiple connectors and transformation components.\n* Implements data mapping and filtering using DataWeave.\n* Connects MuleSoft to cloud platforms, databases, and APIs.\n* Manages error handling, logging, and flow control.\n* Collaborates with teams to optimize integration performance and reusability.",
    "L3": "* Builds scalable, enterprise-grade integrations using MuleSoft APIs, flows, and microservices.\n* Implements CI/CD pipelines for MuleSoft deployments using Git, Jenkins, or Azure DevOps.\n* Designs and manages API gateways, policies, and security configurations.\n* Integrates MuleSoft with data platforms (e.g., Snowflake, Salesforce, SAP).\n* Leads architecture strategy for unified data integration and governance across systems.",
    "hashId": "a150aa874698fce433153059bd3cc3d68ae308ca660152230914d5d44ed36e1d"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Integration",
    "Tools": "Amazon Kinesis",
    "L1": "* Understands the concept of real-time data streaming and Amazon Kinesis\u2019s role.\n* Can navigate the AWS Console to create basic Kinesis Data Streams.\n* Sends and receives simple data records using AWS SDK or CLI.\n* Aware of Kinesis components: Data Streams, Firehose, Analytics, and Video Streams.\n* Understands basic use cases like log collection or event streaming.\n",
    "L2": "* Designs streaming data pipelines using Kinesis Data Streams and Firehose.\n* Connects Kinesis to other AWS services (e.g., S3, Redshift, Lambda).\n* Implements data transformation and buffering using Firehose.\n* Monitors stream performance and configures shard capacity.\n* Collaborates with teams to manage schema evolution and data quality.\n",
    "L3": "* Builds scalable, fault-tolerant streaming architectures using Kinesis and complementary services.\n* Implements real-time analytics using Kinesis Data Analytics and SQL queries.\n* Integrates Kinesis with external systems and orchestration tools (e.g., Apache Flink, Airflow).\n* Applies advanced security, encryption, and access control policies.\n* Leads enterprise streaming strategy, including cost optimization, governance, and compliance.",
    "hashId": "542f70f2b3b2175cc763b66b3b8e01fb78b7727907a184358a60bbbc50342289"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Integration",
    "Tools": "Azure Synapse Analytics",
    "L1": "* Understands basic concepts of data warehousing and Azure Synapse\u2019s unified analytics platform.\n* Can navigate Synapse Studio and run simple SQL queries.\n* Loads small datasets into Synapse using built-in tools or manual uploads.\n* Aware of Synapse components: SQL pools, Spark pools, pipelines, and data lake integration.\n* Understands basic workspace structure and permissions.",
    "L2": "* Designs data pipelines using Synapse Pipelines to ingest and transform data.\n* Connects Synapse to various sources (e.g., Azure Data Lake, Blob Storage, SQL DB).\n* Uses serverless SQL and Spark pools for data processing.\n* Implements data partitioning, indexing, and performance tuning.\n* Collaborates with teams to manage data models and optimize query performance.",
    "L3": "* Builds scalable, enterprise-grade data integration workflows using Synapse and linked services.\n* Integrates Synapse with other Azure services (e.g., Data Factory, Power BI, Purview).\n* Implements CI/CD for Synapse artifacts using Git integration and DevOps pipelines.\n* Applies advanced security, access control, and data governance practices.\n* Leads architecture strategy for unified analytics, lakehouse design, and real-time data processing.",
    "hashId": "54b9f047bb6e8b08009e7f8c07e107ddc6f6df2ef8154ba7f28a758c97fc1f2a"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Integration",
    "Sub-Sub-Category": "Integration",
    "Tools": "Google BigQuery",
    "L1": "* Understands basic BigQuery components: datasets, tables, SQL interface.\n* Loads data using UI or simple scripts (CSV, JSON).\n* Writes basic SQL queries: SELECT, WHERE, JOIN.\n* Understands basic ETL concepts.\n* Familiar with flat tables and basic normalization.\n* Uses filters and LIMIT to reduce data scanned.\n* Sets basic permissions on datasets and tables.\n* Connects BigQuery to tools like Google Sheets or Looker Studio.\n* Reads error messages and uses Query History for troubleshooting.\n* Understands on-demand vs flat-rate pricing models.",
    "L2": "* Understands BigQuery\u2019s architecture: storage vs compute, query optimization.\n* Uses Cloud Storage, Dataflow, or scheduled queries for data ingestion.\n* Applies window functions, subqueries, and CTEs in SQL.\n* Builds ELT pipelines using scheduled queries and Cloud Composer.\n* Applies star/snowflake schemas and denormalization for performance.\n* Uses partitioning and clustering for performance optimization.\n* Implements IAM roles and column-level security.\n* Integrates BigQuery with Pub/Sub, Cloud Functions, and Dataflow.\n* Uses INFORMATION_SCHEMA and logs for debugging.\n* Monitors query costs and applies cost controls.",
    "L3": "* Designs scalable BigQuery architectures using advanced features.\n* Implements streaming ingestion and handles schema evolution.\n* Optimizes complex queries using scripting and stored procedures.\n* Designs modular, reusable pipelines with orchestration and monitoring.\n* Applies advanced modeling techniques like data vault or anchor modeling.\n* Analyzes query execution plans and uses materialized views, BI Engine.\n* Designs secure multi-tenant architectures with audit logging.\n* Builds real-time dashboards and alerting systems.\n* Implements automated monitoring and anomaly detection.\n* Designs cost-efficient architectures and manages quotas effectively.",
    "hashId": "554a519c75a1672e7e512994699d1b1acf37b6ddcecd6ad416c9f3293228de15"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Formats, completeness, and accuracy",
    "Tools": "Global Id",
    "L1": "* Understands basic data formats (CSV, JSON, Avro, etc.).\n* Recognizes the importance of data completeness and accuracy.\n* Identifies missing or inconsistent values manually.\n* Understands the concept of a Global ID or unique identifier in datasets.",
    "L2": "* Validates data formats and schema consistency during ingestion.\n* Implements basic data profiling to assess completeness and accuracy.\n* Uses SQL to detect duplicates, nulls, and outliers.\n* Applies Global ID logic to join and deduplicate datasets reliably.",
    "L3": "* Designs automated data quality checks in ingestion pipelines.\n* Implements anomaly detection and data validation rules using BigQuery SQL and scripting.\n* Uses tools like Cloud Data Loss Prevention (DLP) for sensitive data scanning.\n* Ensures referential integrity and consistency across integrated datasets using Global IDs.\n* Builds dashboards to monitor data quality metrics in real-time.",
    "hashId": "3aa774c1741b1c4f42595cd251edc1d45a021a4374f7c323e01b2e411cf729df"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Formats, completeness, and accuracy",
    "Tools": "Talend",
    "L1": "* Understands basic data formats supported by Talend (CSV, Excel, JSON, XML).\n* Loads and previews data using Talend Studio.\n* Identifies missing or malformed data manually.\n* Uses basic Talend components like tFileInputDelimited, tLogRow, and tFilterRow.",
    "L2": "* Validates schema consistency and data types during ingestion.\n* Uses Talend components like tSchemaComplianceCheck and tMap for data transformation and validation.\n* Implements rules to check completeness (e.g., mandatory fields) and accuracy (e.g., value ranges).\n* Applies deduplication and standardization using tUniqRow, tNormalize, and tDenormalize.",
    "L3": "* Designs automated data quality workflows using Talend Data Quality tools.\n* Implements profiling, cleansing, and enrichment using Talend DQ components (tMatchGroup, tDataQualityRules).\n* Integrates Talend with external metadata repositories and data catalogs for quality monitoring.\n* Builds reusable jobs for continuous data quality checks across pipelines.\n* Uses Talend\u2019s Global ID strategy for entity resolution and master data management.",
    "hashId": "eb586640c453b5e52b8fa58151611f506585aa1db374dafbc58298dd85765e5b"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Formats, completeness, and accuracy",
    "Tools": "Informatica",
    "L1": "* Understands common data formats supported by Informatica (CSV, XML, JSON, Excel).\n* Loads and previews data using Informatica PowerCenter or Cloud Data Integration.\n* Identifies missing or incorrect values manually during mapping design.\n* Uses basic transformations like Expression, Filter, and Lookup to inspect data.",
    "L2": "* Applies data validation rules using transformations (Router, Aggregator, Rank).\n* Implements completeness checks (e.g., mandatory fields, null handling).\n* Uses Informatica Data Quality (IDQ) tools for profiling and cleansing.\n* Applies deduplication and standardization using Match and Cleanse transformations.\n* Designs reusable mappings for consistent data quality enforcement.",
    "L3": "* Builds automated data quality workflows using Informatica Data Quality and Axon.\n* Implements advanced profiling, rule-based cleansing, and exception handling.\n* Integrates Informatica with metadata repositories and data governance tools.\n* Uses Global ID strategies for master data management and entity resolution.\n* Monitors data quality metrics and builds dashboards for continuous improvement.",
    "hashId": "ea0c1fc58c65d0be5ef1d2be91428cee432b147befc511ec967198bd63ada71b"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Formats, completeness, and accuracy",
    "Tools": "Deequ (by AWS)",
    "L1": "* Understands basic data formats (CSV, Parquet, JSON) used with Deequ.\n* Learns how Deequ fits into the AWS ecosystem (typically with Spark on EMR).\n* Runs simple data quality checks using predefined constraints.\n* Uses Deequ to detect nulls, duplicates, and basic schema mismatches.",
    "L2": "* Writes custom constraint checks using Deequ\u2019s Scala or Python APIs.\n* Implements completeness checks (e.g., non-null fields, expected value ranges).\n* Uses Deequ\u2019s VerificationSuite to automate profiling and validation.\n* Applies Deequ in batch pipelines for regular data quality monitoring.",
    "L3": "* Designs scalable, reusable data quality frameworks using Deequ.\n* Integrates Deequ with orchestration tools (e.g., Airflow, AWS Step Functions).\n* Implements anomaly detection and historical trend analysis using Deequ metrics.\n* Builds dashboards or alerting systems based on Deequ\u2019s output (e.g., via CloudWatch or custom BI tools).\n* Combines Deequ with metadata management and governance tools for enterprise-grade quality assurance.",
    "hashId": "1d04577f1499136338c755efc9da3df814aae36343402358c7f8294224f714a6"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Formats, completeness, and accuracy",
    "Tools": "Python (pandas, cerberus, pandera)",
    "L1": "* Understands basic data formats (CSV, Excel, JSON) and loads them using pandas.\n* Uses pandas to inspect missing values (isnull(), dropna(), fillna()).\n* Performs basic data type checks and conversions.\n* Learns the purpose of validation libraries like cerberus and pandera.",
    "L2": "* Applies schema validation using cerberus for dictionaries and JSON-like data.\n* Uses pandera to define and enforce DataFrame schemas (e.g., column types, null constraints).\n* Implements completeness checks (e.g., required fields, value ranges).\n* Detects duplicates, outliers, and inconsistent formats using pandas operations.\n* Builds reusable validation functions for data pipelines.",
    "L3": "* Designs robust data validation frameworks using pandera decorators and hypothesis testing.\n* Integrates validation steps into ETL pipelines or notebooks for automated checks.\n* Implements dynamic schema generation and conditional validation logic with cerberus.\n* Builds dashboards or logs to monitor data quality metrics over time.\n* Combines pandas, cerberus, and pandera for layered validation across formats and completeness dimensions.",
    "hashId": "c4483b03223816c753bbda2ba6691abe3b72c8d6c44298f4f33e00878e830432"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Duplicates and inconsistencies",
    "Tools": "Global Id",
    "L1": "* Understands the concept of duplicates and basic inconsistency issues in datasets.\n* Learns the importance of a Global ID (unique identifier) for record tracking.\n* Uses simple filters and conditional logic to identify duplicate rows.\n* Recognizes inconsistent formats (e.g., date formats, casing) manually.",
    "L2": "* Applies deduplication techniques using tools or SQL logic (e.g., ROW_NUMBER(), DISTINCT, fuzzy matching).\n* Implements Global ID logic to join datasets and resolve identity conflicts.\n* Uses validation rules to detect inconsistencies in values, formats, and relationships.\n* Builds reusable scripts or components to standardize and clean data.",
    "L3": "* Designs entity resolution frameworks using Global ID strategies across systems.\n* Implements probabilistic matching and clustering to detect near-duplicates.\n* Automates inconsistency detection using rule engines or data quality libraries.\n* Integrates Global ID logic into master data management (MDM) workflows.\n* Builds monitoring systems to track duplicate rates and inconsistency trends over time.",
    "hashId": "9913c29582c56a4b6c5b39413e1c00b22af32377b6a6c206c2a546ec8579408a"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Duplicates and inconsistencies",
    "Tools": "Python (pandas, fuzzywuzzy, recordlinkage)",
    "L1": "* Uses pandas to identify exact duplicates (duplicated(), drop_duplicates()).\n* Understands basic string inconsistencies (e.g., casing, whitespace) and cleans them using str.lower(), str.strip().\n* Learns the purpose of fuzzy matching and record linkage in resolving duplicates.\n* Applies simple filtering and conditional logic to spot inconsistencies.",
    "L2": "* Uses fuzzywuzzy to detect near-duplicate strings (e.g., names, addresses).\n* Applies recordlinkage to compare datasets and identify potential matches using blocking and comparison techniques.\n* Implements deduplication logic using similarity scores and thresholds.\n* Builds reusable functions to clean and standardize inconsistent data formats.",
    "L3": "* Designs entity resolution workflows combining pandas, fuzzywuzzy, and recordlinkage.\n* Implements probabilistic matching models and custom comparison strategies.\n* Automates inconsistency detection across large datasets using scalable logic.\n* Integrates Global ID generation and matching logic for master data management.\n* Builds dashboards or reports to monitor duplicate rates and data consistency over time.",
    "hashId": "825957466e3c80064b8a6642994334fd7067a6a0803081a01d94cf63177dd9ae"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Duplicates and inconsistencies",
    "Tools": "OpenRefine",
    "L1": "* Loads datasets in OpenRefine and explores basic data structure.\n* Identifies exact duplicates using built-in filters and facets.\n* Uses text facets to spot inconsistencies in categorical data.\n* Applies basic transformations (e.g., trimming whitespace, changing case).",
    "L2": "* Uses clustering algorithms (e.g., key collision, nearest neighbor) to detect near-duplicates.\n* Applies transformations using GREL (General Refine Expression Language) for standardization.\n* Resolves inconsistencies in formats (e.g., dates, names) using custom expressions.\n* Combines multiple columns to create or validate Global IDs.",
    "L3": "* Designs complex workflows for deduplication and inconsistency resolution.\n* Automates data cleaning using OpenRefine scripts and reconciliation services.\n* Integrates OpenRefine with external APIs or knowledge bases for entity resolution.\n* Uses Global ID strategies to link and merge records across datasets.\n* Exports cleaned data for downstream integration with other tools or pipelines.",
    "hashId": "1e99d34bea563a5b4e8afda34a8c184db041932b8641e16cc50d18d428ba80c8"
  },
  {
    "Category": "Data Engineering",
    "Sub-Category": "Data Quality",
    "Sub-Sub-Category": "Duplicates and inconsistencies",
    "Tools": "Apache Griffin",
    "L1": "* Understands Apache Griffin\u2019s role in data quality management.\n* Learns basic concepts of data profiling and quality dimensions (e.g., uniqueness, consistency).\n* Uses simple Griffin jobs to detect exact duplicates in datasets.\n* Understands the importance of Global ID in identifying unique records.",
    "L2": "* Configures Griffin\u2019s DSL (Domain Specific Language) to define rules for detecting duplicates and inconsistencies.\n* Implements uniqueness checks using Global ID logic across datasets.\n* Uses Griffin\u2019s profiling capabilities to identify patterns and anomalies.\n* Integrates Griffin with data sources like Hive, HDFS, or Kafka for quality checks",
    "L3": "* Designs complex data quality pipelines using Apache Griffin\u2019s measurement and validation modules.\n* Implements entity resolution strategies using Global IDs and rule-based matching.\n* Automates detection of near-duplicates and inconsistencies using custom rules and metrics.\n* Integrates Griffin with orchestration tools (e.g., Apache Airflow) and monitoring systems.\n* Builds dashboards and reports to track data quality KPIs over time.",
    "hashId": "52235e2aad4830565cbd6f316d6f14aced0bae239bafaaacfb692b1ed5955a1a"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "Python (pandas, seaborn, matplotlib)",
    "L1": "* Loads and explores datasets using pandas (head(), info(), describe()).\n* Identifies missing values and basic statistics (mean, median, mode).\n* Uses matplotlib and seaborn for simple visualizations (bar charts, histograms).\n* Understands basic data types and distributions.",
    "L2": "* Performs detailed profiling: value counts, correlation matrices, and group-wise summaries.\n* Visualizes relationships using scatter plots, box plots, and heatmaps.\n* Detects outliers and anomalies using statistical summaries and visual cues.\n* Cleans and transforms data using pandas operations (groupby(), pivot_table(), apply()).",
    "L3": "* Builds automated profiling reports using libraries like pandas-profiling or sweetviz.\n* Designs custom visualizations for complex distributions and multivariate analysis.\n* Applies advanced techniques like feature importance, dimensionality reduction (e.g., PCA).\n* Integrates profiling into reproducible workflows and notebooks for scalable analysis.",
    "hashId": "c37d1ee2589255121eb8fab50b7e44a57d2d807d503f5cf4059024f00a02adb1"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "R (dplyr, ggplot2, DataExplorer)",
    "L1": "* Loads and explores datasets using dplyr (glimpse(), summary(), select(), filter()).\n* Identifies missing values and basic statistics (mean, median, mode).\n* Uses ggplot2 for simple visualizations (bar plots, histograms).\n* Understands basic data types and distributions.",
    "L2": "* Performs detailed profiling using dplyr functions (group_by(), mutate(), summarise()).\n* Visualizes relationships and distributions using ggplot2 (box plots, scatter plots, density plots).\n* Uses DataExplorer to generate automated profiling reports (missing data, correlation, distribution).\n* Detects outliers and inconsistencies using statistical summaries and visual cues.",
    "L3": "* Builds custom profiling workflows combining dplyr, ggplot2, and DataExplorer.\n* Applies advanced techniques like feature engineering and dimensionality reduction (e.g., PCA).\n* Designs reproducible EDA scripts for scalable analysis across datasets.\n* Integrates profiling into reporting pipelines or Shiny dashboards for interactive exploration.",
    "hashId": "d768eaa3b7ffaacf137480fe83be518c5ade3d5782cf9b06fcdb29cb5d0d351a"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "SQL ",
    "L1": "* Uses basic SQL queries (SELECT, WHERE, ORDER BY) to explore data.\n* Retrieves summary statistics using COUNT, MIN, MAX, AVG, SUM.\n* Identifies missing values using IS NULL and simple filters.\n* Understands basic data types and value distributions.",
    "L2": "* Performs group-wise profiling using GROUP BY, HAVING, and aggregate functions.\n* Detects duplicates using ROW_NUMBER(), COUNT(*) OVER(), or GROUP BY.\n* Applies conditional logic (CASE WHEN) to flag anomalies or inconsistencies.\n* Uses joins and subqueries to enrich profiling with reference data.",
    "L3": "* Builds reusable profiling views or stored procedures for automated checks.\n* Implements advanced statistical profiling using window functions and CTEs.\n* Designs dashboards or reports using SQL-based BI tools (e.g., Looker, Tableau, Power BI).\n* Integrates SQL profiling into data quality monitoring pipelines.",
    "hashId": "86f062f6479d7b04b5f3e222335956bd51e6887cfdf3991c71f2d89fa08de420"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "Apache Superset",
    "L1": "* Connects to data sources and explores tables using Superset\u2019s SQL Lab.\n* Uses built-in chart types (bar, pie, line) to visualize basic distributions.\n* Applies filters and groupings to inspect data segments.\n* Understands basic metrics like count, average, and null values.",
    "L2": "* Builds dashboards to profile data across multiple dimensions.\n* Uses advanced visualizations (e.g., box plots, heatmaps) to detect outliers and correlations.\n* Applies calculated columns and custom SQL expressions for profiling.\n* Identifies duplicates and inconsistencies using aggregation and conditional logic.",
    "L3": "* Designs interactive dashboards for dynamic data profiling and exploration.\n* Integrates Superset with external data quality tools or APIs for enriched profiling.\n* Automates data refresh and monitoring for real-time profiling insights.\n* Implements role-based access and metadata tagging for governed profiling workflows.",
    "hashId": "7483ea8c650d572572f995e646561bcaa90349833893036e72afdbbc484ca973"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "Power BI",
    "L1": "* Connects to data sources and loads datasets into Power BI.\n* Uses basic visuals (bar charts, tables, pie charts) to explore distributions.\n* Applies filters and slicers to inspect subsets of data.\n* Understands basic summary statistics using built-in aggregations (count, average, min, max).",
    "L2": "* Uses Power Query Editor for data profiling (column statistics, value distribution, missing data).\n* Applies transformations (e.g., data type changes, trimming, splitting columns) to clean data.\n* Builds calculated columns and measures using DAX for profiling logic.\n* Detects duplicates and inconsistencies using grouping, conditional formatting, and DAX expressions.",
    "L3": "* Designs interactive dashboards for dynamic data profiling and anomaly detection.\n* Implements advanced DAX logic for custom profiling metrics and KPIs.\n* Automates data refresh and integrates profiling into scheduled workflows.\n* Combines Power BI with external tools (e.g., Azure Data Catalog, Python/R scripts) for enriched profiling.\n* Builds monitoring systems to track data quality trends over time.",
    "hashId": "abdec8195b9290d60cfce01f13c51abc02a663cd7604ff0378fda18f375cffa2"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "Tableau",
    "L1": "* Connects to data sources and explores datasets using Tableau\u2019s Data Pane.\n* Uses basic visualizations (bar charts, tables, pie charts) to understand distributions.\n* Applies filters and sorting to inspect subsets of data.\n* Understands basic summary metrics (count, average, min, max) using drag-and-drop analytics.",
    "L2": "* Uses Tableau\u2019s Data Interpreter and metadata features to clean and profile data.\n* Applies calculated fields and table calculations to detect missing values, duplicates, and inconsistencies.\n* Builds dashboards to visualize data quality dimensions (e.g., completeness, uniqueness).\n* Uses Level of Detail (LOD) expressions for advanced profiling logic.",
    "L3": "* Designs interactive dashboards for dynamic data profiling and anomaly detection.\n* Implements complex calculated fields and parameter-driven profiling views.\n* Integrates Tableau with external scripts (Python/R via TabPy or Rserve) for enriched profiling.\n* Automates data refresh and monitoring for real-time profiling insights.\n* Builds governance-friendly profiling systems with metadata tagging and role-based access.",
    "hashId": "aeeb357f288dcda8ce899b6698bbb9af0124f2b2a7fe8210f95e396207d15b58"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Data Profiling ",
    "Tools": "Looker Studio",
    "L1": "* Connects to data sources (e.g., Google Sheets, BigQuery) and explores basic tables.\n* Uses simple charts (bar, pie, table) to visualize distributions and summary statistics.\n* Applies filters and date ranges to inspect subsets of data.\n* Understands basic metrics like count, average, and null values using built-in functions.",
    "L2": "* Builds interactive dashboards to profile data across dimensions (e.g., categories, time).\n* Uses calculated fields to detect missing values, duplicates, and inconsistencies.\n* Applies conditional formatting and custom formulas for profiling logic.\n* Combines multiple data sources for enriched profiling and joins.",
    "L3": "* Designs dynamic dashboards with drill-downs and parameter controls for deep data exploration.\n* Implements advanced calculated fields and regular expressions for data validation.\n* Integrates Looker Studio with external tools (e.g., BigQuery, Google Cloud Functions) for automated profiling.\n* Builds monitoring systems to track data quality metrics over time (e.g., completeness, uniqueness, consistency).",
    "hashId": "661cccbfbcd053a75fce09f6c2c609db737983909e4bf7dacc2a4305a65a0963"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "Python (pandas, numpy)",
    "L1": "* Uses pandas to explore data and identify basic anomalies (e.g., missing values, duplicates).\n* Applies simple statistical summaries (mean(), std(), describe()) to spot outliers.\n* Uses conditional filtering to isolate unusual values (e.g., values outside expected ranges).\n* Understands basic concepts of anomalies and their impact on analysis.",
    "L2": "* Implements rule-based anomaly detection using thresholds and z-scores ((x - mean) / std).\n* Uses numpy for efficient numerical operations and outlier detection.\n* Applies interquartile range (IQR) method to detect outliers.\n* Builds reusable functions to automate anomaly detection across datasets.",
    "L3": "* Designs scalable anomaly detection workflows using statistical and algorithmic approaches.\n* Combines multiple techniques (e.g., rolling statistics, time-series decomposition) for dynamic anomaly detection.\n* Integrates anomaly detection into data pipelines for real-time monitoring.\n* Visualizes anomalies using matplotlib and seaborn (e.g., box plots, time-series plots).\n* Prepares data for machine learning-based anomaly detection models (e.g., isolation forest, DBSCAN).",
    "hashId": "78e8a2305d65733c65a35bb4a0cd0e28db3bdd0b5cb40443e072c862d99bbcb2"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "Statsmodels",
    "L1": "* Understands basic statistical concepts (mean, variance, standard deviation).\n* Uses Statsmodels for simple descriptive statistics and linear regression.\n* Learns how anomalies can be identified through residuals and deviations from expected patterns.\n* Applies basic time-series decomposition using seasonal_decompose().",
    "L2": "* Performs anomaly detection using residual analysis from regression models.\n* Applies time-series models (e.g., ARIMA) to detect unexpected shifts or spikes.\n* Uses hypothesis testing (e.g., t-tests, z-tests) to identify statistically significant anomalies.\n* Combines Statsmodels with pandas for structured anomaly detection workflows.",
    "L3": "* Designs robust anomaly detection pipelines using advanced time-series models (e.g., SARIMA, VAR).\n* Implements outlier detection using influence measures (e.g., Cook\u2019s distance, leverage).\n* Automates anomaly detection in forecasting models with confidence intervals and prediction errors.\n* Integrates Statsmodels with visualization tools to highlight anomalies in dashboards or reports.",
    "hashId": "b3ac6046dbd97314d09d0bb1bea1d3c4969c7688ab68038f2e02711100603ac7"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "Matplotlib / Seaborn / Plotly",
    "L1": "* Uses Matplotlib and Seaborn to create basic plots (line, bar, histogram) for visual inspection of anomalies.\n* Identifies outliers manually by observing unusual spikes or gaps in plots.\n* Understands basic visual cues for anomalies (e.g., extreme values, missing data).\n* Uses Plotly for interactive exploration of simple datasets.",
    "L2": "* Creates box plots, scatter plots, and distribution plots to detect outliers and irregular patterns.\n* Uses time-series plots to identify temporal anomalies (e.g., sudden drops or spikes).\n* Applies conditional formatting and annotations to highlight anomalies.\n* Builds interactive dashboards using Plotly to explore anomalies across dimensions.",
    "L3": "* Designs custom visualizations to detect complex anomalies (e.g., multivariate outliers, trend shifts).\n* Combines statistical techniques with visualizations (e.g., z-score overlays, confidence intervals).\n* Uses Plotly for dynamic anomaly detection in real-time or streaming data.\n* Integrates visual anomaly detection into automated reporting and monitoring systems.",
    "hashId": "3b948b94a9f9b27942ce8db8ac3252f6039fdd627ee976decedcad67576b5701"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "R (anomalize, tsoutliers)",
    "L1": "* Understands basic concepts of anomalies in time-series and tabular data.\n* Uses simple R functions to visualize data and spot outliers manually.\n* Learns how anomalize and tsoutliers fit into the anomaly detection workflow.\n* Applies basic decomposition techniques to separate trend and seasonality.",
    "L2": "* Uses anomalize to detect anomalies in time-series data using decomposition and IQR methods.\n* Applies tsoutliers to identify and label outliers in ARIMA models.\n* Visualizes anomalies using ggplot2 integrated with anomalize outputs.\n* Combines anomaly detection with data cleaning and transformation workflows.",
    "L3": "* Designs automated anomaly detection pipelines using anomalize and tsoutliers.\n* Tunes model parameters for improved sensitivity and specificity in anomaly detection.\n* Integrates anomaly detection with forecasting models and real-time monitoring systems.\n* Builds interactive dashboards (e.g., with Shiny) to explore and report anomalies dynamically.",
    "hashId": "47b4de1726e24b35870d8ab2c35fedf3a19c7c48f73a888433b247eb3c8de99b"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "SQL",
    "L1": "* Uses basic SQL queries (SELECT, WHERE, ORDER BY) to manually inspect anomalies.\n* Identifies missing values using IS NULL and filters for unexpected values.\n* Applies simple range checks to detect outliers (e.g., WHERE value > threshold).\n* Understands basic anomaly concepts like duplicates and nulls.",
    "L2": "* Uses aggregate functions (AVG, STDDEV, COUNT) to compute statistical thresholds.\n* Applies conditional logic (CASE WHEN) to flag anomalies based on business rules.\n* Detects duplicates using GROUP BY, HAVING COUNT(*) > 1, or window functions like ROW_NUMBER().\n* Implements IQR-based outlier detection using subqueries and percentile calculations.",
    "L3": "* Designs reusable views or stored procedures for automated anomaly detection.\n* Uses advanced window functions (LAG, LEAD, RANK) to detect temporal anomalies.\n* Integrates anomaly detection logic into ETL workflows or BI dashboards.\n* Builds monitoring systems using SQL-based alerts and anomaly tracking tables.",
    "hashId": "7ffe6f09e95c69a7963907e36f43a027d3b6b40395682c8b6e22b3869f89cfbf"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "Azure ML",
    "L1": "* Understands basic concepts of anomalies and their impact on data analysis.\n* Uses Azure ML Studio to load datasets and perform basic data exploration.\n* Applies simple statistical techniques (mean, standard deviation) to identify outliers.\n* Learns how to use built-in modules for data cleaning and visualization.",
    "L2": "* Implements rule-based anomaly detection using Azure ML components (e.g., conditional filters, z-score logic).\n* Uses Azure ML Designer to build pipelines that include anomaly detection steps.\n* Applies time-series anomaly detection using built-in models or Python/R scripts within Azure ML.\n* Integrates Azure ML with other Azure services (e.g., Azure Data Lake, Azure Synapse) for scalable data profiling.",
    "L3": "* Designs and trains custom anomaly detection models using Python SDK or AutoML in Azure ML.\n* Implements unsupervised learning techniques (e.g., Isolation Forest, One-Class SVM) for anomaly detection.\n* Automates anomaly detection workflows with scheduled retraining and real-time scoring.\n* Builds dashboards and alerting systems using Azure Monitor and Power BI for anomaly tracking.\n* Integrates anomaly detection into enterprise-grade ML pipelines with CI/CD and governance controls.",
    "hashId": "25b1c08e5fb44f672bd8c96d680bfb523102f9290c774e80d892e45a904c0ee3"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Anomaly Detection",
    "Tools": "Google Vertex AI",
    "L1": "* Understands basic concepts of anomaly detection and its role in EDA.\n* Uses Vertex AI Workbench to explore datasets and identify simple anomalies manually.\n* Applies basic statistical techniques (mean, standard deviation, IQR) using built-in notebooks.\n* Learns how to use AutoML Tables for basic anomaly detection tasks.",
    "L2": "* Trains custom models using Vertex AI AutoML or custom training pipelines to detect anomalies.\n* Uses BigQuery ML within Vertex AI to apply statistical and machine learning models for anomaly detection.\n* Integrates anomaly detection workflows with Vertex AI Pipelines for repeatable analysis.\n* Applies time-series anomaly detection using TensorFlow or scikit-learn within Vertex AI Workbench.",
    "L3": "* Designs and deploys scalable anomaly detection models using Vertex AI custom training and prediction services.\n* Implements unsupervised learning techniques (e.g., Isolation Forest, Autoencoders) for complex anomaly detection.\n* Automates real-time anomaly detection using Vertex AI with streaming data sources (e.g., Pub/Sub, Dataflow).\n* Monitors model performance and anomaly trends using Vertex AI Model Monitoring and logging tools.\n* Integrates anomaly detection into enterprise ML workflows with CI/CD, governance, and alerting systems.",
    "hashId": "62f41deb13b7fab431b3493f884f3408ece94e1550ec3a12920080a571908ad7"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Feature Identification ",
    "Tools": "Python (pandas, numpy)",
    "L1": "* Uses pandas to load and inspect datasets (head(), info(), describe()).\n* Identifies basic features (columns) and understands their data types.\n* Detects missing values and constant columns that may not be useful.\n* Applies simple transformations (e.g., renaming, dropping irrelevant columns).\n",
    "L2": "* Uses statistical summaries (mean(), std(), value_counts()) to assess feature variability.\n* Applies correlation analysis to identify redundant or highly related features.\n* Uses numpy for numerical operations and feature scaling (e.g., normalization).\n* Implements feature selection techniques like variance thresholding and filtering based on domain knowledge.",
    "L3": "* Designs automated feature identification workflows using statistical and algorithmic methods.\n* Applies dimensionality reduction techniques (e.g., PCA) to identify latent features.\n* Uses feature importance scores from models (e.g., tree-based, regression) to rank features.\n* Integrates feature engineering and selection into machine learning pipelines for reproducibility and scalability.",
    "hashId": "2f1951ff65d93fa0701521c33668b5ca127c98cd6e190138bef2d7bc69b816b4"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Feature Identification ",
    "Tools": "Matplotlib / Seaborn / Plotly",
    "L1": "* Uses Matplotlib and Seaborn to create basic plots (histograms, bar charts) to understand feature distributions.\n* Visualizes missing values and constant features to assess relevance.\n* Uses Plotly for interactive exploration of individual features.\n* Understands how visual patterns can indicate feature usefulness.\n",
    "L2": "* Applies pair plots, box plots, and violin plots to explore relationships between features and target variables.\n* Uses correlation heatmaps to identify redundant or highly correlated features.\n* Builds interactive dashboards with Plotly to explore feature behavior across dimensions.\n* Combines visual insights with statistical summaries to refine feature selection.",
    "L3": "* Designs custom visualizations to detect multicollinearity, non-linear relationships, and feature interactions.\n* Uses advanced Plotly features (e.g., 3D plots, animations) for deep feature analysis.\n* Integrates visual feature profiling into automated EDA workflows and notebooks.\n* Combines visual and algorithmic techniques (e.g., SHAP, feature importance overlays) for robust feature identification.",
    "hashId": "508607d14eab95c8f541aa32a4564bf11367b85af5454bfe8dfa32ef45209979"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Feature Identification ",
    "Tools": "SQL",
    "L1": "* Uses SELECT statements to explore columns and understand basic data types.\n* Identifies missing values using IS NULL and constant columns using DISTINCT.\n* Applies simple filters to inspect feature distributions.\n* Understands the role of features in analysis and modeling.",
    "L2": "* Uses aggregate functions (AVG, MIN, MAX, COUNT) to assess feature variability.\n* Applies GROUP BY and HAVING clauses to detect categorical feature usefulness.\n* Performs correlation analysis using SQL joins and derived metrics.\n* Identifies redundant or low-variance features using subqueries and conditional logic.",
    "L3": "* Designs reusable views or stored procedures for automated feature profiling.\n* Implements feature selection logic using statistical thresholds and business rules.\n* Integrates SQL feature identification into data pipelines and BI tools.\n* Combines SQL with external tools (e.g., Python, R, BigQuery ML) for hybrid feature analysis.",
    "hashId": "bc9411703e4748228c97c073a16feb27d8ece770adf742ca4a8e1dd9f677c371"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Feature Identification ",
    "Tools": "Yellowbrick",
    "L1": "* Understands the purpose of feature identification and selection in EDA.\n* Uses basic Yellowbrick visualizers like FeatureImportances and  Rank1D to explore feature relevance.\n* Learns how to integrate Yellowbrick with scikit-learn models for visual analysis.\n* Applies simple visual diagnostics to identify low-variance or irrelevant features.",
    "L2": "* Uses Yellowbrick tools like Rank2D for correlation analysis between features.\n* Applies PCA and Manifold visualizers to explore dimensionality and feature interactions.\n* Combines Yellowbrick with preprocessing steps (e.g., scaling, encoding) for more accurate feature profiling.\n* Interprets visual outputs to refine feature sets for modeling.",
    "L3": "Designs automated feature identification workflows using Yellowbrick in model pipelines.\nIntegrates multiple visualizers (e.g., FeatureImportances, PermutationImportance) for robust feature evaluation.\nUses Yellowbrick to compare feature relevance across different models and datasets.\nBuilds custom visualizations and extends Yellowbrick for specialized feature analysis needs.\n",
    "hashId": "1faa2a23917b3b485fb6202b723f48f5ecb1ebf4bad36ee28368cb6ce41ea4a3"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Feature Identification ",
    "Tools": "R (caret, dplyr, ggplot2)",
    "L1": "* Uses dplyr to inspect and manipulate features (select(), filter(), mutate()).\n* Applies summary() and glimpse() to understand feature distributions and data types.\n* Uses ggplot2 for basic visualizations (e.g., histograms, bar plots) to explore individual features.\n* Understands the concept of features and their role in predictive modeling.",
    "L2": "* Applies dplyr functions to group and summarize data for feature relevance.\n* Uses ggplot2 to visualize relationships between features and target variables (e.g., box plots, scatter plots).\n* Implements feature selection techniques using caret (e.g., recursive feature elimination, correlation filtering).\n* Combines statistical summaries and visual insights to refine feature sets.",
    "L3": "* Designs automated feature selection workflows using caret with cross-validation and model-based importance.\n* Applies dimensionality reduction techniques (e.g., PCA) and visualizes results with ggplot2.\n* Integrates feature engineering and selection into reproducible modeling pipelines.\n* Evaluates feature stability and importance across multiple models and datasets.",
    "hashId": "5a88e10d30110ea859c798d84a9d3a1f5cb599a85b4673eb9201b20940f67a4a"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Exploratory Data Analysis (EDA)",
    "Sub-Sub-Category": "Analytics",
    "Tools": "See Analytics",
    "L1": null,
    "L2": null,
    "L3": null,
    "hashId": "f7253980d2f9cbfc26df6e1018c81e9693d2f084601198f8e14afa83f87a800c"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "Python (pandas, numpy, scipy, statsmodels)",
    "L1": "Descriptive Statistics\n* Understand basic statistical concepts: mean, median, mode, range, variance, standard deviation.\n* Use pandas and numpy to compute summary statistics.\n* Create frequency tables and simple visualizations (histograms, boxplots).\n* Handle missing data and perform basic data cleaning.\n",
    "L2": "Descriptive Statistics\n* Use groupby and aggregation functions in pandas for segmented analysis.\n* Apply z-scores and detect outliers.\n* Visualize distributions using seaborn and matplotlib.",
    "L3": "Descriptive Statistics\n* Automate EDA (Exploratory Data Analysis) workflows.\n* Apply advanced techniques like PCA for dimensionality reduction.\n* Use custom statistical functions and decorators in Python.\n",
    "hashId": "15ce87f30a7d8fd4acdd55ed0877b1eaa7435da8ea65d739247b3a5062d4d8cb"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "Python (pandas, numpy, scipy, statsmodels)",
    "L1": "Inferential Statistics\n* Understand population vs. sample, types of variables.\n* Introduction to probability distributions (normal, binomial).\n* Perform basic hypothesis testing (t-test, chi-square) using scipy.stats.\n* Interpret p-values and confidence intervals.",
    "L2": "Inferential Statistics\n* Perform ANOVA and correlation analysis.\n* Use statsmodels for linear regression and model diagnostics.\n* Understand assumptions behind statistical tests.\n* Apply non-parametric tests (Mann-Whitney U, Kruskal-Wallis).",
    "L3": "Inferential Statistics\n* Build and interpret multiple regression models.\n* Conduct time series analysis and forecasting.\n* Apply Bayesian statistics using statsmodels or PyMC.\n* Evaluate model performance using statistical metrics (AIC, BIC, R\u00b2, adjusted R\u00b2).",
    "hashId": "ffdd451d29be732952dcfd03e0b97b6edc2ce3b2d478d50cd791f6d1abfb1ead"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "SQL ",
    "L1": "Descriptive Statistics\n* Use SQL aggregate functions (COUNT, SUM, AVG, MIN, MAX) to summarize data.\n* Generate frequency distributions using GROUP BY.\n* Filter and sort data using WHERE, ORDER BY.\n* Understand basic statistical terms and their SQL equivalents.",
    "L2": "Descriptive Statistics\n* Use window functions (ROW_NUMBER, RANK, NTILE) for advanced data segmentation.\n* Calculate moving averages and rolling statistics.\n* Perform cohort and trend analysis using SQL.",
    "L3": "Descriptive Statistics\n* Automate statistical reporting using stored procedures and views.\n* Build reusable SQL templates for EDA (Exploratory Data Analysis).\n* Perform multi-dimensional analysis using pivot tables and CUBE/ROLLUP.",
    "hashId": "5d98c77eaabd630715cb9ffecc751e7ef54fdcbd39d6c077daba8d9ee502e498"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "SQL ",
    "L1": "Inferential Statistics\n* Understand the concept of sampling and population.\n* Perform basic comparisons using conditional logic (CASE, IF).\n* Use SQL to prepare data for hypothesis testing (e.g., grouping samples).\n* Export data to statistical tools (e.g., Python, Excel) for further analysis.",
    "L2": "Inferential Statistics\n* Prepare datasets for statistical tests (e.g., t-tests, ANOVA) using SQL joins and filters.\n* Use SQL to simulate random sampling and stratified sampling.\n* Integrate SQL with Python/R for executing statistical models.\n* Apply logic to identify statistically significant differences in groups.",
    "L3": "Inferential Statistics\n* Implement statistical models in SQL (e.g., logistic regression approximations).\n* Use SQL for A/B testing frameworks and experiment tracking.\n* Apply Bayesian logic and probability calculations using SQL expressions.\n* Optimize statistical queries for performance and scalability.",
    "hashId": "70ae4aedccae77c17682d58b443694658ebf450b732c02306bc697d97da347da"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "Minitab",
    "L1": "Descriptive Statistics\n* Navigate Minitab interface and import datasets.\n* Generate basic summary statistics (mean, median, mode, standard deviation).\n* Create simple visualizations (histograms, boxplots, scatterplots).\n* Use Stat > Basic Statistics for quick descriptive analysis.",
    "L2": "Descriptive Statistics\n* Perform segmented analysis using By Variables.\n* Use control charts for process monitoring.\n* Apply transformations and standardization techniques.\n",
    "L3": "Descriptive Statistics\n* Automate repetitive analysis using Minitab macros.\n* Perform multivariate analysis (e.g., PCA).\n* Customize control charts and dashboards for reporting.",
    "hashId": "74368f197636d18bf24337c959defbd1080700918d2fe3b9b0c761547f83e7e6"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "Minitab",
    "L1": "Inferential Statistics\n* Understand concepts of hypothesis testing and confidence intervals.\n* Perform one-sample and two-sample t-tests.\n* Use Minitab Assistant for guided analysis.\n* Interpret p-values and test statistics from output.",
    "L2": "Inferential Statistics\n* Conduct ANOVA and regression analysis.\n* Perform non-parametric tests (Mann-Whitney, Kruskal-Wallis).\n* Analyze categorical data using chi-square tests.\n* Use Stat > Regression for linear modeling and diagnostics.",
    "L3": "Inferential Statistics\n* Build and interpret multiple and logistic regression models.\n* Conduct time series forecasting and ARIMA modeling.\n* Apply DOE (Design of Experiments) and analyze factorial designs.\n* Use advanced statistical tools (e.g., General Linear Model, Mixed Models).",
    "hashId": "63e461f2923167f213d6e9ff345c1e88059361545be60287a6f8e1a1cf8eb5aa"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "SPSS",
    "L1": "Descriptive Statistics\n* Navigate SPSS interface and import datasets (Excel, CSV).\n* Use Descriptive Statistics to compute mean, median, mode, standard deviation.\n* Create basic visualizations (bar charts, histograms, pie charts).\n* Understand variable types and data coding in SPSS.",
    "L2": "Descriptive Statistics\n* Use Explore function for segmented analysis and outlier detection.\n* Apply transformations and recoding of variables.\n* Create boxplots and scatterplots for distribution analysis.",
    "L3": "Descriptive Statistics\n* Automate analysis using SPSS syntax editor.\n* Perform multivariate analysis (e.g., factor analysis, cluster analysis).\n* Customize output tables and graphs for publication-quality reporting.\n",
    "hashId": "395417480d14e0cc37d6d96ee1c4f25424f2d5d6bc29f2fc1129df208611246b"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "SPSS",
    "L1": "Inferential Statistics\n* Perform basic hypothesis tests (one-sample t-test, independent t-test).\n* Use Analyze > Compare Means for group comparisons.\n* Interpret p-values and confidence intervals from SPSS output.\n* Export results and charts for reporting.",
    "L2": "Inferential Statistics\n* Conduct ANOVA and correlation analysis.\n* Perform non-parametric tests (Mann-Whitney U, Wilcoxon, Kruskal-Wallis).\n* Use Linear Regression for predictive modeling.\n* Check assumptions (normality, homoscedasticity) using SPSS tools.",
    "L3": "Inferential Statistics\n* Build and interpret multiple and logistic regression models.\n* Conduct time series analysis and forecasting.\n* Apply advanced statistical models (GLM, Mixed Models).\n* Use SPSS with Python integration for extended analytics.",
    "hashId": "da41c87f7127aaa41c78d7d945cf8b4a3bbd1b9b9cebee1359fd0a62cc45e3f7"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "SAS",
    "L1": "Descriptive Statistics\n* Navigate SAS interface and import datasets.\n* Use PROC MEANS, PROC FREQ for basic statistical summaries.\n* Generate simple visualizations (bar charts, histograms).\n* Understand SAS data step and basic syntax.\n",
    "L2": "Descriptive Statistics\n* Use PROC UNIVARIATE for detailed distribution analysis.\n* Apply data transformations and handle missing values.\n* Create boxplots and scatterplots using PROC SGPLOT.",
    "L3": "Descriptive Statistics\n* Automate analysis using macros and custom procedures.\n* Perform multivariate analysis (e.g., PCA, factor analysis).\n* Build dashboards and reports using SAS Visual Analytics.",
    "hashId": "10e0f2178c685f09fdfe612c922ac09a675fc289d393587fe7de93c7f7123431"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "SAS",
    "L1": "Inferential Statistics\n* Perform basic hypothesis testing (one-sample and two-sample t-tests).\n* Use PROC TTEST and interpret p-values and confidence intervals.\n* Export results for reporting and visualization.\n* Understand concepts of sampling and population.",
    "L2": "Inferential Statistics\n* Conduct ANOVA using PROC ANOVA or PROC GLM.\n* Perform correlation and regression analysis (PROC CORR, PROC REG).\n* Apply non-parametric tests (Wilcoxon, Kruskal-Wallis).\n* Check statistical assumptions and model diagnostics.",
    "L3": "Inferential Statistics\n* Build and interpret multiple and logistic regression models.\n* Conduct time series analysis using PROC ARIMA, PROC TIMESERIES.\n* Apply advanced modeling techniques (Mixed Models, Bayesian analysis).\n* Optimize statistical workflows for large-scale data.",
    "hashId": "def5e6c50887a2406419ac2ee887d756a5dccae1ce55f905f3856889015f7aa1"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "STATA",
    "L1": "Descriptive Statistics\n* Navigate STATA interface and import datasets.\n* Use summarize, tabulate, and list commands for basic statistics.\n* Create simple visualizations (histograms, bar charts, pie charts).\n* Understand variable types and labeling in STATA.\n",
    "L2": "Descriptive Statistics\n* Use by and if conditions for segmented analysis.\n* Detect outliers and apply data transformations.\n* Create boxplots and scatterplots using graph commands.",
    "L3": "Descriptive Statistics\n* Automate analysis using STATA do files and macros.\n* Perform multivariate analysis (e.g., PCA, factor analysis).\n* Customize graphs and tables for publication-quality output.",
    "hashId": "8e74b327c9d65e8069b38e7046f56a14eb7a89b1ae7e4a508ae9fd5b0f558c8e"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "STATA",
    "L1": "Inferential Statistics\n* Perform basic hypothesis tests (one-sample and two-sample t-tests).\n* Use ttest, ci, and mean commands for confidence intervals and comparisons.\n* Interpret STATA output (p-values, test statistics).\n* Export results for reporting and documentation.",
    "L2": "Inferential Statistics\n* Conduct ANOVA using anova and regression using regress.\n* Perform correlation analysis using correlate.\n* Apply non-parametric tests (e.g., ranksum, kwallis).\n* Check assumptions (normality, homoscedasticity) using diagnostic plots.",
    "L3": "Inferential Statistics\n* Build and interpret multiple and logistic regression models (logit, probit).\n* Conduct time series analysis (tsset, arima, forecast).\n* Apply advanced statistical models (GLM, Mixed Models).\n* Use STATA with Mata (matrix programming language) for complex computations",
    "hashId": "2c1e7217bac4ce1d488b27d833ad78226f0eec996729af890311db41fdd7d4cb"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Descriptive  Statistics",
    "Tools": "R (base R, tidyverse, caret, infer)",
    "L1": "Descriptive Statistics\n* Use base R functions (mean(), median(), sd(), summary()) for basic statistics.\n* Create simple visualizations (hist(), boxplot(), plot()).\n* Use tidyverse (dplyr, ggplot2) for data wrangling and plotting.\n* Understand data types, factors, and basic data cleaning.",
    "L2": "Descriptive Statistics\n* Use group_by() and summarise() in dplyr for segmented analysis.\n* Create advanced plots using ggplot2 (e.g., faceted histograms, violin plots).\n* Detect outliers and apply transformations (log, z-score).",
    "L3": "Descriptive Statistics\n* Automate EDA using custom functions and RMarkdown reports.\n* Perform multivariate analysis (PCA, clustering) using prcomp(), kmeans().\n* Create interactive dashboards using shiny or plotly.",
    "hashId": "4a3a1b3aa616e33265c3fb04b116d19fc8a521c59ba2f7370d8dd3ae4ace2273"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": " Inferential Statistics",
    "Tools": "R (base R, tidyverse, caret, infer)",
    "L1": "Inferential Statistics\n* Understand sampling, distributions, and hypothesis testing basics.\n* Perform t-tests and chi-square tests using t.test(), chisq.test().\n* Use the infer package for tidy-style hypothesis testing.\n* Interpret p-values and confidence intervals.\n",
    "L2": "Inferential Statistics\n* Conduct ANOVA and correlation analysis (aov(), cor()).\n* Build linear regression models using lm() and evaluate assumptions.\n* Use caret for model training and cross-validation.\n* Apply non-parametric tests (Wilcoxon, Kruskal-Wallis).",
    "L3": "Inferential Statistics\n* Build and interpret multiple and logistic regression models (glm()).\n* Conduct time series analysis using forecast, ts, prophet.\n* Apply Bayesian statistics using brms, rstanarm.\n* Use caret for advanced model tuning and performance metrics.",
    "hashId": "0a9cdfd0c3b90397feaf1c7c5f28247ec494d95e71476ff1d4bc369e2377d35b"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Hypothesis Testing",
    "Tools": "Python (pandas, numpy, scipy, statsmodels)",
    "L1": "* Use pandas for data loading and basic filtering.\n* Apply scipy.stats.ttest_1samp, ttest_ind, and chi2_contingency for basic tests.\n* Interpret test results and p-values.\n* Visualize distributions using matplotlib or seaborn.",
    "L2": "* Use statsmodels for t-tests, ANOVA (ols, anova_lm), and regression-based hypothesis testing.\n* Apply non-parametric tests (mannwhitneyu, kruskal, wilcoxon) from scipy.stats.\n* Use bootstrapping techniques for hypothesis testing.\n* Automate hypothesis testing workflows using functions and reusable scripts.",
    "L3": "* Use statsmodels for logistic regression and GLM-based hypothesis testing.\n* Implement custom hypothesis tests using simulation or resampling.\n* Integrate hypothesis testing into machine learning pipelines using caret-like workflows in Python (scikit-learn, mlxtend).\n* Evaluate test robustness and assumptions using diagnostic plots and statistical metrics.\n",
    "hashId": "4d9dc5b965eeb7408cb5aa50a75c0516574af0794d9746732fbac83c61bcf794"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Hypothesis Testing",
    "Tools": "R (base R, tidyverse, caret, infer)",
    "L1": "* Use base R functions like t.test(), chisq.test() for simple hypothesis tests.\n* Use tidyverse (dplyr, ggplot2) for data wrangling and visualization.\n* Apply infer package for tidy-style hypothesis testing workflows.\n* Interpret test outputs and visualize distributions.",
    "L2": "* Perform ANOVA (aov()), correlation (cor()), and regression-based hypothesis testing (lm()).\n* Use infer for simulation-based inference and bootstrapping.\n* Apply non-parametric tests (wilcox.test(), kruskal.test()).\n* Use caret for model training and cross-validation in hypothesis-driven modeling.",
    "L3": "* Build and interpret logistic regression models (glm()).\n* Conduct resampling and permutation tests using infer and custom functions.\n* Integrate hypothesis testing into machine learning workflows using caret.\n* Automate testing and reporting using RMarkdown and custom scripts.",
    "hashId": "d8eef5220ff333b991185c16bcd582d460fc63fd7646d2183fdac65dd9974b85"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Hypothesis Testing",
    "Tools": "SPSS ",
    "L1": "* Use Analyze > Compare Means > One-Sample T Test and Independent-Samples T Test.\n* Perform chi-square tests using Analyze > Descriptive Statistics > Crosstabs.\n* Interpret SPSS output tables (test statistics, p-values, confidence intervals).\n* Export results and charts for documentation.",
    "L2": "* Conduct ANOVA using Analyze > Compare Means > One-Way ANOVA.\n* Perform correlation analysis (Analyze > Correlate > Bivariate) and regression (Analyze > Regression > Linear).\n* Apply non-parametric tests (Mann-Whitney U, Wilcoxon, Kruskal-Wallis).\n* Use Explore and Plots to check normality and other assumptions.",
    "L3": "* Build and interpret logistic regression models (Analyze > Regression > Binary Logistic).\n* Conduct repeated measures ANOVA and mixed models.\n* Use SPSS syntax editor to automate hypothesis testing workflows.\n* Integrate SPSS with Python for extended statistical analysis and reporting.",
    "hashId": "e5372a345dbcfcd45f24f2dff1c8a7bc67e923151c9fc69492278dc1fa669ee7"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Hypothesis Testing",
    "Tools": "SAS",
    "L1": "* Use PROC TTEST for one-sample and independent-sample t-tests.\n* Use PROC FREQ for chi-square tests and frequency analysis.\n* Interpret SAS output (test statistics, p-values, confidence intervals).\n* Export results and graphs for reporting.",
    "L2": "* Conduct ANOVA using PROC ANOVA or PROC GLM.\n* Perform correlation analysis (PROC CORR) and regression (PROC REG).\n* Apply non-parametric tests using PROC NPAR1WAY (e.g., Wilcoxon, Kruskal-Wallis).\n* Use diagnostic plots and residual analysis to validate assumptions.",
    "L3": "* Build and interpret logistic regression models using PROC LOGISTIC.\n* Conduct repeated measures ANOVA and mixed models using PROC MIXED.\n* Automate hypothesis testing workflows using SAS macros and stored processes.\n* Integrate SAS with Python or R for extended statistical modeling and visualization.\n* Apply Bayesian hypothesis testing concepts.",
    "hashId": "1de6274fca933e15def4efa70f6094884b87b06c312083d9b4c2fcb1861e55e0"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Hypothesis Testing",
    "Tools": "STATA",
    "L1": "* Use ttest for one-sample and independent-sample t-tests.\n* Use tabulate with chi2 option for chi-square tests.\n* Interpret STATA output (test statistics, p-values, confidence intervals).\n* Export results and graphs for reporting.",
    "L2": "* Conduct ANOVA using anova and regression-based hypothesis testing using regress.\n* Perform correlation analysis using correlate.\n* Apply non-parametric tests (ranksum, kwallis, signrank).\n* Use graph commands for visualizing distributions and test results.",
    "L3": "* Build and interpret logistic regression models using logit, probit.\n* Conduct repeated measures and mixed models using xtreg, mixed.\n* Automate hypothesis testing workflows using do files and macros.\n* Use Mata (STATA\u2019s matrix programming language) for custom statistical computations.",
    "hashId": "02193475bbdb947d028a3f76a6d935c10466b63e157c276e1307ffd3bf3b158f"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Hypothesis Testing",
    "Tools": "Excel",
    "L1": "* Use built-in functions like T.TEST, CHISQ.TEST, and Z.TEST.\n* Perform manual calculations for mean, standard deviation, and variance.\n* Use Data Analysis Toolpak for t-tests and descriptive statistics.\n* Interpret outputs and highlight key results (e.g., p-values, test statistics",
    "L2": "* Conduct ANOVA using Data Analysis Toolpak.\n* Perform correlation analysis using CORREL and regression using LINEST.\n* Apply non-parametric tests manually or with templates (e.g., Wilcoxon, Kruskal-Wallis).\n* Use conditional formatting and charts to visualize test results.",
    "L3": "* Build custom hypothesis testing templates using formulas and macros.\n* Automate statistical reporting using VBA.\n* Integrate Excel with Power Query and Power BI for dynamic hypothesis testing dashboards.\n* Use Solver for optimization-based statistical scenarios.\n",
    "hashId": "233b4e085e8bf162692102b1b4ff6f9fffa30462a016db1089e15d52c51574fe"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "Python (scipy.stats, numpy, statsmodels, matplotlib, seaborn)",
    "L1": "* Use numpy to generate random samples (np.random.normal, np.random.binomial).\n* Visualize distributions using matplotlib and seaborn (histplot, distplot).\n* Use scipy.stats.describe() for basic distribution statistics.\n* Fit and evaluate distributions using scipy.stats.norm, binom, uniform.",
    "L2": "* Use scipy.stats to compute PDF, CDF, and quantiles (.pdf(), .cdf(), .ppf()).\n* Fit data to distributions using scipy.stats.fit() and evaluate goodness-of-fit.\n* Simulate data using multiple distributions and compare them visually.\n* Use statsmodels for probability plots and distribution diagnostics.\n",
    "L3": "* Model real-world data using custom or compound distributions.\n* Use statsmodels for advanced distribution fitting and residual analysis.\n* Apply Bayesian distributions and probabilistic programming (e.g., with PyMC).\n* Automate distribution analysis and visualization for large datasets.",
    "hashId": "30654a5303496190c1ed4a942ad783194bb299c9291a32b1b00bbcdd1e387101"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "R (stats, ggplot2, dplyr, caret)",
    "L1": "* Use base R functions like rnorm(), runif(), rbinom() to generate random samples.\n* Visualize distributions using ggplot2 (geom_histogram(), geom_density()).\n* Use summary() and table() for basic descriptive statistics.\n* Apply dplyr for data wrangling before distribution analysis.",
    "L2": "* Use stats functions like dnorm(), pnorm(), qnorm() for PDF, CDF, and quantiles.\n* Fit distributions to data using fitdistr() from the MASS package.\n* Use ggplot2 for overlaying theoretical distributions on empirical data.\n* Apply caret for preprocessing and modeling with distribution-based features.",
    "L3": "* Model real-world data using compound or custom distributions.\n* Use caret for model tuning and validation involving probabilistic features.\n* Perform Bayesian distribution modeling using packages like brms or rstanarm.\n* Automate distribution analysis and reporting using RMarkdown and custom functions.",
    "hashId": "5f2d4476354bb2d4453c0dfe38513fcf1fbe412ad7627fab77c2a9476259aafb"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "MATLAB",
    "L1": "* Use built-in functions like rand, randn, and binornd to generate random samples.\n* Visualize distributions using histogram, bar, and plot.\n* Use mean, std, var for descriptive statistics.\n* Apply basic probability functions like normpdf, normcdf.",
    "L2": "* Use fitdist to fit data to theoretical distributions.\n* Evaluate goodness-of-fit using chi2gof and kstest.\n* Generate and compare multiple distributions using simulation.\n* Use probplot for visual distribution diagnostics.",
    "L3": "* Model real-world data using compound or custom distributions.\n* Use mle (maximum likelihood estimation) for parameter estimation.\n* Automate distribution analysis using scripts and functions.\n* Integrate probability distributions into statistical models and simulations.",
    "hashId": "1063d63cf9a9c35b36e3969d955873fdcb5771b2381c8b156d4bdd5e35405469"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "SQL ",
    "L1": "* Use COUNT, GROUP BY, and ORDER BY to create frequency tables.\n* Calculate proportions and cumulative frequencies using basic arithmetic in SQL.\n* Visualize distribution-like summaries by exporting data to Excel or BI tools.",
    "L2": "* Use window functions (ROW_NUMBER, NTILE, PERCENT_RANK) to analyze data distribution.\n* Create histograms and percentiles using SQL queries.\n* Simulate random sampling using functions like RAND() or NEWID() (depending on SQL dialect).\n* Prepare data for statistical distribution analysis in external tools (Python, R).",
    "L3": "* Implement logic to approximate normal distribution behavior (e.g., z-score calculations).\n* Use statistical functions (if supported by the SQL engine) for advanced distribution metrics.\n* Integrate SQL with statistical tools (e.g., Python, R, SAS) for distribution modeling.\n* Automate distribution-based reporting using stored procedures and views.",
    "hashId": "e0c2e4e985dc95f4d7bcb82b9574ced2539e0324363aa6921d77a05f4aab34f8"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "Stata",
    "L1": "* Use generate and rnormal(), runiform(), rbinomial() to simulate data.\n* Create histograms and bar charts using histogram, graph bar.\n* Use summarize, tabulate, and mean for descriptive statistics.\n* Understand and interpret basic distribution shapes visually.",
    "L2": "* Use kdensity for kernel density estimation and distribution visualization.\n* Fit data to distributions using fitdistr (from user-written packages).\n* Use qnorm, pnorm, and invnorm for quantile and probability calculations.\n* Apply simulate for generating and analyzing synthetic data distributions.",
    "L3": "* Model real-world data using compound or custom distributions via Mata.\n* Use ml (maximum likelihood estimation) for custom distribution fitting.\n* Automate distribution analysis using do files and macros.\n* Integrate distribution modeling into regression and time series workflows.",
    "hashId": "31afee246f6328abe300ce9b4d73dabd81811d5c3752fd4275ad4274e9cebb2f"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "SPSS ",
    "L1": "* Use Analyze > Descriptive Statistics > Frequencies to view distribution shapes.\n* Generate histograms and bar charts to visualize data distributions.\n* Use Explore to view skewness, kurtosis, and normality indicators.\n* Interpret basic distribution metrics from SPSS output.",
    "L2": "* Use Analyze > Descriptive Statistics > Explore for detailed distribution analysis.\n* Apply transformations (log, square root) to normalize data.\n* Conduct normality tests (Kolmogorov-Smirnov, Shapiro-Wilk).\n* Use probability plots and Q-Q plots to assess distribution fit.",
    "L3": "* Fit data to theoretical distributions using syntax and advanced procedures.\n* Use Generalized Linear Models to model data with non-normal distributions.\n* Automate distribution analysis using SPSS syntax editor.\n* Integrate SPSS with Python for extended distribution modeling and visualization.",
    "hashId": "c2dcc0dbed1ec6f29593e9d600b8007ee4633c705fbad5799d80fcb53ec1c8c8"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "Looker Studio",
    "L1": "* Connect to data sources (e.g., Google Sheets, BigQuery) containing distribution data.\n* Create basic charts (bar, pie, histogram-style) to show frequency distributions.\n* Use calculated fields to derive proportions and percentages.\n* Apply filters and date ranges to segment data.",
    "L2": "* Create custom visualizations to approximate distribution curves (e.g., using line charts).\n* Use blended data sources to compare distributions across groups.\n* Apply conditional formatting and thresholds to highlight distribution patterns.\n* Use calculated fields to simulate z-scores or percentiles.",
    "L3": "* Integrate external statistical models (e.g., from Python/R) via connected data sources.\n* Build interactive dashboards that visualize distribution shifts over time.\n* Use advanced calculated fields to approximate distribution-based metrics.\n* Automate distribution reporting using scheduled exports and embedded dashboards.",
    "hashId": "b8a4133734c519e385d24e857ae43b2f9b9c3f41fc48667aedb91ab1155301e0"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Probability Distributions",
    "Tools": "Power BI",
    "L1": "* Import data from Excel, SQL, or other sources.\n* Create basic visuals (bar charts, histograms) to show frequency distributions.\n* Use DAX functions like COUNT, DISTINCTCOUNT, and PERCENTILEX.INC for summary statistics.\n* Apply filters and slicers to segment data distributions.",
    "L2": "* Use DAX to calculate statistical measures (mean, variance, standard deviation).\n* Create histograms using binning techniques and custom measures.\n* Use scatter plots and density plots to visualize distribution patterns.\n* Apply conditional formatting and tooltips to highlight distribution insights.",
    "L3": "* Integrate external statistical models (e.g., Python or R scripts) within Power BI visuals.\n* Build interactive dashboards that visualize distribution changes over time.\n* Use advanced DAX to simulate distribution-based metrics (e.g., z-scores, percentiles).\n* Automate distribution reporting using Power BI Service (scheduled refresh, alerts).",
    "hashId": "062471ae071dc2f23c4a0d57ab19f28ac65c7dd786b788512bdb998020cee8d7"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Experimental Design (A/B Testing)",
    "Tools": "Python (scipy.stats, numpy, statsmodels, matplotlib, seaborn)",
    "L1": "* Use numpy and pandas to clean and prepare A/B test data.\n* Calculate basic metrics (mean, proportion, difference).\n* Perform simple t-tests using scipy.stats.ttest_ind.\n* Visualize group comparisons using matplotlib and seaborn.",
    "L2": "* Use statsmodels for regression-based A/B testing (ols, logit).\n* Apply non-parametric tests (mannwhitneyu) for non-normal data.\n* Simulate experiments using numpy and visualize distributions.\n* Use bootstrapping techniques to estimate confidence intervals.",
    "L3": "* Use statsmodels for advanced modeling (GLM, mixed models).\n* Implement Bayesian A/B testing using external libraries (e.g., PyMC, scipy.stats).\n* Automate experiment analysis and reporting using reusable Python scripts.\n* Build dashboards or notebooks for real-time A/B test monitoring.",
    "hashId": "48516c5cdc8f0a2862a08fcbecce83d1b102d8156ea456df996c69f3df2f2df4"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Experimental Design (A/B Testing)",
    "Tools": "R (stats, ggplot2, dplyr, caret, fitdistrplus)",
    "L1": "* Use dplyr to clean and segment data into groups.\n* Calculate group-level metrics (mean, proportion) using summarise().\n* Perform basic t-tests using t.test() from stats.\n* Visualize group comparisons using ggplot2 (geom_bar(), geom_boxplot()).",
    "L2": "* Use fitdistrplus to assess and fit distributions to experimental data.\n* Apply non-parametric tests (wilcox.test(), kruskal.test()) for non-normal data.\n* Use caret for preprocessing and modeling (e.g., logistic regression).\n* Simulate experiments and visualize results using ggplot2 (geom_density(), facet_wrap()).",
    "L3": "* Build and interpret logistic regression models using glm() and caret.\n* Automate A/B test workflows using custom functions and RMarkdown.\n* Use simulation and bootstrapping techniques for robust inference.\n* Integrate A/B testing results into dashboards or reports using shiny.",
    "hashId": "b8f731e3be650a2c3efb1acfd8bacc0766aa9a8a739a8267bf91985fe13c4ab1"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Experimental Design (A/B Testing)",
    "Tools": "SQL",
    "L1": "* Use SELECT, GROUP BY, and WHERE to segment control and treatment groups.\n* Calculate basic statistics (mean, count, proportion) for each group.\n* Export data for statistical testing in external tools (e.g., Excel, Python, R).\n* Create simple visual summaries using BI tools connected to SQL.",
    "L2": "* Use window functions (ROW_NUMBER, RANK, PERCENT_RANK) to track user behavior over time.\n* Simulate random assignment using functions like RAND() or NEWID() (depending on SQL dialect).\n* Calculate lift, relative improvement, and confidence intervals using SQL expressions.\n* Prepare clean datasets for hypothesis testing in statistical tools.\n",
    "L3": "* Automate experiment tracking using stored procedures and views.\n* Integrate SQL with Python/R for in-database statistical testing and modeling.\n* Build dashboards that monitor experiment metrics in real time.\n* Use advanced SQL logic to simulate bootstrapping or permutation tests.",
    "hashId": "90d71e454329508a352b886e784b203b73b4ff5aadcd60c1983d6961a41bbc0b"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Experimental Design (A/B Testing)",
    "Tools": "SPSS ",
    "L1": "* Use Split File or Select Cases to separate control and treatment groups.\n* Perform independent-sample t-tests using Analyze > Compare Means > Independent-Samples T Test.\n* Use Frequencies and Descriptives to summarize group data.\n* Visualize group comparisons using bar charts and boxplots.",
    "L2": "* Conduct ANOVA using Analyze > Compare Means > One-Way ANOVA.\n* Perform non-parametric tests (Mann-Whitney U, Kruskal-Wallis) for non-normal data.\n* Use Analyze > Regression > Linear for regression-based A/B testing.\n* Check assumptions using normality tests and plots (Explore, Q-Q plots).",
    "L3": "* Build logistic regression models using Analyze > Regression > Binary Logistic.\n* Use General Linear Model and Mixed Models for complex experimental designs.\n* Automate A/B test workflows using SPSS syntax editor.\n* Integrate SPSS with Python for advanced statistical modeling and visualization.\n",
    "hashId": "7ed08ba04b1a00e906b24d8cb6befe761330cc9e83934889aef24c2bdd191307"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Experimental Design (A/B Testing)",
    "Tools": "Stata",
    "L1": "* Use ttest to compare means between two groups.\n* Use tabulate and summarize to explore group-level metrics.\n* Visualize group differences using graph bar, boxplot.\n* Use generate and if to create and filter experimental groups.\n",
    "L2": "* Conduct ANOVA using anova and regression-based testing using regress.\n* Apply non-parametric tests (ranksum, kwallis) for non-normal data.\n* Use simulate to model experimental outcomes and variability.\n* Check assumptions using diagnostic plots and residual analysis.",
    "L3": "* Build logistic regression models using logit, probit.\n* Use xtreg and mixed for repeated measures and hierarchical designs.\n* Automate A/B test workflows using do files and macros.\n* Use Mata for advanced statistical modeling and simulation.",
    "hashId": "d9008694f443e1328efe6fc52c88ee51ac714f46b0dc67bc9481c6009a2d4af8"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Statistical Analysis",
    "Sub-Sub-Category": "Experimental Design (A/B Testing)",
    "Tools": "Visualisation and reporting",
    "L1": "* Use Excel, PowerPoint, or BI tools to create simple charts comparing control vs. treatment groups.\n* Highlight key metrics (conversion rate, mean difference) in tables.\n* Use color coding to indicate statistical significance.\n* Export charts and tables for stakeholder presentations.",
    "L2": "* Use tools like Python (matplotlib, seaborn) or R (ggplot2) to create detailed visualizations.\n* Build dashboards in Power BI, Looker Studio, or Tableau to monitor live A/B test metrics.\n* Include statistical annotations (e.g., error bars, significance stars) in visuals.\n* Automate report generation using Jupyter Notebooks or RMarkdown.",
    "L3": "* Create interactive dashboards with filters for segment-level analysis.\n* Use advanced visualizations (e.g., distribution plots, lift curves, Bayesian probability charts).\n* Automate end-to-end reporting pipelines using tools like Python, R, or BI platforms.\n* Integrate experiment results into executive summaries and strategic decision frameworks.",
    "hashId": "05c9b59bb21585bad91ce37c2215b580209194a0b956916a03e19d4a5ebfac65"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "Python (scikit-learn, statsmodels, xgboost, lightgbm, tensorflow, keras, pandas, numpy, matplotlib, seaborn, plotly)",
    "L1": "* Basic syntax, loops, conditionals, functions\n* Load CSVs, basic pandas/numpy operations\n* Simple plots using matplotlib and seaborn\n* Train/test split, Linear & Logistic Regression using scikit-learn\n* Descriptive statistics, simple regression with statsmodels\n* Basic usage of xgboost/lightgbm with default parameters\n* Build simple neural networks using keras/tensorflow\n* Accuracy, confusion matrix\n* Label encoding, one-hot encoding\n* Save/load models using pickle or joblib",
    "L2": "* Object-oriented programming, error handling\n* Data wrangling, merging, reshaping with pandas\n* Custom plots, subplots, styling with seaborn and plotly\n* Cross-validation, pipelines, hyperparameter tuning\n* Hypothesis testing, ANOVA, regression diagnostics\n* Feature importance, early stopping, parameter tuning\n* CNNs, RNNs, model evaluation techniques\n* ROC-AUC, precision/recall, F1-score\n* Scaling, binning, interaction terms\n* Basic Flask APIs for model serving",
    "L3": "* Performance optimization, decorators, generators\n* Efficient data pipelines, handling large datasets\n* Interactive dashboards, animations with plotly\n* Ensemble methods, custom transformers, model persistence\n* Time series modeling, GLMs, advanced diagnostics\n* Custom loss functions, advanced tuning, GPU acceleration\n* Custom layers, callbacks, model deployment strategies\n* Cost-sensitive metrics, advanced evaluation techniques\n* Automated feature selection, domain-specific features\n* Docker, CI/CD pipelines, cloud deployment (AWS/GCP/Azure)",
    "hashId": "67263534337b55d311e8283ddb9659c7eac8357ce5d90954c94b642fa9f33e95"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "R (caret, randomForest, glmnet, e1071, nnet, dplyr, ggplot2, tidyverse)",
    "L1": "* Basic R syntax, loops, and functions\n* Data manipulation using dplyr\n* Simple visualizations with ggplot2\n* Basic models using caret (e.g., linear/logistic regression)\n* Default usage of randomForest\n* Intro to regularization with glmnet\n* Simple SVM and Naive Bayes using e1071\n* Basic neural networks with nnet\n* Evaluation using accuracy and confusion matrix\n* Basic usage of tidyverse for data handling\n",
    "L2": "* Writing functions and handling errors in R\n* Advanced data wrangling with dplyr and tidyverse\n* Custom plots and faceting in ggplot2\n* Cross-validation and grid search with caret\n* Feature importance and tuning in randomForest\n* Hyperparameter tuning in glmnet\n* Kernel selection and tuning in e1071\n* Multi-layer networks with nnet\n* Evaluation using ROC-AUC, precision/recall, F1-score\n* Data reshaping and joins using tidyverse",
    "L3": "* Performance optimization and vectorization in R\n* Handling large datasets and parallel processing\n* Interactive plots using plotly + ggplot2\n* Custom workflows and ensembles with caret\n* Advanced tuning and interpretation in randomForest\n* Elastic net and model comparison with glmnet\n* Custom kernels and probabilistic models with e1071\n* Deep learning integration via keras in R\n* Cost-sensitive metrics and advanced diagnostics\n* Reproducible pipelines using tidyverse and purrr",
    "hashId": "49c5cb9dba407ee0f7c6a5fca73ba7119d25148ba1a43934eba6ba9aac4a5ad4"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "MLflow",
    "L1": "* Understand MLflow components: Tracking, Projects, Models, Registry\n* Log basic parameters, metrics, and models\n* Use MLflow with simple scikit-learn or R models\n* Run experiments locally and view results in MLflow UI\n* Save and load models using MLflow\u2019s built-in functions",
    "L2": "* Integrate MLflow with training pipelines (e.g., sklearn, xgboost, keras)\n* Use MLflow Projects for reproducible code packaging\n* Register models and manage versions in MLflow Model Registry\n* Compare multiple experiment runs and analyze performance\n* Deploy models using MLflow\u2019s REST API or local serving",
    "L3": "* Automate experiment tracking in CI/CD workflows\n* Customize MLflow tracking with tags, artifacts, and nested runs\n* Integrate MLflow with cloud platforms (AWS SageMaker, Azure ML, GCP)\n* Use MLflow with Docker and Kubernetes for scalable deployment\n* Implement model governance and lifecycle management using MLflow Registry",
    "hashId": "8f8fd98f15ea91cc4bf11a5da2b034bb483a9804e417c0f54a5c008ac5f40e07"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "Tableau",
    "L1": "* Understand Tableau interface and basic navigation\n* Connect to structured data sources (Excel, CSV, SQL)\n* Create simple charts: bar, line, scatter\n* Use filters, basic calculations, and parameters\n* Build basic dashboards for supervised learning outputs (e.g., model accuracy)\n",
    "L2": "* Use calculated fields for model metrics (e.g., precision, recall)\n* Create dynamic dashboards with interactive filters and actions\n* Visualize model predictions and classification results\n* Integrate Tableau with Python/R scripts via TabPy or Rserve\n* Build storyboards to communicate model insights",
    "L3": "* Automate data refresh and dashboard updates\n* Use Tableau Prep for data cleaning and transformation\n* Visualize complex model outputs (e.g., feature importance, ROC curves)\n* Embed Tableau dashboards into web apps or portals\n* Collaborate using Tableau Server or Tableau Cloud for model monitoring",
    "hashId": "1a95918524a0c9f310c83204443e2c153173f7279453e2c385245f0333a5bd47"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "Power BI",
    "L1": "* Understand Power BI interface and basic navigation\n* Connect to structured data sources (Excel, CSV, SQL)\n* Create basic visuals: bar, line, pie charts\n* Use filters, slicers, and basic DAX formulas\n* Build simple dashboards to display model outputs (e.g., accuracy, predictions)",
    "L2": "* Use DAX for calculated columns and measures (e.g., precision, recall)\n* Create interactive dashboards with drill-through and bookmarks\n* Visualize classification results and regression outputs\n* Integrate Power BI with Python/R scripts for model execution and visualization\n* Use Power Query for advanced data shaping and transformation",
    "L3": "* Automate data refresh with scheduled updates and gateways\n* Visualize advanced model metrics (e.g., ROC curves, feature importance)\n* Embed Power BI reports into apps or web portals\n* Collaborate and share dashboards via Power BI Service\n* Monitor deployed models and performance using real-time dashboards",
    "hashId": "e0263c4fab75b925aef3bf913d31c3ce8de4d12e50c0e14bba22d259e670dfd6"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "Looker Studio",
    "L1": "* Understand Looker Studio interface and basic navigation\n* Connect to structured data sources (Google Sheets, CSV, BigQuery)\n* Create basic charts: bar, line, pie, tables\n* Use filters, date ranges, and basic calculated fields\n* Build simple dashboards to display model outputs (e.g., accuracy, predictions)",
    "L2": "* Use calculated fields for model metrics (e.g., precision, recall, F1-score)\n* Create interactive dashboards with controls and drill-downs\n* Visualize classification results and regression outputs\n* Integrate with Python/R via connected data sources or APIs\n* Use blended data sources for comparative model analysis",
    "L3": "* Automate data refresh with scheduled updates from cloud sources\n* Visualize advanced model metrics (e.g., ROC curves, feature importance)\n* Embed dashboards into web portals or internal tools\n* Collaborate and share dashboards with access control\n* Monitor deployed models and performance using real-time data feeds",
    "hashId": "5503fec0082c81d5394fe7fbfcd8e9b933ff77db18781a2a32c4e9856b905fd0"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Supervised Learning",
    "Tools": "Tensorboard",
    "L1": "* Understand TensorBoard\u2019s purpose and interface\n* Visualize basic training metrics (loss, accuracy) from TensorFlow/Keras models\n* Launch TensorBoard locally and navigate scalar plots\n* Log simple metrics using tf.summary\n* Use TensorBoard with pre-built Keras callbacks",
    "L2": "* Track multiple experiments and compare model performance\n* Visualize histograms, distributions, and weights\n* Monitor learning rate schedules and training dynamics\n* Use TensorBoard for debugging model behavior\n* Integrate TensorBoard with custom training loops",
    "L3": "* Visualize embeddings and projector plots for model interpretability\n* Use TensorBoard for profiling performance (e.g., GPU/CPU usage)\n* Customize TensorBoard dashboards with advanced summaries\n* Integrate TensorBoard with cloud platforms (e.g., TensorBoard.dev, GCP)\n* Automate logging and monitoring in production ML pipelines",
    "hashId": "d1592e97e5a029cba258e5627b4127940dd940af83f536ed98ede93d780aff3a"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Unsupervised Learning",
    "Tools": "Python (scikit-learn, scipy, numpy, pandas, matplotlib, seaborn, umap-learn, hdbscan, matplotlib, seaborn, plotly)",
    "L1": "* Understand basic concepts: clustering, dimensionality reduction\n* Use pandas and numpy for data preparation\n* Visualize data distributions using matplotlib and seaborn\n* Apply basic clustering algorithms (e.g., KMeans via scikit-learn)\n* Perform simple PCA for dimensionality reduction\n* Use scipy for basic distance metrics and hierarchical clustering",
    "L2": "* Tune clustering models (e.g., silhouette score, elbow method)\n* Use umap-learn for non-linear dimensionality reduction\n* Apply hdbscan for density-based clustering\n* Visualize clusters and embeddings using plotly and seaborn\n* Handle high-dimensional data and preprocessing pipelines\n* Combine clustering with feature engineering for downstream tasks",
    "L3": "* Build custom clustering pipelines using scikit-learn and hdbscan\n* Optimize UMAP parameters for better separation and interpretability\n* Visualize complex cluster structures and embeddings interactively\n* Integrate unsupervised learning into semi-supervised or hybrid models\n* Use clustering for anomaly detection and segmentation\n* Automate unsupervised workflows and evaluate stability across datasets",
    "hashId": "0d8ebd153510e66237731f2358e46a621ff34718ec057ac4a71a26a7b418b3fa"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Unsupervised Learning",
    "Tools": "R (cluster, factoextra, caret, arules, ggplot2, shiny)",
    "L1": "* Understand basic unsupervised learning concepts (clustering, association rules)\n* Use cluster for basic clustering algorithms (e.g., K-means)\n* Visualize clusters using factoextra and ggplot2\n* Perform simple association rule mining using arules\n* Use caret for basic preprocessing and clustering workflows\n* Build basic interactive dashboards using shiny",
    "L2": "* Tune clustering models and evaluate with silhouette scores\n* Apply hierarchical clustering and visualize dendrograms\n* Use factoextra for PCA and clustering interpretation\n* Perform market basket analysis with arules (support, confidence, lift)\n* Integrate clustering workflows with caret pipelines\n* Build interactive cluster exploration tools using shiny",
    "L3": "* Customize clustering algorithms and distance metrics\n* Combine PCA with clustering for high-dimensional data\n* Visualize complex cluster structures and association networks\n* Automate unsupervised learning workflows in R\n* Deploy interactive clustering dashboards using advanced shiny features\n* Use unsupervised learning for segmentation, anomaly detection, and feature extraction",
    "hashId": "ccf098719c6a6d5b3b5f948910b2beae93fb072c92f5aeda5842ba13e64c401c"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Unsupervised Learning",
    "Tools": "Tableau",
    "L1": "* Understand Tableau interface and basic chart types\n* Connect to structured data sources (Excel, CSV, SQL)\n* Visualize clustering outputs (e.g., cluster labels, centroids)\n* Use filters and basic calculated fields to explore unsupervised results\n* Build simple dashboards to display PCA or clustering summaries",
    "L2": "* Create interactive dashboards with drill-downs and filters\n* Visualize dimensionality reduction outputs (e.g., PCA components)\n* Use calculated fields to interpret cluster metrics (e.g., silhouette score)\n* Integrate Tableau with Python/R via TabPy or Rserve for unsupervised models\n* Blend multiple data sources for comparative cluster analysis",
    "L3": "* Automate data refresh and dashboard updates for unsupervised workflows\n* Visualize complex embeddings (e.g., UMAP) and cluster structures interactively\n* Embed Tableau dashboards into web portals or internal tools\n* Collaborate using Tableau Server or Tableau Cloud for real-time monitoring\n* Use Tableau Prep for preprocessing and feature extraction before clustering",
    "hashId": "423cb64ab6620b50ee71a4af78a4f2691d783ca2c6b33e7227043c08eafd0a35"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Unsupervised Learning",
    "Tools": "Power BI",
    "L1": "* Understand Power BI interface and basic visualizations\n* Connect to structured data sources (Excel, CSV, SQL)\n* Display clustering results (e.g., cluster labels, centroids) in visuals\n* Use slicers and filters to explore unsupervised learning outputs\n* Create simple dashboards to show PCA or segmentation summaries",
    "L2": "* Use DAX to calculate and display cluster metrics (e.g., silhouette score)\n* Visualize dimensionality reduction outputs (e.g., PCA components)\n* Integrate Power BI with Python/R scripts for clustering and embeddings\n* Create interactive dashboards with drill-through and bookmarks\n* Use Power Query for preprocessing and shaping data for clustering",
    "L3": "* Automate data refresh and real-time updates for unsupervised workflows\n* Visualize complex embeddings (e.g., UMAP) and cluster structures interactively\n* Embed Power BI dashboards into web apps or internal portals\n* Collaborate and share dashboards via Power BI Service with access control\n* Monitor unsupervised model outputs and segmentation trends over time",
    "hashId": "b3678f265adaabb01b1e7a4c1171230b127978cf3f88c6b43840e39fa3a20d88"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Unsupervised Learning",
    "Tools": "Looker Studio",
    "L1": "* Understand Looker Studio interface and basic chart types\n* Connect to structured data sources (Google Sheets, CSV, BigQuery)\n* Display clustering outputs (e.g., cluster labels, group summaries)\n* Use filters and basic calculated fields to explore unsupervised results\n* Build simple dashboards to visualize PCA or segmentation summaries",
    "L2": "* Use calculated fields to interpret cluster metrics (e.g., silhouette score)\n* Visualize dimensionality reduction outputs (e.g., PCA components)\n* Create interactive dashboards with controls and drill-downs\n* Blend multiple data sources for comparative cluster analysis\n* Integrate Looker Studio with external tools (e.g., Python/R via APIs or connected sources)\n",
    "L3": "* Automate data refresh from cloud sources for real-time clustering updates\n* Visualize complex embeddings (e.g., UMAP) and cluster structures interactively\n* Embed dashboards into web portals or internal tools\n* Collaborate and share dashboards with access control and versioning\n* Monitor unsupervised model outputs and segmentation trends over time",
    "hashId": "ca33a4b0b40ca23949bed83d11d2f57dcaea626c92255dd1ba194c4bed318f84"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Selection & Evaluation",
    "Tools": "Python (scikit-learn, xgboost, lightgbm, statsmodels, optuna, mlxtend, shap\n)",
    "L1": "* Use scikit-learn for basic model training and evaluation (e.g., accuracy, confusion matrix)\n* Apply train/test split and cross-validation\n* Use statsmodels for statistical summaries and basic regression diagnostics\n* Visualize evaluation metrics using matplotlib and seaborn\n* Understand basic model comparison techniques",
    "L2": "* Use xgboost and lightgbm for advanced model training and evaluation\n* Apply grid search and randomized search for hyperparameter tuning\n* Use mlxtend for model stacking and voting ensembles\n* Evaluate models using precision, recall, F1-score, ROC-AUC\n* Visualize feature importance and learning curves\n* Use optuna for automated hyperparameter optimization",
    "L3": "* Implement custom scoring functions and evaluation pipelines\n* Use SHAP for model interpretability and feature impact analysis\n* Perform nested cross-validation and advanced resampling techniques\n* Compare multiple models using statistical tests (e.g., paired t-test, McNemar\u2019s test)\n* Automate model selection workflows with optuna and mlxtend\n* Visualize complex evaluation metrics and decision boundaries using plotly",
    "hashId": "34ebad6249e90afc9ab44ff24d0e2073f62f4cc97dce0df561b1f69d3cc0e7ec"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Selection & Evaluation",
    "Tools": "R (caret, mlr, e1071, glmnet, randomForest)",
    "L1": "* Use caret for basic model training and evaluation (e.g., accuracy, confusion matrix)\n* Apply train/test split and simple cross-validation\n* Use e1071 for basic classification models and evaluation\n* Train basic regression models using glmnet\n* Visualize model performance using base R and ggplot2\n* Train simple decision trees and random forests using randomForest",
    "L2": "* Use caret for grid search and resampling techniques\n* Apply mlr for structured model comparison and benchmarking\n* Tune hyperparameters for models like glmnet and randomForest\n* Evaluate models using precision, recall, F1-score, ROC-AUC\n* Use e1071 for SVM evaluation and parameter tuning\n* Visualize evaluation metrics and learning curves with ggplot2",
    "L3": "* Implement nested cross-validation and advanced resampling strategies\n* Use mlr for automated model selection and ensemble learning\n* Perform statistical comparison of models (e.g., paired t-tests)\n* Apply regularization techniques and interpret coefficients with glmnet\n* Use advanced diagnostics and feature importance from randomForest\n* Automate model evaluation workflows and reporting",
    "hashId": "bc632e51f770f16f6e0799c7e12486d5323a531ed7cf92f87b951c95510209f3"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Selection & Evaluation",
    "Tools": "Tableau",
    "L1": "* Understand Tableau interface and basic chart types\n* Connect to structured data sources (Excel, CSV, SQL)\n* Display basic model evaluation metrics (e.g., accuracy, confusion matrix)\n* Use filters and calculated fields to explore model outputs\n* Build simple dashboards to compare model predictions",
    "L2": "* Visualize advanced metrics (e.g., precision, recall, F1-score, ROC-AUC)\n* Create interactive dashboards with drill-downs and parameter controls\n* Use calculated fields to compare multiple models\n* Integrate Tableau with Python/R via TabPy or Rserve for dynamic evaluation\n* Blend multiple data sources for model benchmarking",
    "L3": "* Automate dashboard updates with real-time model performance data\n* Visualize complex evaluation outputs (e.g., SHAP values, lift charts)\n* Embed Tableau dashboards into web portals or internal tools\n* Collaborate using Tableau Server or Tableau Cloud for model monitoring\n* Use Tableau Prep for preprocessing and evaluation data shaping",
    "hashId": "2813f80e38ce3b7dad1e0e335b42b4dce294e9f50e71cb1d1bff79408ea75b42"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Selection & Evaluation",
    "Tools": "Power BI",
    "L1": "* Understand Power BI interface and basic visualizations\n* Connect to structured data sources (Excel, CSV, SQL)\n* Display basic model evaluation metrics (e.g., accuracy, confusion matrix)\n* Use filters, slicers, and basic DAX formulas to explore model outputs\n* Build simple dashboards to compare model predictions",
    "L2": "* Use DAX to calculate advanced metrics (e.g., precision, recall, F1-score)\n* Visualize ROC curves and classification performance\n* Create interactive dashboards with drill-through and bookmarks\n* Integrate Power BI with Python/R scripts for dynamic model evaluation\n* Use Power Query for preprocessing and shaping evaluation data",
    "L3": "* Automate dashboard updates with real-time model performance data\n* Visualize complex evaluation outputs (e.g., SHAP values, lift charts)\n* Embed Power BI dashboards into web apps or internal portals\n* Collaborate and share dashboards via Power BI Service with access control\n* Monitor deployed models and evaluation trends over time",
    "hashId": "2a47d3218c4a6a47fb635a4e244535d508d0d6312c9fd9959e081811c561da6c"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Selection & Evaluation",
    "Tools": "Looker Studio",
    "L1": "* Understand Looker Studio interface and basic chart types\n* Connect to structured data sources (Google Sheets, CSV, BigQuery)\n* Display basic model evaluation metrics (e.g., accuracy, confusion matrix)\n* Use filters and calculated fields to explore model outputs\n* Build simple dashboards to compare model predictions",
    "L2": "* Use calculated fields to visualize advanced metrics (e.g., precision, recall, F1-score)\n* Create interactive dashboards with controls and drill-downs\n* Blend multiple data sources for model benchmarking\n* Integrate Looker Studio with external tools (e.g., Python/R via APIs or connected sources)\n* Visualize ROC curves and classification performance summaries",
    "L3": "* Automate dashboard updates with real-time model performance data\n* Visualize complex evaluation outputs (e.g., SHAP values, lift charts)\n* Embed dashboards into web portals or internal tools\n* Collaborate and share dashboards with access control and versioning\n* Monitor deployed models and evaluation trends over time",
    "hashId": "f65fc34bf62a0728854788c6a2039d691f86ccc6ed1ff7d5d57eaf0d950fc4c1"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Selection & Evaluation",
    "Tools": "ML Flow",
    "L1": "* Understand MLflow components: Tracking, Projects, Models, Registry\n* Log basic parameters, metrics, and models during training\n* Use MLflow with simple models (e.g., scikit-learn, xgboost)\n* Run experiments locally and view results in MLflow UI\n* Save and load models using MLflow\u2019s built-in functions",
    "L2": "* Use MLflow Tracking to compare multiple model runs and metrics\n* Register models and manage versions in MLflow Model Registry\n* Integrate MLflow with hyperparameter tuning tools (e.g., Optuna)\n* Deploy models using MLflow\u2019s REST API or local serving\n* Track evaluation metrics like ROC-AUC, precision, recall, F1-score",
    "L3": "* Automate model selection workflows with MLflow and orchestration tools\n* Customize MLflow logging with tags, artifacts, and nested runs\n* Integrate MLflow with cloud platforms (e.g., AWS SageMaker, Azure ML)\n* Use MLflow for model governance, lifecycle management, and CI/CD\n* Monitor deployed models and evaluation metrics in production environments",
    "hashId": "52ef105b398eddf3a0567e6da89b7a942c16cb2799e5719ba6b46c0989956e43"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Feature Engineering",
    "Tools": "Python (pandas, numpy, scikit-learn, category_encoders, feature-engine, tsfresh, nltk, spacy)",
    "L1": "* Use pandas and numpy for basic data cleaning and transformation\n* Apply simple encoding techniques (e.g., label encoding, one-hot encoding)\n* Handle missing values and basic imputation\n* Extract basic features from text using nltk (e.g., token count, word frequency)\n* Use scikit-learn for scaling and normalization",
    "L2": "* Apply advanced encoding techniques using category_encoders (e.g., target, ordinal, binary encoding)\n* Use feature-engine for modular feature transformation pipelines\n* Perform time-based feature extraction (e.g., lag features, rolling stats)\n* Use tsfresh for automated time series feature extraction\n* Apply spacy for named entity recognition and part-of-speech tagging\n* Create interaction features and polynomial features using scikit-learn",
    "L3": "* Build custom transformers and integrate them into scikit-learn pipelines\n* Use tsfresh for domain-specific time series feature selection\n* Apply NLP-based feature engineering using spacy and nltk (e.g., sentiment, topic modeling)\n* Automate feature selection and transformation workflows\n* Evaluate feature importance using model-based techniques (e.g., SHAP, permutation importance)\n* Optimize feature engineering for deployment and scalability",
    "hashId": "3dd4ba33920f8c464852bad209bf55e301172eb1b2076a5f66e108ed8ad42a9e"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Feature Engineering",
    "Tools": "R (caret, dplyr, recipes, text2vec)",
    "L1": "* Use dplyr for basic data cleaning and transformation\n* Apply simple encoding techniques (e.g., one-hot, label encoding) using caret\n* Handle missing values and basic imputation\n* Create basic derived features (e.g., ratios, flags)\n* Use recipes for structured preprocessing workflows",
    "L2": "* Apply advanced feature transformations using recipes (e.g., normalization, interaction terms)\n* Use caret for preprocessing within model training pipelines\n* Perform text preprocessing and feature extraction using text2vec (e.g., TF-IDF, tokenization)\n* Engineer time-based features and categorical aggregations\n* Combine multiple preprocessing steps using tidy workflows",
    "L3": "* Build custom feature engineering steps using recipes and caret\n* Apply dimensionality reduction techniques (e.g., PCA) as part of feature pipelines\n* Use text2vec for advanced NLP features (e.g., word embeddings, topic modeling)\n* Automate feature selection and transformation workflows\n* Optimize feature engineering for deployment and reproducibility",
    "hashId": "123e5897a3591b60f2493e391964f1b3dc28aec530da11e7b15dc97ab95ff93f"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Feature Engineering",
    "Tools": "Tableau",
    "L1": "* Understand Looker Studio interface and basic chart types\n* Connect to structured data sources (Google Sheets, CSV, BigQuery)\n* Display basic engineered features (e.g., flags, ratios, categories)\n* Use filters and calculated fields to explore feature distributions\n* Build simple dashboards to visualize feature impact on model outputs",
    "L2": "* Create calculated fields for derived features (e.g., log transformations, binning)\n* Blend multiple data sources to enrich feature sets\n* Visualize relationships between features and target variables\n* Integrate Looker Studio with external tools (e.g., Python/R via APIs or connected sources)\n* Use interactive controls to explore feature subsets and their influence",
    "L3": "* Automate feature monitoring dashboards with real-time data updates\n* Visualize complex feature interactions and importance scores\n* Embed dashboards into web portals or internal tools for feature tracking\n* Collaborate and share dashboards with access control and versioning\n* Monitor feature drift and stability over time using dynamic visualizations",
    "hashId": "7f7a42580e7610cb4863362e40fbba37fedade3569b9e9ff3130ed1b55147562"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Feature Engineering",
    "Tools": "Power BI",
    "L1": "* Use Power BI interface to create basic calculated columns and measures\n* Perform simple transformations (e.g., ratios, flags, bins) using DAX\n* Use Power Query for basic data cleaning and formatting\n* Visualize feature distributions and relationships with basic charts\n* Build dashboards to display engineered features and their impact",
    "L2": "* Apply advanced DAX functions for feature creation (e.g., time-based, conditional logic)\n* Use Power Query for multi-step transformations and derived columns\n* Integrate Python/R scripts for external feature engineering workflows\n* Create interactive dashboards to explore feature relationships and segmentation\n* Blend multiple data sources to enrich feature sets",
    "L3": "* Automate feature monitoring dashboards with scheduled refreshes\n* Visualize complex feature interactions and importance scores\n* Track feature drift and stability over time using dynamic visuals\n* Embed dashboards into apps or portals for real-time feature tracking\n* Collaborate via Power BI Service for feature governance and sharing",
    "hashId": "dbcae1b4bc300a9421c6bcf2765b8e01765825e765320dd404ea1517a9d371d5"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Feature Engineering",
    "Tools": "Looker Studio",
    "L1": "* Use Looker Studio interface to create basic calculated fields (e.g., flags, ratios, bins)\n* Connect to structured data sources (Google Sheets, CSV, BigQuery)\n* Visualize simple feature distributions and relationships\n* Build dashboards to display engineered features and their impact on outcomes\n* Apply filters and date ranges to explore feature subsets",
    "L2": "* Create derived features using expressions (e.g., log transformations, conditional logic)\n* Blend multiple data sources to enrich feature sets\n* Visualize relationships between features and target variables\n* Use interactive controls to explore feature subsets and segmentation\n* Integrate Looker Studio with external tools (e.g., Python/R via APIs or connected sources)",
    "L3": "* Automate feature monitoring dashboards with scheduled refreshes\n* Visualize complex feature interactions and importance scores\n* Embed dashboards into web portals or internal tools for real-time feature tracking\n* Collaborate and share dashboards with access control and versioning\n* Monitor feature drift and stability over time using dynamic visualizations",
    "hashId": "500fed865c0fbcdb4c55cc02f564629747cbc8d0717d0b69cf421c3d8be81d3e"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Tuning & Optimization",
    "Tools": "Python (scikit-learn, xgboost, lightgbm, optuna, hyperopt, ray[tune], keras-tuner\n)",
    "L1": "* Use scikit-learn for basic hyperparameter tuning (e.g., GridSearchCV, RandomizedSearchCV)\n* Tune simple models like decision trees, logistic regression\n* Understand evaluation metrics and their role in tuning\n* Visualize tuning results using matplotlib and seaborn\n* Apply basic tuning to xgboost and lightgbm using default parameters\n",
    "L2": "* Use optuna and hyperopt for automated hyperparameter optimization\n* Apply Bayesian optimization and early stopping techniques\n* Tune ensemble models (xgboost, lightgbm) with custom parameter grids\n* Use keras-tuner for optimizing neural network architectures\n* Integrate tuning workflows with cross-validation and pipelines\n* Visualize optimization history and parameter importance",
    "L3": "* Use ray[tune] for distributed and scalable hyperparameter tuning\n* Implement custom objective functions and search spaces in optuna and hyperopt\n* Optimize deep learning models with complex architectures using keras-tuner\n* Combine multiple tuning strategies (e.g., Bayesian + grid search)\n* Automate tuning workflows in production ML pipelines\n* Monitor tuning performance and convergence using advanced visualizations",
    "hashId": "5ac25a08e65d3f4e2383cbda990d3e3be2cefe442fafbb9da9ee593b26c4412c"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Tuning & Optimization",
    "Tools": "R (caret, mlr, e1071, tune)",
    "L1": "* Use caret for basic hyperparameter tuning (e.g., grid search)\n* Tune simple models like decision trees, logistic regression\n* Apply basic resampling methods (e.g., k-fold cross-validation)\n* Use e1071 for tuning SVM parameters\n* Visualize tuning results using base R or ggplot2",
    "L2": "* Use mlr for structured tuning workflows and benchmarking\n* Apply advanced resampling strategies (e.g., repeated CV, bootstrapping)\n* Tune regularized models using glmnet with cross-validation\n* Use tune (from tidymodels) for flexible tuning and model comparison\n* Combine tuning with preprocessing pipelines using caret or recipes",
    "L3": "* Implement nested cross-validation for robust model evaluation\n* Use mlr or tune for automated and parallelized hyperparameter optimization\n* Customize search spaces and objective functions\n* Integrate tuning into reproducible ML workflows using tidymodels\n* Monitor tuning performance and convergence across multiple models",
    "hashId": "5384a204abcd085396d0416472b57e2bbdf4ee61d5bb94f317c49b72f6f1bec6"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Tuning & Optimization",
    "Tools": "MFlow",
    "L1": "* Understand MLflow\u2019s role in tracking model tuning experiments\n* Log basic hyperparameters and evaluation metrics during training\n* Use MLflow UI to compare tuning results across runs\n* Save and load tuned models using MLflow\u2019s built-in functions\n* Integrate MLflow with simple tuning workflows (e.g., GridSearchCV)",
    "L2": "* Track automated hyperparameter optimization runs (e.g., Optuna, Hyperopt) using MLflow\n* Use MLflow Projects to package and reproduce tuning workflows\n* Register tuned models and manage versions in MLflow Model Registry\n* Visualize tuning performance and parameter impact across multiple runs\n* Deploy tuned models using MLflow\u2019s REST API or local serving\n",
    "L3": "* Integrate MLflow with distributed tuning frameworks (e.g., Ray Tune)\n* Automate tuning workflows in CI/CD pipelines with MLflow tracking\n* Customize MLflow logging with nested runs, tags, and artifacts\n* Monitor tuning convergence and performance in production environments\n* Use MLflow with cloud platforms (e.g., AWS SageMaker, Azure ML) for scalable tuning\n",
    "hashId": "f1c288856226be3a491920a18ccd04daa19d404a77eeb3118a48f58129f6da4f"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Tuning & Optimization",
    "Tools": "Tensorboard (for deep learning models)",
    "L1": "* Understand TensorBoard\u2019s role in monitoring deep learning training\n* Use built-in Keras/TensorFlow callbacks to log training metrics (e.g., loss, accuracy)\n* Launch TensorBoard locally and navigate scalar plots\n* Visualize basic hyperparameter effects on model performance\n* Track training and validation metrics over epochs",
    "L2": "* Monitor multiple tuning runs and compare performance\n* Visualize learning rate schedules, weight distributions, and gradients\n* Use TensorBoard with custom training loops and tf.summary for flexible logging\n* Track hyperparameter tuning experiments using tools like keras-tuner or optuna with TensorBoard integration\n* Analyze overfitting and underfitting using training curves",
    "L3": "* Use TensorBoard\u2019s HParams dashboard for structured hyperparameter tuning analysis\n* Visualize embeddings and projector plots for model interpretability\n* Profile model performance (e.g., GPU/CPU usage, bottlenecks)\n* Integrate TensorBoard into automated tuning pipelines and CI/CD workflows\n* Monitor tuning convergence and optimization trends across distributed training setups",
    "hashId": "6583826f151e9c28d7ea2cae2f628b4b8e2caa0fdb81fa953f555d87d32a7923"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Machine Learning & Modeling",
    "Sub-Sub-Category": "Model Tuning & Optimization",
    "Tools": "R Markdown",
    "L1": "* Use R Markdown to document basic model tuning steps and results\n* Embed code chunks for simple grid search using caret or mlr\n* Display tuning metrics (e.g., accuracy, confusion matrix) in tables and plots\n* Format reports with headings, bullet points, and inline code\n* Knit documents to HTML, PDF, or Word for sharing",
    "L2": "* Document advanced tuning workflows using tune and tidymodels\n* Visualize hyperparameter search results using ggplot2 within R Markdown\n* Use parameterized reports to dynamically update tuning inputs\n* Combine multiple models and compare performance in structured layouts\n* Integrate interactive elements (e.g., DT tables, plotly charts)",
    "L3": "* Automate tuning documentation with dynamic R Markdown reports\n* Use R Markdown for reproducible model tuning pipelines\n* Embed results from parallelized or distributed tuning runs\n* Create dashboards using R Markdown + flexdashboard for real-time tuning insights\n* Integrate R Markdown with version control and CI/CD for model governance",
    "hashId": "7b5ab90b5fcb65a4f0363b09bb15c427b843cfd4aad2f6cedd1f195d267d1c0e"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Data Visualization",
    "Sub-Sub-Category": "See Data Visualization",
    "Tools": null,
    "L1": null,
    "L2": null,
    "L3": null,
    "hashId": "14a2d77104f215de5e7a3d77ffe8b18fa29eb0a42e2d18667cf7a3263f66de85"
  },
  {
    "Category": null,
    "Sub-Category": null,
    "Sub-Sub-Category": null,
    "Tools": null,
    "L1": null,
    "L2": null,
    "L3": null,
    "hashId": "cee21da146799dddc76352a2dccb518faead71723f20c0f4f05b3a0a10d00af5"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "Python (Flask, FastAPI, joblib, pickle, scikit-learn, tensorflow, torch)",
    "L1": "* Understand the concept of model deployment and its purpose\n* Differentiate between batch and real-time inference\n* Serialize models using joblib or pickle\n* Train and save simple models using scikit-learn\n* Build basic prediction APIs using Flask\n* Set up Python environments and manage dependencies (pip, requirements.txt)\n* Test APIs manually using tools like Postman or curl\nImplement basic logging for debugging",
    "L2": "* Develop RESTful APIs using Flask and FastAPI\n* Validate inputs and handle multiple endpoints\n* Load and serve models built with scikit-learn, TensorFlow, or PyTorch\n* Integrate preprocessing and postprocessing steps in APIs\n* Schedule batch jobs using tools like cron or Airflow\n* Implement asynchronous real-time inference with FastAPI\n* Containerize applications using Docker\n* Write and manage Dockerfiles\n* Implement structured logging and basic performance monitoring",
    "L3": "* Optimize API performance with asynchronous processing and FastAPI\n* Scale applications using Gunicorn, Nginx, or other load balancers\n* Secure APIs using OAuth, JWT, and input sanitization\n* Implement rate limiting and error handling\n* Manage model versioning and rollback strategies\n* Automate CI/CD pipelines using GitHub Actions, Jenkins, etc.\n* Monitor model performance using Prometheus, Grafana\n* Detect and respond to model drift\n* Deploy models on cloud platforms (AWS Lambda, SageMaker, GCP Vertex AI, Azure ML)\n* Use Kubernetes for orchestration and scaling\n* Integrate with streaming platforms like Kafka or RabbitMQ\n* Deploy models in edge or microservice environments",
    "hashId": "c332018e57f02602c41f110b1b25953eea8523fddff8a0f6801270501e25548f"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "R (Plumber, caret, mlr)",
    "L1": "* Understand the basics of model deployment and operationalization\n* Differentiate between batch and real-time deployment\n* Train and save models using caret or mlr\n* Serialize models using R\u2019s native save/load functions (saveRDS, readRDS)\n* Create simple REST APIs using Plumber\n* Test endpoints using tools like Postman or curl\n* Set up R environment and manage packages (install.packages, renv)",
    "L2": "* Build structured APIs with multiple endpoints using Plumber\n* Integrate preprocessing and postprocessing steps in API workflows\n* Deploy models trained with caret or mlr for real-time predictions\n* Schedule batch jobs using R scripts and task schedulers (e.g., cron, Windows Task Scheduler)\n* Containerize R applications using Docker\n* Implement basic logging and error handling in APIs\n* Monitor API performance (e.g., response time, throughput)",
    "L3": "* Optimize API performance and scalability with Plumber and Docker\n* Secure APIs with authentication and input validation\n* Implement model versioning and rollback strategies\n* Automate deployment workflows using CI/CD tools (e.g., GitHub Actions, Jenkins)\n* Monitor model performance and detect drift using custom metrics\n* Deploy models on cloud platforms (e.g., RStudio Connect, AWS EC2, Azure)\n* Integrate with real-time data pipelines or messaging systems (e.g., Kafka via R packages)\n* Use orchestration tools like Kubernetes for scalable deployment",
    "hashId": "fe64d6281ea4bbf25b3880dc4ccdc5497fe3b040b91d40a99d0d3b9b6ba96462"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "Docker , Kubernetes",
    "L1": "* Understand the purpose of containerization and orchestration\n* Install and set up Docker locally\n* Write basic Dockerfile to containerize a Python or R model API\n* Build and run Docker containers\n* Use basic Docker commands (docker build, docker run, docker ps)\n* Understand container lifecycle and image layers\n* Learn basic Kubernetes concepts: pods, nodes, clusters",
    "L2": "* Create multi-stage Dockerfiles for optimized builds\n* Use docker-compose for managing multi-container applications\n* Push and pull images from Docker Hub or private registries\n* Configure Kubernetes manifests (deployment.yaml, service.yaml)\n* Deploy containerized models to a local Kubernetes cluster (e.g., Minikube)\n* Set resource limits and environment variables in Kubernetes\n* Implement health checks and readiness probes\n* Use Kubernetes services for internal and external access",
    "L3": "* Automate Docker image builds and deployments using CI/CD pipelines\n* Use Helm charts for templated Kubernetes deployments\n* Scale deployments using Kubernetes Horizontal Pod Autoscaler\n* Implement rolling updates and rollback strategies\n* Secure containers and Kubernetes clusters (e.g., secrets, RBAC)\n* Monitor deployments using Prometheus, Grafana, or ELK stack\n* Integrate with cloud-native Kubernetes services (e.g., EKS, GKE, AKS)\n* Handle real-time data streams with Kubernetes-based microservices\n* Optimize resource utilization and cost in production environments",
    "hashId": "bb653d72a5f02a2b7deb49e6fbf4a21f12f469bd29783e2f4895fe17b2aad903"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "AWS Sagemaker",
    "L1": "* Understand the purpose and capabilities of AWS SageMaker\n* Train and deploy basic models using SageMaker Studio or notebook instances\n* Use built-in algorithms and pre-trained models\n* Save and load models using SageMaker\u2019s Estimator and Model classes\n* Deploy models to real-time endpoints using predictor.predict()\n* Perform batch inference using SageMaker Batch Transform\n* Manage basic IAM roles and permissions for SageMaker access",
    "L2": "* Customize training scripts and Docker containers for SageMaker\n* Use SageMaker Pipelines for automated workflows\n* Monitor deployed endpoints using CloudWatch\n* Implement model versioning and endpoint updates\n* Use SageMaker Experiments to track model performance\n* Integrate SageMaker with other AWS services (e.g., S3, Lambda, API Gateway)\n* Optimize model performance using hyperparameter tuning jobs\n* Deploy models using SageMaker SDK (boto3, sagemaker Python SDK)",
    "L3": "* Build and deploy custom containers using SageMaker Inference Toolkit\n* Implement multi-model endpoints for efficient resource usage\n* Use SageMaker Model Registry for lifecycle management\n* Automate CI/CD pipelines with SageMaker and AWS CodePipeline/CodeBuild\n* Monitor and detect model drift using SageMaker Model Monitor\n* Secure endpoints with VPC, encryption, and IAM policies\n* Scale deployments using autoscaling and multi-instance endpoints\n* Deploy models in hybrid or edge environments using SageMaker Edge Manager\n* Integrate with real-time data streams (e.g., Kinesis, Kafka) for live inference",
    "hashId": "73541db6890a7d607e2dd6d885de7bf701e1b6d61793f206bb508f246f18e17f"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "Azure ML",
    "L1": "* Understand the Azure ML ecosystem and its components\n* Use Azure ML Studio for basic model training and deployment\n* Train models using built-in algorithms or custom scripts\n* Register and deploy models to real-time endpoints\n* Perform batch inference using Azure ML pipelines\n* Manage workspaces, compute instances, and datasets\n* Use basic Azure ML SDK functions (azureml.core, azureml.train)",
    "L2": "* Build and deploy models using Azure ML SDK in Python\n* Create and manage reusable pipelines for training and inference\n* Use Azure ML Designer for drag-and-drop workflows\n* Monitor endpoint performance using Azure Application Insights\n* Implement model versioning and endpoint updates\n* Integrate Azure ML with other Azure services (e.g., Blob Storage, Key Vault, API Management)\n* Automate training and deployment using Azure ML Pipelines\n* Use AutoML for model selection and tuning",
    "L3": "* Build custom Docker environments for training and inference\n* Deploy models using Kubernetes (AKS) for scalable real-time inference\n* Implement CI/CD workflows using Azure DevOps or GitHub Actions\n* Use Azure ML Model Registry for lifecycle management\n* Monitor and detect model drift using Azure ML Data Drift Monitor\n* Secure endpoints with managed identities, VNETs, and role-based access control (RBAC)\n* Optimize resource usage and cost with compute scaling and spot instances\n* Integrate with real-time data streams (e.g., Event Hubs, IoT Hub)\n* Deploy models to edge devices using Azure IoT and Azure ML Edge",
    "hashId": "c9c535bd30f3eb9f2db8a99ddcec5ae71f78e803323b0288a34e68bca9a5238b"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "Google Vertex AI",
    "L1": "* Understand the purpose and components of Google Vertex AI\n* Use Vertex AI Workbench for model development and experimentation\n* Train models using AutoML or custom training jobs\n* Deploy models to real-time endpoints using Vertex AI Model Deployment\n* Perform batch predictions using Vertex AI Batch Prediction\n* Manage datasets and models in Vertex AI dashboard\n* Set up basic IAM roles and permissions for access control",
    "L2": "* Use Vertex AI SDK (google-cloud-aiplatform) for programmatic model deployment\n* Customize training containers and environments\n* Monitor deployed endpoints using Cloud Monitoring and Logging\n* Implement model versioning and endpoint updates\n* Integrate Vertex AI with other GCP services (e.g., BigQuery, Cloud Storage, Pub/Sub)\n* Automate workflows using Vertex AI Pipelines\n* Use Feature Store for consistent feature engineering across training and inference",
    "L3": "* Build and deploy custom containers for training and inference\n* Implement CI/CD pipelines using Cloud Build and GitHub Actions\n* Use Vertex AI Model Registry for lifecycle management\n* Monitor model performance and detect drift using Vertex AI Model Monitoring\n* Secure endpoints with VPC Service Controls, IAM policies, and encryption\n* Scale deployments using autoscaling and multi-region endpoints\n* Integrate with real-time data streams (e.g., Pub/Sub, Dataflow) for live inference\n* Deploy models to edge devices using Vertex AI Edge Manager\n* Optimize cost and performance using managed infrastructure and resource tuning",
    "hashId": "ead9c1e7a91c5c10ce085aa19a56bd4f5c2afd91a900521966263bc0cba0205d"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "TensorFlow Serving",
    "L1": "* Understand the purpose of TensorFlow Serving in model deployment\n* Export TensorFlow models in the SavedModel format\n* Install and run TensorFlow Serving locally using Docker\n* Serve a basic model and test predictions via REST or gRPC endpoints\n* Use curl or Postman to send inference requests\n* Understand the directory structure required for model versioning",
    "L2": "* Configure multiple models and versions in TensorFlow Serving\n* Use Docker Compose to manage TensorFlow Serving with other services\n* Optimize model loading and memory usage\n* Integrate TensorFlow Serving with Python clients for real-time inference\n* Monitor model performance and logs using Prometheus and Grafana\n* Secure endpoints with basic authentication and HTTPS\n* Automate model export and deployment workflows",
    "L3": "* Deploy TensorFlow Serving in production environments using Kubernetes\n* Implement rolling updates and canary deployments for model versions\n* Scale TensorFlow Serving with load balancers and autoscaling\n* Integrate with CI/CD pipelines for automated deployment\n* Use TensorFlow Serving with TensorFlow Extended (TFX) for end-to-end ML workflows\n* Implement advanced monitoring and alerting for model health and drift\n* Optimize inference latency and throughput for high-performance applications\n* Integrate with streaming platforms (e.g., Kafka, Pub/Sub) for real-time data processing",
    "hashId": "7b7621b8846a60f7bf610814282c94e032ca1e7d94b4e4e66751e92fde870d80"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "Apache Airflow",
    "L1": "* Understand the purpose of Apache Airflow in ML workflows and deployment\n* Install and configure Airflow locally (using pip or Docker)\n* Learn basic Airflow concepts: DAGs, tasks, operators, and scheduling\n* Create simple DAGs to automate ML tasks (e.g., data preprocessing, model training)\n* Trigger batch inference jobs using PythonOperator or BashOperator\n* Monitor DAG runs via Airflow UI",
    "L2": "* Use Airflow to orchestrate end-to-end ML pipelines (training \u2192 evaluation \u2192 deployment)\n* Integrate Airflow with model storage (e.g., S3, GCS) and databases\n* Schedule batch model inference jobs with retry logic and alerts\n* Use custom Python scripts and Docker containers within Airflow tasks\n* Implement branching and conditional logic in DAGs\n* Use Airflow connections and variables for dynamic configuration\n* Deploy Airflow on cloud platforms (e.g., AWS MWAA, GCP Composer)",
    "L3": "* Build modular and reusable DAGs for scalable ML deployment\n* Integrate Airflow with CI/CD pipelines for automated model deployment\n* Trigger real-time model deployment workflows using sensors and external triggers\n* Monitor model performance and drift using Airflow-integrated monitoring tools\n* Secure Airflow deployments with role-based access control and secrets management\n* Optimize Airflow performance with task parallelism and resource tuning\n* Use Airflow with KubernetesPodOperator for scalable containerized tasks\n* Integrate Airflow with ML platforms (e.g., SageMaker, Vertex AI, Azure ML) for hybrid workflows",
    "hashId": "1d5138a5251d7510e37588ec8114565635f8e2e7a4ff0e296145034a88938500"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Model Deployment (Batch/Real-time)",
    "Tools": "Luigi",
    "L1": "* Understand Luigi\u2019s role in workflow orchestration and batch processing\n* Install and configure Luigi locally\n* Learn basic Luigi concepts: Tasks, Targets, Parameters\n* Create simple Luigi pipelines for data preprocessing or model training\n* Run Luigi tasks from the command line\n* Monitor task execution using the Luigi web UI",
    "L2": "* Build multi-step ML workflows using Luigi (e.g., data ingestion \u2192 training \u2192 evaluation)\n* Use external targets (e.g., files, databases, S3) for task dependencies\n* Implement retry logic and task failure handling\n* Schedule batch inference jobs using Luigi\n* Modularize code for reusable and maintainable pipelines\n* Integrate Luigi with model storage and versioning systems\n* Use Luigi with Docker containers for isolated task environments",
    "L3": "* Scale Luigi workflows using distributed execution and remote workers\n* Integrate Luigi with CI/CD pipelines for automated deployment\n* Secure Luigi workflows with access control and secrets management\n* Monitor and log workflow performance and failures\n* Optimize pipeline execution with parallelism and resource management\n* Integrate Luigi with cloud services (e.g., AWS, GCP) for hybrid workflows\n* Use Luigi with other orchestration tools (e.g., Airflow, Kubeflow) in complex ML systems\n* Deploy models as part of end-to-end batch pipelines with automated retraining and evaluation",
    "hashId": "bc809bb445a608edd9adaadb20fdc001c624aed0b66d81e9760f14ca98e08507"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "Python(FLASK, Fast API, Pickle, Joblib, Pydantic, Marshmallow, ONNX, Docker, Kubernetes, )",
    "L1": "* Understand the basics of RESTful APIs and their role in model deployment\n* Build simple APIs using Flask or FastAPI to serve ML models\n* Serialize and deserialize models using Pickle or Joblib\n* Test API endpoints using Postman or curl\n* Use Pydantic or Marshmallow for basic input validation\n* Export models to ONNX format for interoperability\n* Containerize basic API applications using Docker\n* Learn basic Kubernetes concepts: pods, deployments, services",
    "L2": "* Develop structured APIs with multiple endpoints and input/output schemas\n* Use FastAPI for asynchronous request handling and performance optimization\n* Implement robust input validation and error handling using Pydantic or Marshmallow\n* Serve models in ONNX format for cross-framework compatibility\n* Create and manage Dockerfiles and use docker-compose for multi-container setups\n* Deploy containerized APIs to a local Kubernetes cluster (e.g., Minikube)\n* Monitor API performance and logs\n* Implement versioning for models and endpoints",
    "L3": "* Build scalable, production-grade APIs with FastAPI and advanced routing\n* Secure APIs using OAuth2, JWT, and HTTPS\n* Optimize model inference performance using ONNX Runtime\n* Automate CI/CD pipelines for API deployment using GitHub Actions or Jenkins\n* Deploy APIs to cloud-native Kubernetes clusters (e.g., EKS, GKE, AKS)\n* Implement autoscaling, rolling updates, and canary deployments in Kubernetes\n* Integrate APIs with real-time data streams (e.g., Kafka, RabbitMQ)\n* Monitor and alert on API health using Prometheus and Grafana\n* Use advanced Docker techniques (multi-stage builds, caching, secrets management)",
    "hashId": "9001253a6a31deb0b41fbe898464b4d9b423b2c3c23ef37c846cb0adc8e40179"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "R (Plumber, PMML, Postman, Swagger UI )",
    "L1": "* Understand the basics of RESTful APIs and their role in model deployment\n* Create simple APIs using Plumber to serve R models\n* Export models using PMML for standardized deployment\n* Test API endpoints using Postman\n* Use basic HTTP methods (GET, POST) in Plumber routes\n* Save and load models using R\u2019s native serialization (saveRDS, readRDS)\n* View and interact with Plumber API documentation via Swagger UI",
    "L2": "* Build multi-endpoint APIs with input validation and error handling\n* Integrate preprocessing and postprocessing logic into API workflows\n* Use PMML for model portability across platforms\n* Customize Plumber API documentation using Swagger UI\n* Secure APIs with basic authentication and HTTPS\n* Containerize Plumber APIs using Docker\n* Schedule batch inference jobs using R scripts and task schedulers\n* Log API requests and responses for monitoring\n",
    "L3": "* Deploy Plumber APIs in production using Docker and Kubernetes\n* Implement scalable and resilient API architectures\n* Integrate APIs with CI/CD pipelines for automated deployment\n* Use advanced input validation and schema enforcement\n* Monitor API performance and model drift using external tools\n* Secure APIs with OAuth2, token-based access, and encrypted communication\n* Deploy APIs to cloud platforms (e.g., RStudio Connect, AWS, Azure)\n* Integrate APIs with real-time data pipelines or messaging systems",
    "hashId": "44e76a3b91f9fb36b34eb0479642a427e20c8ff0de8051e05440cb1434ba8789"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "Azure Functions",
    "L1": "* Understand the concept of serverless computing and Azure Functions\n* Create basic HTTP-triggered Azure Functions using the Azure portal or VS Code\n* Deploy simple ML models using Azure Functions and Python/R scripts\n* Test endpoints using Postman or curl\n* Use local settings and environment variables for configuration\n* Understand function app structure and lifecycle",
    "L2": "* Develop and deploy Azure Functions using Azure CLI or GitHub Actions\n* Integrate Azure Functions with Azure ML for real-time model inference\n* Use bindings (e.g., Blob Storage, Cosmos DB) for input/output data\n* Implement input validation and error handling in function code\n* Secure functions using API keys and Azure Active Directory\n* Monitor function performance using Azure Application Insights\n* Handle batch inference using timer-triggered or queue-triggered functions",
    "L3": "* Build scalable APIs using durable functions and orchestrator patterns\n* Optimize cold start performance and resource allocation\n* Implement CI/CD pipelines for automated deployment and testing\n* Secure APIs with OAuth2, managed identities, and VNET integration\n* Integrate with real-time data streams (e.g., Event Hubs, Service Bus)\n* Use Azure Functions in hybrid workflows with Logic Apps and Azure ML Pipelines\n* Monitor and alert on function health, latency, and failures\n* Deploy models using ONNX or TensorFlow with GPU-enabled function plans",
    "hashId": "8f2a2b9c211ebb58863f0eb9545445aa0e497e3a4a2e094df813bc48de71227e"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "AWS Lambda",
    "L1": "* Understand the concept of serverless architecture and AWS Lambda\n* Create basic Lambda functions using the AWS Console or CLI\n* Trigger Lambda functions via HTTP using API Gateway\n* Deploy simple ML models using serialized formats (e.g., Pickle, Joblib)\n* Test endpoints using Postman or curl\n* Set up IAM roles and permissions for Lambda execution\n* Use environment variables for configuration",
    "L2": "* Package and deploy Lambda functions using AWS SAM or Serverless Framework\n* Integrate Lambda with other AWS services (e.g., S3, DynamoDB, SageMaker)\n* Handle input validation and error responses in Lambda code\n* Optimize cold start performance and memory usage\n* Use layers to manage dependencies and model files\n* Monitor function performance using CloudWatch Logs and Metrics\n* Implement batch inference using scheduled triggers (e.g., CloudWatch Events)",
    "L3": "* Build scalable APIs using Lambda + API Gateway with throttling and caching\n* Secure APIs using Cognito, OAuth2, and custom authorizers\n* Automate CI/CD pipelines for Lambda deployment using CodePipeline or GitHub Actions\n* Use Lambda with ONNX or TensorFlow Lite for optimized inference\n* Implement asynchronous workflows using Step Functions\n* Monitor and alert on model performance and drift\n* Deploy models in hybrid architectures (e.g., Lambda + SageMaker endpoints)\n* Optimize cost and performance with provisioned concurrency and reserved capacity",
    "hashId": "c70870382ce9b935cd6cfa12f2b4ea452fb5ead9c7602d0ee2c08a7c0c251870"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "Google Cloud Functions",
    "L1": "* Understand the basics of serverless architecture and Google Cloud Functions\n* Create simple HTTP-triggered functions using the GCP Console or Cloud Shell\n* Deploy basic ML models using serialized formats (e.g., Pickle, Joblib)\n* Test endpoints using Postman or curl\n* Set up IAM roles and permissions for function access\n* Use environment variables for configuration\n* Understand function lifecycle and cold start behavior",
    "L2": "* Develop and deploy functions using the gcloud CLI or Cloud Functions Framework\n* Integrate Cloud Functions with other GCP services (e.g., Cloud Storage, BigQuery, Vertex AI)\n* Handle input validation and structured responses\n* Use Pub/Sub or Cloud Scheduler for batch and event-driven inference\n* Monitor function performance using Cloud Logging and Cloud Monitoring\n* Secure endpoints using API keys and IAM policies\n* Package dependencies and models efficiently for deployment",
    "L3": "* Build scalable APIs using Cloud Functions + API Gateway\n* Secure APIs with OAuth2, Firebase Auth, and VPC Service Controls\n* Automate CI/CD pipelines using Cloud Build and GitHub Actions\n* Optimize performance with memory tuning and concurrency settings\n* Use ONNX or TensorFlow Lite for lightweight inference in serverless environments\n* Implement asynchronous workflows using Cloud Tasks or Workflows\n* Monitor model drift and performance with custom metrics and alerts\n* Integrate with real-time data streams (e.g., Pub/Sub, Dataflow) for live inference\n* Deploy hybrid workflows combining Cloud Functions with Vertex AI and other GCP services",
    "hashId": "b694ac6f6cc9355fd26add59378aa4937ed11914be1b3502f0aa2641b10074c7"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "ELK Stack (Elasticsearch, Logstash, Kibana)",
    "L1": "* Understand the purpose of the ELK Stack in logging and monitoring APIs\n* Install and configure basic ELK components locally or via Docker\n* Send simple logs from API applications to Logstash\n* Store and search logs in Elasticsearch\n* Visualize basic metrics and logs in Kibana\n* Use standard logging formats (JSON, plain text) for ingestion\n* Connect Flask/FastAPI apps to ELK using basic logging handlers",
    "L2": "* Configure Logstash pipelines for structured log ingestion and transformation\n* Use Elasticsearch queries to filter and analyze API logs\n* Create custom dashboards in Kibana for API performance monitoring\n* Implement structured logging in Python using logging and loguru\n* Monitor API metrics like response time, error rate, and request volume\n* Secure ELK Stack with basic authentication and role-based access\n* Integrate ELK with Dockerized API deployments",
    "L3": "* Deploy and scale ELK Stack using Kubernetes and Helm charts\n* Implement centralized logging for microservices and distributed APIs\n* Use Filebeat or Metricbeat for lightweight log and metric shipping\n* Set up alerting in Kibana for anomalies and failures in API behavior\n* Optimize Elasticsearch indexing and query performance\n* Secure ELK Stack with TLS, API keys, and advanced access controls\n* Integrate ELK with CI/CD pipelines for automated monitoring\n* Correlate logs across services for end-to-end traceability in real-time deployments",
    "hashId": "1c5b78143eb140877c9c85b755cf9bd4ffbaabb4309760c971309f297c188118"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "API Development",
    "Tools": "MLFlow",
    "L1": "* Understand the purpose of MLflow in model tracking and deployment\n* Install and configure MLflow locally\n* Log basic model parameters, metrics, and artifacts using MLflow Tracking\n* Serve models using mlflow models serve for local REST API\n* Register models manually in the MLflow Model Registry\n* Test MLflow endpoints using Postman or curl\n* Use MLflow with common libraries like scikit-learn, XGBoost, TensorFlow",
    "L2": "* Automate model logging and registration during training workflows\n* Use MLflow Projects to package and reproduce ML code\n* Deploy models to cloud environments using MLflow\u2019s built-in support (e.g., AWS SageMaker)\n* Implement model versioning and stage transitions (Staging \u2192 Production)\n* Secure MLflow APIs with authentication and access control\n* Integrate MLflow with CI/CD pipelines for automated deployment\n* Use MLflow with Docker for containerized model serving\n* Monitor model performance and update models based on feedback",
    "L3": "* Deploy MLflow in production using remote tracking servers and databases\n* Scale MLflow with Kubernetes and cloud-native storage (e.g., S3, GCS, Azure Blob)\n* Customize MLflow model flavors and inference logic\n* Integrate MLflow with orchestration tools (e.g., Airflow, Kubeflow, Azure ML Pipelines)\n* Implement advanced monitoring and alerting for deployed models\n* Use MLflow with ONNX or custom model formats for cross-platform compatibility\n* Secure MLflow with enterprise-grade identity management (OAuth2, LDAP)\n* Track and manage experiments across distributed teams and environments",
    "hashId": "27ca1fa16809d2b2f4fa9807ca4fb1eb2ae33d307d373cad8781d6f5d72c9941"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "Python (Evidently AI, MLFlow, DVC, Airflow, Prometheus & Grafana, Kubeflow)",
    "L1": "* Understand the importance of monitoring and retraining in ML lifecycle\n* Use Evidently AI for basic data and model drift detection\n* Log model metrics and parameters using MLflow Tracking\n* Track data and model versions using DVC\n* Schedule simple retraining workflows using Airflow\n* Visualize basic metrics using Grafana connected to Prometheus\n* Learn basic concepts of Kubeflow pipelines and components",
    "L2": "* Set up automated drift detection and alerts using Evidently AI\n* Use MLflow Model Registry for managing model stages and versions\n* Implement reproducible pipelines with DVC and Git integration\n* Build modular DAGs in Airflow for retraining and evaluation workflows\n* Monitor API and model performance metrics using Prometheus & Grafana\n* Deploy and manage ML workflows using Kubeflow Pipelines\n* Integrate monitoring tools with deployed models for real-time feedback",
    "L3": "* Automate retraining triggers based on drift or performance thresholds\n* Implement CI/CD pipelines for retraining using MLflow, DVC, and Airflow\n* Use Evidently AI in production for continuous monitoring and reporting\n* Scale monitoring infrastructure using Prometheus Operator and Grafana dashboards\n* Secure and optimize retraining workflows in Kubeflow with resource management\n* Integrate Kubeflow with cloud-native services (e.g., GCP, AWS, Azure)\n* Track lineage and reproducibility across data, models, and pipelines\n* Implement governance and audit trails for retraining decisions",
    "hashId": "8639a0dcf9638cffaa1796397c63093df6b5ff6bd466fdffabc9f65d90a2a3d0"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "R (DVC via CLI, MLFLow, Prometheus & Grafana)",
    "L1": "* Understand the importance of monitoring and retraining in ML workflows\n* Use MLflow to log basic model metrics, parameters, and artifacts from R\n* Track model versions and stages using MLflow Model Registry\n* Use DVC via CLI to version datasets and models alongside R scripts\n* Set up basic Prometheus and Grafana dashboards for system-level monitoring\n* Learn how to export metrics from R scripts to Prometheus-compatible formats",
    "L2": "* Automate logging and tracking of experiments using MLflow in R\n* Use DVC to manage data and model dependencies in collaborative workflows\n* Schedule retraining workflows using R scripts and external schedulers (e.g., cron, Airflow)\n* Integrate Prometheus with R APIs (e.g., Plumber) for real-time metric collection\n* Build custom Grafana dashboards to visualize model performance and drift\n* Monitor API latency, error rates, and throughput using Prometheus metrics\n",
    "L3": "* Implement automated retraining pipelines triggered by drift detection or performance thresholds\n* Use MLflow with remote tracking servers and cloud storage for scalable experiment management\n* Integrate DVC with CI/CD pipelines for reproducible retraining workflows\n* Secure and optimize monitoring infrastructure with Prometheus Operator and Grafana provisioning\n* Track lineage and reproducibility across data, models, and retraining cycles\n* Deploy monitoring and retraining workflows in cloud or hybrid environments\n* Use advanced alerting in Grafana for proactive model maintenance",
    "hashId": "fc02dd0b53ee5c93128743da69205d34556c73bd65a79e6e3c5898ec26d8f3a6"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "GitHub Actions",
    "L1": "* Understand the role of GitHub Actions in automating ML workflows\n* Create basic workflows using .github/workflows/*.yml files\n* Automate tasks like code linting, testing, and model packaging\n* Trigger workflows on push, pull request, or schedule\n* Use pre-built actions for Python environment setup and dependency installation\n* Store secrets and environment variables securely in GitHub",
    "L2": "* Automate model training and evaluation pipelines using GitHub Actions\n* Integrate with tools like MLflow, DVC, and Airflow for experiment tracking and versioning\n* Use matrix builds to test models across multiple environments or datasets\n* Trigger retraining workflows based on data or code changes\n* Deploy models to cloud platforms (e.g., AWS, Azure, GCP) using GitHub Actions\n* Monitor workflow status and logs for debugging and optimization\n* Use caching and artifacts to speed up CI/CD processes",
    "L3": "* Implement full CI/CD pipelines for model retraining and deployment\n* Integrate with Prometheus & Grafana for monitoring workflow performance\n* Automate drift detection and retraining triggers using GitHub Actions + Evidently AI\n* Use custom Docker containers and self-hosted runners for scalable execution\n* Secure workflows with fine-grained permissions and audit trails\n* Orchestrate multi-step workflows across multiple repositories and environments\n* Combine GitHub Actions with Kubeflow for hybrid cloud-native ML operations\n* Implement rollback strategies and version control for deployed models",
    "hashId": "552e0343d8feb220f71c453d6c938d09a2d2842d6e181e8ae53af32a8d917fe9"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "AWS SageMaker",
    "L1": "* Understand the importance of monitoring and retraining in ML lifecycle\n* Use SageMaker Studio or notebooks to log model metrics and parameters\n* Register models in SageMaker Model Registry\n* Perform manual retraining and redeployment of models\n* Use CloudWatch for basic monitoring of endpoint performance\n* Schedule retraining jobs using SageMaker Processing Jobs or cron-based triggers",
    "L2": "* Automate model monitoring using SageMaker Model Monitor\n* Detect data drift and quality issues using built-in monitoring tools\n* Use SageMaker Pipelines to automate retraining workflows\n* Track model versions, lineage, and metadata using Model Registry\n* Integrate with CloudWatch Logs and Metrics for detailed monitoring\n* Trigger retraining based on performance thresholds or drift detection\n* Use SageMaker Experiments to compare model performance across runs",
    "L3": "* Implement continuous monitoring and retraining pipelines using SageMaker Pipelines + Model Monitor\n* Use EventBridge or Lambda to automate retraining triggers\n* Secure monitoring and retraining workflows with IAM, VPC, and KMS\n* Scale retraining jobs using SageMaker Training with Spot Instances\n* Integrate with external tools (e.g., MLflow, DVC, Airflow) for hybrid workflows\n* Monitor real-time inference metrics and detect anomalies using CloudWatch Alarms\n* Deploy retrained models with rollback and version control strategies\n* Use SageMaker Clarify for bias and explainability monitoring in production",
    "hashId": "f40a8c7d7fcacd5cb42ef3755b1088ec43e20d65934368cb191b0094a48c6875"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "Apache Airflow",
    "L1": "* Understand the role of Apache Airflow in orchestrating ML workflows\n* Install and configure Airflow locally or via Docker\n* Create simple DAGs to automate tasks like data preprocessing or model training\n* Use basic operators (e.g., PythonOperator, BashOperator) for ML tasks\n* Schedule periodic retraining jobs using Airflow\u2019s built-in scheduler\n* Monitor DAG execution status via the Airflow UI",
    "L2": "* Build modular DAGs for end-to-end ML workflows (training \u2192 evaluation \u2192 deployment)\n* Integrate Airflow with tools like MLflow, DVC, or S3 for model tracking and storage\n* Implement conditional logic and branching in DAGs for retraining decisions\n* Use Sensors and Trigger Rules to automate retraining based on external events\n* Log and monitor model performance metrics within Airflow tasks\n* Secure Airflow with role-based access control and environment isolation\n* Use Airflow with cloud services (e.g., AWS, GCP) for scalable retraining workflows",
    "L3": "* Automate drift detection and retraining triggers using Airflow + monitoring tools (e.g., Evidently AI, Prometheus)\n* Implement CI/CD pipelines for retraining using Airflow + GitHub Actions or Jenkins\n* Use KubernetesPodOperator or DockerOperator for scalable, containerized retraining jobs\n* Integrate Airflow with Kubeflow Pipelines or Vertex AI for hybrid orchestration\n* Monitor and alert on retraining workflow failures and performance bottlenecks\n* Track lineage and reproducibility across data, models, and retraining cycles\n* Optimize DAG performance with parallelism, task queues, and resource tuning\n* Secure retraining workflows with secrets management and audit logging",
    "hashId": "505a03c3c0d458f0efb2e5df729a9e0317e0bcfc192a9d4754d1f8dc9670ef17"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "Google Vertex AI",
    "L1": "* Understand the role of Vertex AI in managing ML lifecycle\n* Use Vertex AI Workbench for basic model training and logging\n* Register models in Vertex AI Model Registry\n* Perform manual retraining and redeployment of models\n* Monitor endpoint performance using Cloud Monitoring\n* Schedule retraining jobs using Cloud Scheduler or notebooks",
    "L2": "* Use Vertex AI Model Monitoring to detect data drift and anomalies\n* Automate retraining workflows using Vertex AI Pipelines\n* Track model versions, lineage, and metadata using Model Registry\n* Integrate with BigQuery, Cloud Storage, and Pub/Sub for data ingestion\n* Monitor real-time metrics and logs using Cloud Logging and Cloud Monitoring\n* Trigger retraining based on performance thresholds or drift detection\n* Use Vertex AI Experiments to compare model performance across runs",
    "L3": "* Implement continuous monitoring and retraining pipelines using Vertex AI Pipelines + Model Monitoring\n* Use Eventarc or Cloud Functions to automate retraining triggers\n* Secure workflows with IAM, VPC Service Controls, and KMS\n* Scale retraining jobs using custom containers and distributed training\n* Integrate with external tools (e.g., MLflow, DVC, Airflow) for hybrid workflows\n* Monitor real-time inference metrics and detect anomalies using custom dashboards\n* Deploy retrained models with rollback and version control strategies\n* Use Vertex AI Explainability and Bias Detection for responsible retraining",
    "hashId": "f04ff7bec731b0c27c9a44d0b2467807061e164f4c5c374a8122ed4695b59b61"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "Monitoring & Retraining",
    "Tools": "Jenkins",
    "L1": "* Understand the role of Jenkins in automating ML workflows\n* Install and configure Jenkins locally or via Docker\n* Create basic Jenkins jobs to run ML scripts (e.g., training, evaluation)\n* Trigger jobs manually or on schedule (e.g., daily retraining)\n* Use Jenkins to monitor job status and logs\n* Store credentials and environment variables securely in Jenkins",
    "L2": "* Automate model training and retraining pipelines using Jenkins\n* Integrate Jenkins with tools like MLflow, DVC, and GitHub\n* Use Jenkins pipelines (Jenkinsfile) for modular and reusable workflows\n* Trigger retraining based on data changes or performance thresholds\n* Deploy models to staging or production environments using Jenkins jobs\n* Monitor pipeline performance and failures using Jenkins dashboards\n* Use Jenkins with cloud services (e.g., AWS, GCP, Azure) for scalable retraining",
    "L3": "* Implement full CI/CD pipelines for monitoring, retraining, and deployment\n* Integrate Jenkins with Prometheus & Grafana for real-time monitoring and alerting\n* Automate drift detection and retraining triggers using Jenkins + Evidently AI\n* Use Jenkins with Kubernetes for scalable, containerized retraining workflows\n* Secure Jenkins pipelines with role-based access control and audit logging\n* Orchestrate multi-repo and multi-environment workflows across teams\n* Optimize pipeline performance with parallel execution and resource management\n* Integrate Jenkins with Kubeflow, Airflow, or Vertex AI for hybrid ML operations",
    "hashId": "2393604a8b1cce10793d92831547fea8755556309cebfc67a151379abe821050"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "GitHub",
    "L1": "* Understand the basics of CI/CD and its importance in ML workflows\n* Use GitHub for version control of ML code, data scripts, and notebooks\n* Create simple GitHub Actions workflows to automate testing and linting\n* Trigger workflows on code push, pull requests, or scheduled intervals\n* Store secrets and environment variables securely in GitHub\n* Use pre-built actions for setting up Python environments and installing dependencies",
    "L2": "* Automate model training, evaluation, and packaging using GitHub Actions\n* Use matrix builds to test models across multiple environments or datasets\n* Integrate with tools like MLflow, DVC, or Docker for experiment tracking and reproducibility\n* Deploy models to staging environments or cloud platforms (e.g., AWS, Azure, GCP)\n* Use caching and artifacts to optimize workflow performance\n* Monitor workflow status, logs, and failures via GitHub UI\n* Implement branching strategies and version control for models and pipelines",
    "L3": "* Build full CI/CD pipelines for ML workflows including training, testing, deployment, and monitoring\n* Automate retraining triggers based on drift detection or performance thresholds\n* Integrate GitHub Actions with Airflow, Kubeflow, or Vertex AI for hybrid orchestration\n* Use custom Docker containers and self-hosted runners for scalable execution\n* Secure workflows with fine-grained permissions, audit trails, and compliance checks\n* Implement rollback strategies and model governance in deployment pipelines\n* Monitor pipeline health and performance using external tools (e.g., Prometheus, Grafana)\n* Coordinate multi-repo and multi-environment deployments across teams",
    "hashId": "2e96f04589133691e6e519b6e8869cfd37b08e2c71b8369af92da50ce32b2091"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "Gitlab",
    "L1": "* Understand the basics of CI/CD and GitLab\u2019s role in automating ML workflows\n* Use GitLab for version control of ML code, data scripts, and notebooks\n* Create simple .gitlab-ci.yml files to automate testing and linting\n* Trigger pipelines on code push, merge requests, or scheduled intervals\n* Store secrets and environment variables securely using GitLab CI/CD variables\n* Use shared runners for basic pipeline execution",
    "L2": "* Automate model training, evaluation, and packaging in GitLab pipelines\n* Use Docker containers in GitLab CI for consistent environments\n* Integrate with tools like MLflow, DVC, or S3 for experiment tracking and artifact management\n* Deploy models to staging environments or cloud platforms (e.g., AWS, GCP, Azure)\n* Use caching and artifacts to optimize pipeline performance\n* Monitor pipeline status, logs, and failures via GitLab UI\n* Implement branching strategies and version control for models and workflows",
    "L3": "* Build full CI/CD pipelines for ML workflows including training, testing, deployment, and monitoring\n* Automate retraining triggers based on drift detection or performance thresholds\n* Use GitLab Runners with Kubernetes for scalable, containerized ML workflows\n* Secure pipelines with fine-grained permissions, audit logs, and compliance policies\n* Integrate GitLab CI/CD with Airflow, Kubeflow, or Vertex AI for hybrid orchestration\n* Implement rollback strategies and model governance in deployment pipelines\n* Monitor pipeline health and performance using external tools (e.g., Prometheus, Grafana)\n* Coordinate multi-repo and multi-environment deployments across teams",
    "hashId": "c85a45ef4bfc83bb53f6070d2f96f84e5f2b59b31e2718a25670d52e545cfad6"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "Jenkins",
    "L1": "* Understand the basics of CI/CD and Jenkins' role in automating ML workflows\n* Install and configure Jenkins locally or via Docker\n* Create simple Jenkins jobs to run ML scripts (e.g., training, testing)\n* Trigger jobs manually or on schedule (e.g., daily retraining)\n* Use Jenkins UI to monitor job status and logs\n* Store credentials and environment variables securely in Jenkins",
    "L2": "* Use Jenkins Pipelines (Jenkinsfile) to automate ML workflows\n* Integrate Jenkins with tools like MLflow, DVC, and GitHub/GitLab\n* Automate model training, evaluation, packaging, and deployment\n* Use Jenkins with Docker containers for consistent environments\n* Trigger retraining workflows based on data or performance changes\n* Deploy models to staging or production environments (e.g., AWS, GCP, Azure)\n* Monitor pipeline performance and failures using Jenkins dashboards",
    "L3": "* Build full CI/CD pipelines for ML workflows including monitoring, retraining, and rollback\n* Use Jenkins with Kubernetes for scalable, containerized ML operations\n* Secure pipelines with role-based access control, audit logging, and secrets management\n* Integrate Jenkins with Prometheus & Grafana for real-time monitoring and alerting\n* Automate drift detection and retraining triggers using Jenkins + tools like Evidently AI\n* Coordinate multi-repo and multi-environment deployments across teams\n* Implement governance and compliance checks in ML deployment pipelines\n* Integrate Jenkins with Kubeflow, Airflow, or Vertex AI for hybrid orchestration",
    "hashId": "291d510d07efd0e5e6256b944219a3163925a362c3bac3d254949f05fb4906cd"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "Azure DevOps Pipelines",
    "L1": "* Understand the basics of CI/CD and Azure DevOps Pipelines\n* Use Azure Repos for version control of ML code and data scripts\n* Create simple YAML pipelines for tasks like testing and linting\n* Trigger pipelines on code commits, pull requests, or scheduled intervals\n* Use built-in tasks for Python environment setup and dependency installation\n* Store secrets and environment variables securely using Azure Key Vault or pipeline variables",
    "L2": "* Automate model training, evaluation, and packaging in Azure Pipelines\n* Use Docker containers for consistent ML environments\n* Integrate with tools like MLflow, DVC, or Azure ML for tracking and deployment\n* Deploy models to staging environments or Azure ML endpoints\n* Use pipeline caching and artifacts to optimize performance\n* Monitor pipeline status, logs, and failures via Azure DevOps UI\n* Implement branching strategies and version control for models and workflows",
    "L3": "* Build full CI/CD pipelines for ML workflows including monitoring, retraining, and rollback\n* Automate retraining triggers based on drift detection or performance thresholds\n* Use Azure DevOps Pipelines with Kubernetes and Azure ML Pipelines for scalable orchestration\n* Secure pipelines with role-based access control, audit logging, and compliance policies\n* Integrate with Prometheus & Grafana or Application Insights for real-time monitoring\n* Implement governance, model approval workflows, and rollback strategies\n* Coordinate multi-repo and multi-environment deployments across teams\n* Use templates and reusable pipeline components for enterprise-scale ML operations",
    "hashId": "e3705574cb890e2cdd5ead6bb1fda5c14d22685a57779a5b4cc1fff694e19f0a"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "TensorFlow Serving",
    "L1": "* Understand the role of TensorFlow Serving in deploying ML models\n* Export models in SavedModel format for serving\n* Use tensorflow_model_server to serve models locally\n* Test REST and gRPC endpoints using curl or Postman\n* Write basic shell scripts to automate model serving setup\n* Understand versioning structure in TensorFlow Serving directories",
    "L2": "* Automate model export and deployment using shell scripts or Python\n* Use Docker to containerize TensorFlow Serving setups\n* Integrate TensorFlow Serving with CI/CD tools like GitHub Actions, GitLab CI, or Jenkins\n* Monitor model serving logs and performance metrics\n* Implement model versioning and dynamic model loading\n* Use Kubernetes to deploy TensorFlow Serving in scalable environments\n* Trigger model updates based on new training outputs",
    "L3": "* Build full CI/CD pipelines for training, validation, export, and deployment using TensorFlow Serving\n* Integrate with MLflow, DVC, or Airflow for experiment tracking and reproducibility\n* Automate rollback and canary deployments for model updates\n* Secure TensorFlow Serving endpoints with HTTPS and authentication\n* Monitor and alert on model performance using Prometheus & Grafana\n* Optimize inference latency and throughput using batching and hardware acceleration\n* Deploy TensorFlow Serving in hybrid cloud/on-prem environments with Kubernetes\n* Use TensorFlow Serving with TFX for end-to-end ML lifecycle automation",
    "hashId": "85f45e9a6464b3c8e5cab7b76a999ffb72bfc025e349150cd658e339511c91de"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "Pytest",
    "L1": "* Understands the concept of CI/CD and its relevance in ML workflows.\n* Familiar with basic Pytest usage for unit testing.\n* Can write and execute simple test cases for ML code.\n* Uses basic tools like Git and shell scripts for version control and automation.",
    "L2": "* Sets up CI pipelines using tools like GitHub Actions or Jenkins.\n* Writes Pytest scripts for validating data, models, and edge cases.\n* Understands model versioning and reproducibility in ML projects.\n* Uses Docker and Pytest fixtures to enhance testing and deployment.\n",
    "L3": "* Automates testing for data pipelines and model training workflows.\n* Integrates Pytest with CI/CD tools for continuous validation of ML models.\n* Applies mocking, parametrization, and advanced Pytest features.\n* Works with tools like MLflow, DVC, and basic Kubernetes for scalable deployment.\n",
    "hashId": "a7dbfa46d73b62938d9198191830eca9fa09302992679950b264dc8ab29d16d2"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "Seldon Core",
    "L1": "* Understands the purpose of CI/CD in ML workflows.\n* Can deploy basic ML models using Seldon Core with minimal configuration.\n* Familiar with containerization (Docker) and basic Kubernetes concepts.\n* Able to write simple Pytest scripts for model input/output validation.",
    "L2": "* Builds automated CI/CD pipelines integrating Seldon Core for model deployment.\n* Uses Pytest for testing model performance, data integrity, and API endpoints.\n* Applies version control and reproducibility practices using tools like DVC or MLflow.\n* Configures Seldon Core with custom inference graphs and REST/gRPC endpoints.\n",
    "L3": "* Designs scalable, secure, and resilient CI/CD pipelines for ML models using Seldon Core.\n* Implements advanced Pytest features (mocking, fixtures, parametrization) for robust testing.\n* Integrates monitoring and logging tools (Prometheus, Grafana) with Seldon deployments.\n* Leads deployment strategy across teams, ensuring compliance and rollback capabilities.",
    "hashId": "0514d4345e2e3708925a25998142e0dda49a70f307340b06a52f4e432238c1ae"
  },
  {
    "Category": "Data Science",
    "Sub-Category": "Deployment & Operationalization",
    "Sub-Sub-Category": "CI/CD for ML",
    "Tools": "MLFlow",
    "L1": "* Understands the basics of CI/CD and its role in ML workflows.\n* Can use MLFlow for basic experiment tracking and model logging.\n* Familiar with version control (Git) and containerization (Docker).\n* Able to run simple Pytest tests to validate model inputs/outputs.",
    "L2": "* Builds CI/CD pipelines integrating MLFlow for model versioning and deployment.\n* Uses MLFlow to track experiments, manage model registry, and automate deployment.\n* Applies Pytest for testing data pipelines, model performance, and API endpoints.\n* Works with tools like GitHub Actions, Jenkins, and DVC for reproducibility and automation.",
    "L3": "* Designs scalable CI/CD pipelines using MLFlow for production-grade ML systems.\n* Implements advanced Pytest features (mocking, fixtures, parametrization) for robust testing.\n* Integrates MLFlow with cloud platforms (AWS/GCP/Azure) and orchestration tools (Airflow, Kubeflow).\n* Leads deployment strategy, ensuring monitoring, rollback, and compliance mechanisms are in place.",
    "hashId": "5d778b6ac79e9ff6363f60ab40398013b9b2dd368fbd594d98fa75d254562eaf"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": null,
    "Sub-Sub-Category": null,
    "Tools": "Python, scikit-learn, TensorFlow, PyTorch, Keras",
    "L1": null,
    "L2": null,
    "L3": null,
    "hashId": "674ed9d34f6a0f901550744cf735984eb144eabeff3cfa5884a36e9bf633e6e8"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "understanding of supervised, unsupervised, and reinforcement learning",
    "Tools": "Python(scikit - learn, TensorFlow, Keras, PyTorch, XGBoost, LightGBM, OpenAI Gym, Stabke baseline 3)",
    "L1": "* Understands basic concepts of supervised, unsupervised, and reinforcement learning.\n* Can implement simple models using scikit-learn (e.g., linear regression, k-means).\n* Familiar with Python syntax and basic data handling using pandas and NumPy.\n* Has explored basic environments in OpenAI Gym for reinforcement learning.",
    "L2": "* Applies supervised and unsupervised learning using TensorFlow, Keras, and PyTorch.\n* Understands hyperparameter tuning, model evaluation, and cross-validation.\n* Uses XGBoost and LightGBM for structured data problems.\n* Can train and evaluate reinforcement learning agents using Stable Baselines3 in OpenAI Gym environments.",
    "L3": "* Designs and optimizes deep learning architectures for complex tasks using TensorFlow or PyTorch.\n* Implements custom training loops and loss functions.\n* Applies ensemble methods and advanced boosting techniques with XGBoost and LightGBM.\n* Develops and fine-tunes reinforcement learning agents using Stable Baselines3, including custom environments and reward shaping.",
    "hashId": "daa05ddda3c050f27c1af97bd1007df0fef4c5be94897333ab0347cc6215fcf4"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "Neural networks and deep learning basics",
    "Tools": "Python (tensorflow, Keras, PyTorch, scikit-learn, OpenCV, Hugging Face Transformers, FastAI, Matplotlib, Seaborn, Plotly)",
    "L1": "* Understands the structure and function of basic neural networks (e.g., perceptron, feedforward).\n* Can build simple models using Keras or TensorFlow for classification tasks.\n* Uses scikit-learn for preprocessing and basic ML workflows.\n* Visualizes data using Matplotlib, Seaborn, or Plotly.\n* Has basic exposure to OpenCV for image data handling.",
    "L2": "* Builds and trains deep learning models using TensorFlow, Keras, and PyTorch.\n* Understands concepts like activation functions, loss functions, backpropagation, and optimization.\n* Applies FastAI for rapid prototyping and transfer learning.\n* Uses OpenCV for image preprocessing and augmentation.\n* Begins working with Hugging Face Transformers for NLP tasks.",
    "L3": "* Designs custom neural network architectures using PyTorch or TensorFlow.\n* Implements advanced techniques like regularization, dropout, batch normalization, and learning rate scheduling.\n* Fine-tunes pre-trained models using Hugging Face Transformers for domain-specific tasks.\n* Uses FastAI for efficient experimentation and deployment.\n* Applies OpenCV for complex image processing and computer vision pipelines.\n* Creates interactive visualizations and dashboards using Plotly and Seaborn.",
    "hashId": "c1bb6366ee00399aa7024ffe5b0c2bb277133c6d98dad9e0977a0d8bf89cfaec"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "Neural networks and deep learning basics",
    "Tools": "R (keras (R interface to TensorFlow), Deep learning in R, torch (R interface to PyTorch), Neural networks in R, caret, nnet, Basic neural networks",
    "L1": "* Understands basic neural network concepts (e.g., perceptron, layers, activation functions).\n* Can build simple neural networks using nnet or caret for classification tasks.\n* Uses keras in R for basic deep learning models with pre-defined layers.\n* Familiar with data preprocessing and visualization in R.",
    "L2": "* Builds and trains deep learning models using keras and torch in R.\n* Understands backpropagation, loss functions, and optimization techniques.\n* Applies model evaluation techniques like cross-validation and confusion matrices.\n* Uses R packages for handling image and tabular data in deep learning workflows.\n",
    "L3": "* Designs custom neural network architectures using torch in R.\n* Implements advanced techniques like dropout, batch normalization, and learning rate scheduling.\n* Fine-tunes models and integrates deep learning workflows with other R packages.\n* Applies deep learning to real-world tasks such as image classification, NLP, and time series forecasting using R interfaces.",
    "hashId": "e935d697e38304055eff48e638b1ea113159e7c325098dff8b96826eed6d6a87"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "Neural networks and deep learning basics",
    "Tools": "Google Colab",
    "L1": "* Understands basic neural network concepts (e.g., layers, activation functions, forward pass).\n* Can run pre-written Python notebooks in Google Colab.\n* Uses Colab for basic model training using libraries like Keras, TensorFlow, or scikit-learn.\n* Familiar with uploading datasets and using Colab\u2019s GPU/TPU runtime.",
    "L2": "* Writes and modifies deep learning code in Colab using TensorFlow, PyTorch, and Keras.\n* Uses Colab for data preprocessing, visualization, and model evaluation.\n* Applies Colab features like mounting Google Drive, using environment variables, and managing dependencies.\n* Trains models on larger datasets using Colab\u2019s GPU acceleration.",
    "L3": "* Designs and trains custom neural network architectures in Colab.\n* Implements advanced techniques like transfer learning, fine-tuning, and hyperparameter optimization.\n* Integrates Colab with external APIs, datasets, and cloud storage for scalable workflows.\n* Uses Colab for collaborative development, version control (via Git), and experiment tracking.",
    "hashId": "012e3a4e6509e26badd196391c35f0111e596ea9660990d639b6fa44ba4f7bcd"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "Model evaluation and tuning",
    "Tools": "Python (scikit-learn, xgboost, lightgbm, optuna, hyperopt, ray[tune], keras-tuner, Matplotlib, Seaborn, Plotly)\n)",
    "L1": "* Understands basic evaluation metrics (accuracy, precision, recall, F1-score, RMSE).\n* Uses scikit-learn for model training and evaluation.\n* Visualizes performance using Matplotlib and Seaborn.\n* Can manually adjust basic hyperparameters (e.g., learning rate, depth).\n",
    "L2": "* Applies advanced metrics (ROC-AUC, confusion matrix, log loss) for model evaluation.\n* Uses xgboost, lightgbm, and keras-tuner for structured data and neural networks.\n* Implements hyperparameter tuning using optuna and hyperopt.\n* Creates interactive visualizations with Plotly for model insights.",
    "L3": "* Designs automated hyperparameter tuning workflows using ray[tune], optuna, and hyperopt.\n* Combines multiple models and tuning strategies for ensemble learning.\n* Evaluates models across multiple datasets and cross-validation folds.\n* Integrates tuning and evaluation into CI/CD pipelines for scalable ML workflows.",
    "hashId": "45a1f1486186412e7a3c669e77a4ab4742a25c7a17afa58ba1085adf878ecfab"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "Model evaluation and tuning",
    "Tools": "R (caret, mlr, e1071, glmnet, Model training, tuning, and evaluation, ggplot2, plotly, yardstick, Visualization and metrics )",
    "L1": "* Understands basic evaluation metrics (accuracy, precision, recall, RMSE).\n* Uses caret and nnet for basic model training and evaluation.\n* Visualizes model performance using ggplot2 and plotly.\n* Performs manual tuning of simple models (e.g., adjusting parameters in glmnet or e1071).",
    "L2": "* Applies cross-validation and grid search using caret and mlr.\n* Uses yardstick for detailed metric computation and comparison.\n* Implements regularized models (e.g., Lasso, Ridge) using glmnet.\n* Creates interactive and comparative visualizations using plotly.",
    "L3": "* Designs automated tuning workflows using mlr and custom resampling strategies.\n* Combines multiple models and tuning strategies for ensemble learning.\n* Evaluates models across multiple datasets and folds with advanced metrics.\n* Integrates visualization and evaluation into reproducible R workflows for scalable ML pipelines.",
    "hashId": "e422a4a477a8462bab01d24db7b20c165561d03f3b0d74ec73f478a36d3dda25"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Foundations of AI & Machine Learning",
    "Sub-Sub-Category": "Model evaluation and tuning",
    "Tools": "Google Colab",
    "L1": "* Understands basic evaluation metrics (accuracy, precision, recall, RMSE).\n* Runs pre-written notebooks in Google Colab for model training and evaluation.\n* Uses Colab to visualize metrics using libraries like Matplotlib, Seaborn, and Plotly.\n* Familiar with uploading datasets and using Colab\u2019s GPU runtime.",
    "L2": "* Writes and modifies Python code in Colab for model evaluation using scikit-learn, xgboost, and lightgbm.\n* Implements hyperparameter tuning using tools like optuna, hyperopt, and keras-tuner.\n* Uses Colab features like Drive integration, environment setup, and interactive widgets.\n* Visualizes tuning results and performance comparisons using advanced plotting libraries.\n",
    "L3": "* Designs automated tuning workflows using ray[tune], optuna, and hyperopt in Colab.\n* Integrates Colab notebooks with external APIs, datasets, and experiment tracking tools.\n* Uses Colab for collaborative development, version control (Git), and scalable ML experimentation.\n* Builds reusable and modular notebooks for model evaluation pipelines.",
    "hashId": "4c9f1932aea0a63592751acef329cd3197d2b7e704fe5019d98928c359187f61"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "NLP",
    "Sub-Sub-Category": "Text preprocessing and tokenization",
    "Tools": "Python (NLTK, spaCy, Transformers (Hugging Face), TextBlob, Gensim)",
    "L1": "* Understands basic NLP concepts like tokenization, stopword removal, and stemming.\n* Uses NLTK and TextBlob for simple text preprocessing tasks.\n* Can tokenize text, remove punctuation, and convert to lowercase.\n* Familiar with basic usage of spaCy for rule-based tokenization.",
    "L2": "* Applies advanced preprocessing techniques like lemmatization, POS tagging, and named entity recognition using spaCy.\n* Uses Gensim for text vectorization (e.g., TF-IDF, Word2Vec).\n* Begins working with Transformers for tokenization of pre-trained models (e.g., BERT).\n* Handles multilingual text and custom tokenization pipelines.",
    "L3": "* Designs custom tokenization workflows using Transformers and spaCy.\n* Integrates preprocessing with model training pipelines for tasks like sentiment analysis, classification, and summarization.\n* Optimizes tokenization for large-scale datasets and real-time applications.\n* Applies domain-specific preprocessing strategies and builds reusable NLP components.",
    "hashId": "30fb82407981f3720e1d82e5f5d8bc74db44841826491deb1b10ba7a44f85a3a"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "NLP",
    "Sub-Sub-Category": "Text preprocessing and tokenization",
    "Tools": "R (tm (Text Mining), Text cleaning and preprocessing,  tidytext, Tokenization and tidy NLP workflows, Tokenization and embeddings ",
    "L1": "* Understands basic text preprocessing steps: lowercasing, punctuation removal, stopword filtering.\n* Uses the tm package for basic text cleaning and tokenization.\n* Familiar with simple workflows for converting raw text into structured formats (e.g., document-term matrices).",
    "L2": "* Applies tidytext for tidy NLP workflows including tokenization, stemming, and lemmatization.\n* Uses caret and nnet for integrating text features into machine learning models.\n* Begins working with word embeddings and vectorization techniques.\n* Visualizes token frequency and distribution using ggplot2 and plotly.",
    "L3": "* Designs custom tokenization and embedding pipelines using tidytext and external embedding models.\n* Implements advanced preprocessing strategies for domain-specific NLP tasks.\n* Integrates text preprocessing with model training and evaluation workflows.\n* Builds scalable and reproducible NLP pipelines using R\u2019s functional and tidyverse tools.",
    "hashId": "1bffbca9ad16bfa1510f3a90cf372b249ca4c497ba04cf0e8ab428409318ee91"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "NLP",
    "Sub-Sub-Category": "Embeddings (Word2Vec, BERT, etc.)",
    "Tools": "Python (Gensim, SpaCy, Transformers (Hugging Face), sentence-transformers, TensorFlow, Keras, PyTorch, scikit-learn)",
    "L1": "* Understands the concept of word embeddings and their role in NLP.\n* Uses Gensim to generate basic Word2Vec embeddings.\n* Applies pre-trained embeddings from spaCy or Transformers for simple tasks.\n* Familiar with vector representations and similarity measures.",
    "L2": "* Fine-tunes pre-trained models like BERT using Transformers and sentence-transformers.\n* Uses TensorFlow, Keras, or PyTorch to integrate embeddings into custom NLP models.\n* Applies embeddings for downstream tasks like classification, clustering, and semantic search.\n* Evaluates embedding quality using visualization and similarity metrics.",
    "L3": "* Designs custom embedding models using deep learning frameworks (TensorFlow, PyTorch).\n* Implements domain-specific fine-tuning and transfer learning with Transformers.\n* Builds scalable embedding pipelines for large corpora and real-time applications.\n* Integrates embeddings into advanced NLP systems (e.g., retrieval-augmented generation, semantic search engines).",
    "hashId": "675612f255740bc2f970d6c77a4d6a70117320e3474452acadade4cd4b68b242"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "NLP",
    "Sub-Sub-Category": "Embeddings (Word2Vec, BERT, etc.)",
    "Tools": "R (text2vec, Word2Vec, GloVe embeddings, text, Embedding generation and manipulation)",
    "L1": "* Understands the concept of word embeddings and their role in NLP.\n* Uses text2vec to generate basic Word2Vec or GloVe embeddings.\n* Applies simple tokenization and preprocessing before embedding generation.\n* Visualizes word similarities and basic vector operations.",
    "L2": "* Applies pre-trained embeddings and integrates them into NLP models.\n* Uses text and text2vec for embedding manipulation and downstream tasks (e.g., classification, clustering).\n* Evaluates embedding quality using similarity metrics and visualizations.\n* Begins working with sentence-level embeddings and dimensionality reduction techniques.",
    "L3": "* Designs custom embedding pipelines using text2vec and external embedding sources.\n* Fine-tunes embeddings for domain-specific corpora.\n* Integrates embeddings into complex NLP workflows (e.g., semantic search, topic modeling).\n* Builds scalable and reproducible embedding workflows in R for large datasets.",
    "hashId": "06baf40196638a5e0e15813aa1f7bdb0530b11b44c8da483196c0fe010974ede"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "NLP",
    "Sub-Sub-Category": "Embeddings (Word2Vec, BERT, etc.)",
    "Tools": "Google Colab\u00a0",
    "L1": "* Understands the concept of word embeddings and their role in NLP tasks.\n* Runs pre-written notebooks in Google Colab using libraries like Gensim, spaCy, and Transformers.\n* Loads and applies pre-trained embeddings (e.g., Word2Vec, GloVe, BERT) for basic text similarity tasks.\n* Uses Colab\u2019s GPU runtime for faster embedding computation.",
    "L2": "* Fine-tunes pre-trained models (e.g., BERT) using Transformers and sentence-transformers in Colab.\n* Implements embedding workflows using TensorFlow, Keras, or PyTorch.\n* Visualizes embeddings using dimensionality reduction techniques (e.g., PCA, t-SNE).\n* Uses Colab features like Drive integration and interactive widgets for managing experiments.",
    "L3": "* Designs custom embedding models and training pipelines in Colab.\n* Implements domain-specific fine-tuning and embedding generation for large corpora.\n* Integrates embeddings into downstream NLP tasks (e.g., semantic search, classification, summarization).\n* Uses Colab for collaborative development, version control (Git), and scalable experimentation with embeddings.",
    "hashId": "2a54d66dc4234f6551c3eedadc7610761d9a96ab955e0875a153f0e8c9b7ae23"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "NLP",
    "Sub-Sub-Category": "Embeddings (Word2Vec, BERT, etc.)",
    "Tools": "RStudio,\u00a0R Markdown",
    "L1": "* Understands the concept of word embeddings and their role in NLP.\n* Uses RStudio to run basic embedding workflows using packages like text2vec.\n* Documents simple embedding generation and visualization in R Markdown.\n* Applies pre-trained embeddings (e.g., GloVe) for basic similarity tasks.",
    "L2": "* Implements Word2Vec and GloVe embeddings using text2vec and custom tokenization.\n* Uses R Markdown to create reproducible reports with embedded code, outputs, and visualizations.\n* Begins integrating embeddings into downstream tasks like classification or clustering.\n* Applies dimensionality reduction techniques (e.g., PCA, t-SNE) for embedding visualization.",
    "L3": "* Designs and documents complex embedding pipelines in RStudio using text2vec, text, and external models.\n* Fine-tunes embeddings for domain-specific corpora and tasks.\n* Builds interactive and dynamic NLP reports using R Markdown with plotly, ggplot2, and embedded widgets.\n* Integrates embeddings into scalable NLP workflows and reproducible research pipelines.",
    "hashId": "47e200d524322ad34e21155931a06c3f4b6cf04b32a4439c958eb541f6b4e5cb"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "Language models (GPT, BERT, LLaMA, Claude)",
    "Tools": "JavaScript (BERT, GPT-2 (in-browser inference))",
    "L1": "* Understands basic concepts of generative language models (e.g., GPT, BERT).\n* Uses pre-trained models like GPT-2 or BERT in-browser via JavaScript libraries (e.g., transformers.js, bert.js).\n* Can run simple inference tasks like text generation or classification in the browser.\n* Familiar with loading models and tokenizing input text using JavaScript.",
    "L2": "* Implements interactive NLP applications (e.g., chatbots, summarizers) using GPT-2 or BERT in-browser.\n* Optimizes model loading and inference performance for browser environments.\n* Integrates JavaScript-based models with front-end frameworks (e.g., React, Vue).\n* Begins experimenting with quantized or distilled versions of models for faster inference.",
    "L3": "* Designs custom generative model workflows using JavaScript and WebAssembly for efficient browser-based inference.\n* Fine-tunes or adapts models offline and deploys them for in-browser use.\n* Integrates multiple models (e.g., GPT + BERT) for hybrid tasks like retrieval-augmented generation.\n* Builds scalable, privacy-preserving generative AI applications entirely in-browser.",
    "hashId": "9727464f3fae0c90283e1905ffcc6b055fa8d738e18ee8cb5775152f1a4b438c"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "Language models (GPT, BERT, LLaMA, Claude)",
    "Tools": "Python (GPT, Claude, LLaMA (via APIs), SBERT (Sentence-BERT), BERT, T5, Claude, GPT-3, GPT-4, LLaMA, LLaMA 2)",
    "L1": "* Understands basic concepts of transformer-based language models (e.g., GPT, BERT).\n* Uses pre-trained models via simple Python interfaces like transformers (Hugging Face).\n* Runs basic inference tasks (e.g., text generation, classification) using models like GPT-2, BERT, or T5.\n* Familiar with loading models and tokenizers using transformers and sentence-transformers",
    "L2": "* Fine-tunes models like BERT, GPT-2, or T5 on custom datasets using PyTorch or TensorFlow/Keras.\n* Uses SBERT for sentence-level embeddings and semantic similarity tasks.\n* Integrates APIs for models like Claude, GPT-3, and LLaMA for advanced NLP tasks.\n* Applies models to real-world use cases like summarization, Q&A, and sentiment analysis.",
    "L3": "* Designs and trains custom generative models using PyTorch or TensorFlow.\n* Fine-tunes large-scale models (e.g., LLaMA 2, GPT-4) using distributed training and optimization techniques.\n* Implements retrieval-augmented generation (RAG), prompt engineering, and multi-model orchestration.\n* Builds scalable, production-ready NLP systems using APIs (Claude, GPT) and open-source models (LLaMA, BERT).",
    "hashId": "d6f321d83dd6cb94bedd182f0738a70dad2cf3cc0308378bf2855b598685dd48"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "Diffusion models (for image generation)",
    "Tools": "Python (Diffusers (by Hugging Face), Stable Diffusion, CompVis, InvokeAI, AUTOMATIC1111, OpenAI DALL\u00b7E, Generative Adversarial Networks (GANs), Keras, PyTorch, TensorFlow)",
    "L1": "* Understands the basic concept of diffusion models and generative adversarial networks (GANs).\n* Uses pre-trained models like Stable Diffusion or DALL\u00b7E via simple Python scripts or web interfaces.\n* Runs basic image generation tasks using Hugging Face\u2019s diffusers library.\n* Familiar with basic Python and model inference workflows.",
    "L2": "* Implements image generation pipelines using Stable Diffusion via diffusers, InvokeAI, or AUTOMATIC1111.\n* Fine-tunes pre-trained models on custom datasets using PyTorch or TensorFlow.\n* Applies GANs for structured image generation tasks using Keras or PyTorch.\n* Uses APIs and local deployments for models like DALL\u00b7E and CompVis.",
    "L3": "* Designs and trains custom diffusion models and GAN architectures from scratch.\n* Optimizes model performance using techniques like classifier-free guidance, latent diffusion, and prompt engineering.\n* Integrates image generation models into scalable applications (e.g., creative tools, design automation).\n* Builds hybrid pipelines combining GANs, diffusion models, and transformers for multimodal generation.",
    "hashId": "15fcde2443370f88f3848c44854452b4e659e1fa7201a4d15913d468b3e08ed2"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "Diffusion models (for image generation)",
    "Tools": "RunwayML",
    "L1": "* Understands the concept of diffusion models and their role in generative image tasks.\n* Uses RunwayML\u2019s web interface to generate images from text prompts using models like Stable Diffusion.\n* Familiar with basic prompt crafting and model selection within RunwayML.\n* Can export generated images and use them in simple creative projects.\n",
    "L2": "* Customizes generation settings (e.g., resolution, style, seed) in RunwayML.\n* Uses RunwayML\u2019s workflow builder to chain models and automate creative tasks.\n* Integrates RunwayML with external tools (e.g., Figma, Adobe tools) for design workflows.\n* Begins experimenting with fine-tuning or uploading custom assets for personalized outputs.",
    "L3": "* Designs complex generative pipelines using RunwayML\u2019s advanced features and API.\n* Fine-tunes diffusion models or uses custom-trained models within RunwayML.\n* Builds interactive applications or creative tools powered by RunwayML\u2019s backend.\n* Collaborates across teams using RunwayML for scalable, cloud-based generative workflows.\n",
    "hashId": "4759bd4221fcf632bcd5f2c4ce2e552c178477cb497cb1b41be7cb62882b4784"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "Diffusion models (for image generation)",
    "Tools": "Google Colab",
    "L1": "* Understands the concept of diffusion models and their role in generative image tasks.\n* Runs pre-built notebooks in Google Colab using models like Stable Diffusion or DALL\u00b7E.\n* Uses Colab\u2019s GPU runtime to accelerate image generation.\n* Familiar with basic prompt input and model configuration in Colab notebooks.",
    "L2": "* Customizes Colab notebooks to modify generation parameters (e.g., guidance scale, steps, resolution).\n* Uses Hugging Face\u2019s diffusers library in Colab for controlled image generation.\n* Integrates Colab with Google Drive for dataset access and output storage.\n* Begins experimenting with model variants (e.g., CompVis, InvokeAI) and prompt engineering.",
    "L3": "* Fine-tunes diffusion models in Colab using custom datasets and training loops.\n* Implements advanced workflows including latent diffusion, inpainting, and style transfer.\n* Combines diffusion models with other generative techniques (e.g., GANs, transformers) in Colab.\n* Builds modular, reusable Colab notebooks for scalable and collaborative generative AI projects.",
    "hashId": "b01db8d6d668b44eb30b2e7e1c7c0083c7fa99204014dc0f494342cab92777a5"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "GANs (Generative Adversarial Networks)",
    "Tools": "Weights & Biases",
    "L1": "* Understands the basic architecture of GANs (generator vs. discriminator).\n* Uses Weights & Biases to log training metrics (e.g., loss curves) for simple GAN models.\n* Can visualize generated images and training progress in W&B dashboards.\n* Familiar with integrating W&B into basic PyTorch or TensorFlow GAN scripts.",
    "L2": "* Implements custom GAN architectures and tracks hyperparameters using W&B.\n* Uses W&B\u2019s sweeps feature for automated hyperparameter tuning.\n* Logs model checkpoints, image outputs, and training diagnostics.\n* Collaborates with team members using shared W&B projects and reports.",
    "L3": "* Designs complex GAN workflows (e.g., conditional GANs, StyleGAN) with full W&B integration.\n* Uses W&B for experiment tracking across multiple runs, datasets, and configurations.\n* Builds dashboards for real-time monitoring and comparison of GAN variants.\n* Integrates W&B with CI/CD pipelines for scalable and reproducible GAN development.",
    "hashId": "79a4472f60b5a4bae6e736edfa6cf4b48af054a5b4a718af64a75c11f234f7dc"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "GANs (Generative Adversarial Networks)",
    "Tools": "MLFlow",
    "L1": "* Understands the basic structure of GANs (generator and discriminator).\n* Uses MLFlow to log simple metrics like loss values during GAN training.\n* Tracks basic model parameters and outputs (e.g., generated images) in MLFlow.\n* Familiar with integrating MLFlow into Python scripts for experiment tracking.",
    "L2": "* Implements custom GAN architectures and uses MLFlow to log hyperparameters, metrics, and artifacts.\n* Uses MLFlow\u2019s UI to compare training runs and visualize model performance.\n* Applies MLFlow for tracking model versions and storing generated outputs.\n* Begins using MLFlow\u2019s Projects and Models features for reproducible GAN workflows.\n",
    "L3": "* Designs scalable GAN training pipelines with full MLFlow integration (including automated logging, model registry, and deployment).\n* Uses MLFlow for hyperparameter tuning, experiment orchestration, and collaborative tracking.\n* Integrates MLFlow with cloud platforms or CI/CD pipelines for production-grade GAN development.\n* Builds dashboards and reports using MLFlow for monitoring and sharing GAN performance across teams.\n",
    "hashId": "ed708601eb8ba1574bf26840a35d9757c84c240b01d02a77a75fb997c21b6231"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "GANs (Generative Adversarial Networks)",
    "Tools": "Python (TensorFlow, Keras, PyTorch, TorchGAN, TF-GAN, NVIDIA StyleGAN, StyleGAN2, StyleGAN3, CycleGAN, Pix2Pix, \tOpenCV, Matplotlib, Seaborn)",
    "L1": "* Understands the basic architecture of GANs (generator vs. discriminator).\n* Implements simple GANs using Keras or PyTorch for tasks like image generation.\n* Visualizes training outputs and loss curves using Matplotlib or Seaborn.\n* Uses OpenCV for basic image preprocessing and display.",
    "L2": "* Builds and trains advanced GAN variants like CycleGAN and Pix2Pix for image-to-image translation.\n* Uses TorchGAN or TF-GAN for modular GAN development.\n* Applies data augmentation and preprocessing using OpenCV.\n* Evaluates GAN performance using metrics like FID and visual inspection.\n",
    "L3": "* Implements and fine-tunes state-of-the-art GANs like StyleGAN2, StyleGAN3, and custom architectures.\n* Optimizes GAN training using techniques like progressive growing, adaptive discriminator augmentation, and mixed precision.\n* Integrates GANs into production pipelines for tasks like content creation, image enhancement, and domain adaptation.\n* Builds scalable GAN workflows with experiment tracking, visualization, and deployment tools.",
    "hashId": "57678e6db8d04d7315e593223c7b3570fb08bd73f2c61621f95098d8a3c230d1"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Generative Modeling Techniques",
    "Sub-Sub-Category": "GANs (Generative Adversarial Networks)",
    "Tools": "Google Colab ",
    "L1": "* Understands the basic structure of GANs (generator and discriminator).\n* Runs pre-built GAN notebooks in Google Colab using TensorFlow, Keras, or PyTorch.\n* Uses Colab\u2019s GPU runtime for faster training and image generation.\n* Visualizes outputs and training metrics using Matplotlib or Seaborn.",
    "L2": "* Modifies GAN architectures and training parameters in Colab notebooks.\n* Implements GAN variants like CycleGAN, Pix2Pix, or DCGAN using Colab.\n* Uses Colab features like Drive integration for dataset access and result storage.\n* Begins experimenting with advanced visualization and logging tools.",
    "L3": "* Designs and trains custom GAN architectures (e.g., StyleGAN2, StyleGAN3) in Colab.\n* Implements advanced training techniques like progressive growing, adaptive augmentation, and mixed precision.\n* Builds modular, reusable GAN pipelines in Colab for scalable experimentation.\n* Integrates Colab with external APIs, experiment tracking tools (e.g., W&B, MLFlow), and collaborative workflows.",
    "hashId": "dada0f634928fbfbcc03b3a4e26879f92caf4904c6c686c67792dcaf9fbaeb8f"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Designing effective prompts",
    "Tools": "FlowiseAI",
    "L1": "* Understands what prompts are and how they influence the behavior of large language models (LLMs).\n* Uses FlowiseAI\u2019s visual interface to create basic prompt flows for simple tasks like Q&A or summarization.\n* Familiar with connecting basic nodes (e.g., input \u2192 LLM \u2192 output) in FlowiseAI.\n* Experiments with prompt templates and observes how changes affect responses.",
    "L2": "* Designs multi-step prompt workflows in FlowiseAI, incorporating context, examples, and conditional logic.\n* Applies prompt engineering principles such as clarity, specificity, and role-based instructions.\n* Integrates external tools or APIs into FlowiseAI flows for enhanced functionality.\n* Begins testing and refining prompts for consistency and reliability across different inputs",
    "L3": "* Builds complex, dynamic prompt chains in FlowiseAI for tasks like retrieval-augmented generation (RAG), agent-based reasoning, or multi-turn conversations.\n* Applies advanced prompt engineering strategies including few-shot learning, chain-of-thought prompting, and system-level instructions .\n* Optimizes prompt flows for performance, latency, and accuracy in production environments.\n* Collaborates across teams using FlowiseAI for scalable, low-code Gen AI application development.",
    "hashId": "5ffd50f67a50a972ea1f6880a867e89c18eab725db812b20317ba29a1cf1e80d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Designing effective prompts",
    "Tools": "OpenAI API (GPT-3, GPT-4)",
    "L1": "* Understands what prompts are and how they influence model outputs.\n* Uses basic prompt formats with OpenAI API to perform tasks like summarization, Q&A, or translation.\n* Familiar with using playground or simple Python scripts to send prompts to GPT-3 or GPT-4.\n* Experiments with temperature, max tokens, and top-p settings to observe changes in responses.",
    "L2": "* Designs structured prompts using techniques like few-shot learning, role-based instructions, and contextual priming.\n* Uses OpenAI API programmatically to build applications with dynamic prompt generation.\n* Begins applying prompt chaining and conditional logic for multi-step tasks.\n* Evaluates prompt effectiveness based on consistency, relevance, and accuracy of responses.",
    "L3": "* Implements advanced strategies like chain-of-thought prompting, retrieval-augmented generation (RAG), and tool use via function calling.\n* Fine-tunes prompt templates for domain-specific tasks and integrates them into production workflows.\n* Builds scalable systems using OpenAI API with prompt orchestration, caching, and fallback logic.\n* Collaborates across teams to standardize prompt design and testing for enterprise-grade Gen AI applications.",
    "hashId": "75f80f496c094eb4c326b16b4b6d176d60f2f8ddf78da80e42bbfc3b981b0d0a"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Designing effective prompts",
    "Tools": "Anthropic API (Claude)",
    "L1": "* Understands what prompts are and how they influence Claude\u2019s responses.\n* Uses basic prompt formats with Anthropic\u2019s Claude API for tasks like summarization, Q&A, and rewriting.\n* Familiar with Claude\u2019s conversational style and safety-first design principles.\n* Experiments with temperature, max tokens, and stop sequences to control output.",
    "L2": "* Designs structured prompts using techniques like role-based instructions, few-shot examples, and contextual priming.\n* Uses Claude\u2019s API programmatically to build applications with dynamic prompt generation.\n* Applies prompt chaining and conditional logic for multi-step tasks.\n* Begins evaluating prompt effectiveness based on coherence, relevance, and safety.",
    "L3": "* Implements advanced strategies like chain-of-thought prompting, tool use, and multi-agent orchestration with Claude.\n* Fine-tunes prompt templates for domain-specific tasks and integrates them into scalable workflows.\n* Builds robust systems using Claude\u2019s API with prompt orchestration, fallback logic, and safety filters.\n* Collaborates across teams to standardize prompt design and testing for enterprise-grade Gen AI applications.",
    "hashId": "1123d8fbe8fea479ae6ee7908f84c3cc06163122776c43b166627960780d7f9a"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Designing effective prompts",
    "Tools": "Hugging Face Transformers (BERT, GPT-2, T5, LLaMA, Falcon, Mistral)",
    "L1": "* Understands the role of prompts in transformer-based models.\n* Uses pre-trained models like GPT-2, BERT, or T5 via Hugging Face\u2019s transformers library for basic tasks (e.g., text generation, classification).\n* Experiments with simple prompt formats and observes model behavior.\n* Familiar with loading models and tokenizers using Python",
    "L2": "* Designs structured prompts for models like T5 (e.g., \u201ctranslate English to French: \u2026\u201d) and BERT (e.g., masked language modeling).\n* Applies prompt engineering techniques such as few-shot prompting and task-specific formatting.\n* Uses models like LLaMA, Falcon, or Mistral via Hugging Face for more advanced tasks.\n* Begins chaining prompts and integrating them into Python applications.",
    "L3": "* Implements advanced prompting strategies like chain-of-thought, instruction tuning, and multi-turn dialogue.\n* Fine-tunes models with custom prompts and datasets for domain-specific tasks.\n* Builds scalable prompt workflows using Hugging Face pipelines, APIs, and model hubs.\n* Combines multiple models (e.g., BERT for retrieval + GPT for generation) in hybrid Gen AI systems.",
    "hashId": "2dc6d916b061ef99ac36c26f20f3ef6bb157e93ac0ba22bb03792cd469149752"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Designing effective prompts",
    "Tools": "LangChain (GPT, Claude, LLaMA, etc)",
    "L1": "* Understands the basic concept of prompt engineering and its role in Gen AI workflows.\n* Can write simple prompts for tasks like summarization, Q&A, and text generation.\n* Familiar with LangChain\u2019s basic components (e.g., chains, prompts, models).\n* Uses pre-built LangChain templates and examples.\n* Can experiment with GPT or Claude using playgrounds or basic LangChain scripts.\n* Understands prompt formatting (e.g., instructions vs. examples).",
    "L2": "* Designs structured prompts for multi-step tasks (e.g., reasoning, classification).\n* Understands prompt tuning techniques (e.g., few-shot, zero-shot, chain-of-thought).\n* Can integrate LangChain with external tools (e.g., APIs, memory, agents).\n* Familiar with model-specific prompt styles (e.g., GPT vs. Claude vs. LLaMA).\n* Applies prompt engineering for domain-specific use cases (e.g., HR, finance).\n* Evaluates prompt effectiveness using metrics like relevance, coherence, and accuracy.",
    "L3": "* Crafts dynamic and context-aware prompts using LangChain\u2019s advanced features (e.g., custom chains, agents, tools).\n* Designs prompt strategies for complex workflows (e.g., retrieval-augmented generation, multi-agent systems).\n* Optimizes prompts for performance across different LLMs (e.g., Claude\u2019s constitutional AI, LLaMA\u2019s token efficiency).\n* Implements prompt injection defenses and safety-aware prompt design.\n* Conducts A/B testing and iterative refinement of prompts using LangChain evaluation modules.\n* Contributes to prompt libraries or builds reusable prompt components for enterprise use.",
    "hashId": "4f62d0480c0693b372e3c4955d5973bf1cc2bf542d8e8ccbe37bf894f7c0e851"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Designing effective prompts",
    "Tools": "Prompt Layer",
    "L1": "* Understands what a prompt layer is and its role in Gen AI workflows.\n* Can use basic prompt templates within LangChain (e.g., PromptTemplate, ChatPromptTemplate).\n* Familiar with static prompts and simple input variables.\n* Uses prompt layers to wrap basic instructions for LLMs like GPT or Claude.\n* Can test prompts manually and observe model responses.",
    "L2": "* Designs modular prompt layers for multi-step chains and agents.\n* Uses LangChain\u2019s advanced prompt classes (e.g., FewShotPromptTemplate, StructuredPromptTemplate).\n* Understands how to dynamically inject context, memory, or tool outputs into prompts.\n* Can switch prompt formats based on model type (e.g., GPT vs. LLaMA).\n* Applies prompt layering for tasks like retrieval-augmented generation (RAG) or tool-based reasoning.",
    "L3": "* Builds reusable and scalable prompt layers for enterprise-grade applications.\n* Implements prompt layers that adapt based on user input, context, and model feedback.\n* Designs prompt layers for multi-agent orchestration and tool chaining.\n* Optimizes prompt layers for latency, token efficiency, and model-specific quirks.\n* Integrates prompt layers with LangChain\u2019s evaluation and debugging tools.\n* Contributes to prompt layer libraries or frameworks for internal teams.",
    "hashId": "240b4210d87dbd242042b6bd8f8af5622640e0abb3e636588c3ac209fda08fbb"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Few-shot, zero-shot, and chain-of-thought prompting",
    "Tools": "Python (OpenAI API,   Anthropic API, Hugging Face Transformers, LangChain)",
    "L1": "* Understands the basic concepts of zero-shot and few-shot prompting.\n* Can write simple prompts using Python with OpenAI or Anthropic APIs.\n* Uses Hugging Face Transformers for basic text generation tasks.\n* Familiar with LangChain\u2019s basic prompt templates and chains.\n* Can run basic examples of zero-shot and few-shot prompts using playgrounds or notebooks.",
    "L2": "* Implements few-shot and chain-of-thought prompting in Python using LangChain and Hugging Face.\n* Understands prompt formatting for different models (e.g., GPT-4, Claude, LLaMA).\n* Uses LangChain to build chains that incorporate reasoning steps (CoT).\n* Applies prompt strategies to improve model performance on classification, reasoning, and summarization tasks.\n* Can dynamically construct prompts using Python scripts and APIs.",
    "L3": "* Designs and tests optimized few-shot and CoT prompts across multiple LLMs using Python.\n* Builds adaptive prompt systems using LangChain agents and memory modules.\n* Implements prompt evaluation and refinement pipelines using LangChain and Hugging Face tools.\n* Fine-tunes prompt strategies for specific domains (e.g., legal, HR, finance) using Python.\n* Integrates prompt engineering with retrieval-augmented generation (RAG) and multi-agent orchestration.\n* Benchmarks prompt effectiveness across OpenAI, Anthropic, and Hugging Face models using Python-based metrics.",
    "hashId": "266dd56e95d7429de17e6ab300eccb4eb3dae46697f695a21948c2ae49be0c3d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Prompt Engineering",
    "Sub-Sub-Category": "Prompt tuning and optimization",
    "Tools": "Python (Open AI API, Anthropic API, Hugging Face Transformers, PEFT (Parameter-Efficient Fine-Tuning), LangChain, PromptLAyer)",
    "L1": "* Understands the basic concepts of zero-shot and few-shot prompting.\n* Can write simple prompts using Python with OpenAI or Anthropic APIs.\n* Uses Hugging Face Transformers for basic text generation tasks.\n* Familiar with LangChain\u2019s basic prompt templates and chains.\n* Can run basic examples of zero-shot and few-shot prompts using playgrounds or notebooks.",
    "L2": "* Implements few-shot and chain-of-thought prompting in Python using LangChain and Hugging Face.\n* Understands prompt formatting for different models (e.g., GPT-4, Claude, LLaMA).\n* Uses LangChain to build chains that incorporate reasoning steps (CoT).\n* Applies prompt strategies to improve model performance on classification, reasoning, and summarization tasks.\n* Can dynamically construct prompts using Python scripts and APIs.",
    "L3": "* Designs and tests optimized few-shot and CoT prompts across multiple LLMs using Python.\n* Builds adaptive prompt systems using LangChain agents and memory modules.\n* Implements prompt evaluation and refinement pipelines using LangChain and Hugging Face tools.\n* Fine-tunes prompt strategies for specific domains (e.g., legal, HR, finance) using Python.\n* Integrates prompt engineering with retrieval-augmented generation (RAG) and multi-agent orchestration.\n* Benchmarks prompt effectiveness across OpenAI, Anthropic, and Hugging Face models using Python-based metrics.",
    "hashId": "c6ea3137c2f2e26d323dadb0c2127a05f7a227bf17aa93e02e1fed7cca92471b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Fine-tuning LLMs on domain-specific data",
    "Tools": "Python (hugging Face Transformers, PEFT, OpenAI API,  Anthropic API, LLaMA / Mistral / Falcon, LangChain)",
    "L1": "* Understands the concept of fine-tuning and its importance for domain adaptation.\n* Can load pre-trained models using Hugging Face Transformers.\n* Familiar with basic usage of OpenAI and Anthropic APIs for prompt-based customization.\n* Explores LangChain for chaining prompts and integrating external data sources.\n* Understands the difference between prompt tuning and full model fine-tuning.",
    "L2": "* Applies PEFT techniques (e.g., LoRA, Prefix Tuning) to fine-tune models on small domain-specific datasets.\n* Uses Hugging Face\u2019s Trainer API or PEFT library to run fine-tuning experiments.\n* Fine-tunes open-source models like LLaMA, Mistral, or Falcon using Python scripts.\n* Integrates fine-tuned models into LangChain workflows for task-specific applications.\n* Evaluates model performance using metrics like accuracy, F1-score, and perplexity.",
    "L3": "* Designs scalable fine-tuning pipelines using PEFT for efficient training on large datasets.\n* Fine-tunes models across multiple architectures (e.g., LLaMA, Falcon, Mistral) and compares performance.\n* Implements domain-specific adapters and modular fine-tuning strategies.\n* Combines fine-tuning with retrieval-augmented generation (RAG) and LangChain agents.\n* Benchmarks fine-tuned models against base models using custom evaluation frameworks.\n* Deploys fine-tuned models via APIs or integrates them into enterprise applications.",
    "hashId": "82704aebf8f37ed9c0ae023127b952f3b2fc7ee479b1c07cbe4211e51953315d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Fine-tuning LLMs on domain-specific data",
    "Tools": "Google Vertex AI",
    "L1": "* Understands the purpose of fine-tuning LLMs for domain-specific tasks.\n* Familiar with Google Cloud Platform (GCP) and basic Vertex AI services.\n* Can use pre-trained models available in Vertex AI for inference.\n* Knows how to upload datasets and create basic training jobs via the Vertex AI console.\n* Understands the difference between prompt tuning and full model fine-tuning.",
    "L2": "* Fine-tunes models using Vertex AI\u2019s custom training pipelines with Python SDKs.\n* Applies PEFT techniques (e.g., LoRA, adapters) using Hugging Face models on Vertex AI.\n* Uses Vertex AI Workbench for managing experiments and tracking model performance.\n* Integrates domain-specific datasets (e.g., CSV, JSONL) for supervised fine-tuning.\n* Deploys fine-tuned models as endpoints and tests them via REST or Python APIs.",
    "L3": "* Designs scalable fine-tuning workflows using Vertex AI Pipelines and custom containers.\n* Fine-tunes open-source models (e.g., LLaMA, Falcon, Mistral) using GCP infrastructure.\n* Implements advanced optimization techniques (e.g., mixed precision, distributed training).\n* Combines fine-tuning with retrieval-augmented generation (RAG) and LangChain agents.\n* Monitors and evaluates model performance using Vertex AI Model Monitoring.\n* Automates model deployment and versioning using CI/CD workflows on GCP.\n",
    "hashId": "884c25b49285863bf701dc33d4244572a20e1719615baadb83d6976e9e79b868"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Fine-tuning LLMs on domain-specific data",
    "Tools": "Azure AI Studio",
    "L1": "* Understands the concept of fine-tuning and its relevance for domain-specific tasks.\n* Familiar with Azure AI Studio interface and basic model deployment workflows.\n* Can use pre-trained models (e.g., GPT, LLaMA) available in Azure AI Studio for inference.\n* Knows how to upload datasets and create basic training jobs using Azure ML Studio or AI Studio.\n* Understands the difference between prompt engineering and model fine-tuning.",
    "L2": "* Fine-tunes models using Azure AI Studio\u2019s custom training pipelines and Python SDKs.\n* Applies PEFT techniques (e.g., LoRA, adapters) using Hugging Face models within Azure environments.\n* Uses Azure ML Workspaces and Compute Instances for managing training jobs.\n* Integrates domain-specific datasets (e.g., tabular, text, JSONL) for supervised fine-tuning.\n* Deploys fine-tuned models as endpoints and tests them via REST APIs or Azure SDKs.",
    "L3": "* Designs scalable fine-tuning workflows using Azure ML Pipelines and custom Docker environments.\n* Fine-tunes open-source models (e.g., LLaMA, Falcon, Mistral) using distributed training on Azure.\n* Implements advanced optimization techniques (e.g., mixed precision, gradient checkpointing).\n* Combines fine-tuning with retrieval-augmented generation (RAG) and LangChain agents.\n* Monitors and evaluates model performance using Azure AI Studio\u2019s integrated tools.\n* Automates model deployment, versioning, and CI/CD workflows using Azure DevOps.",
    "hashId": "04a7558484ebe9a2460cb076ebd2b97efd0976c6bae12acb7c6a2aed413d2737"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Fine-tuning LLMs on domain-specific data",
    "Tools": "MLFlow",
    "L1": "* Understands the purpose of fine-tuning LLMs for domain-specific tasks.\n* Familiar with MLflow\u2019s basic components: tracking, projects, models, and registry.\n* Can log simple experiments and model parameters using MLflow in Python.\n* Uses MLflow to monitor training runs for Hugging Face or OpenAI API-based models.\n* Understands how MLflow integrates with tools like LangChain and PEFT for prompt tuning.",
    "L2": "* Fine-tunes models using Hugging Face Transformers and tracks experiments with MLflow.\n* Applies PEFT techniques (e.g., LoRA, adapters) and logs hyperparameters, metrics, and artifacts.\n* Uses MLflow\u2019s model registry to manage versions of fine-tuned models.\n* Integrates MLflow with cloud platforms (e.g., Azure, GCP) for scalable training.\n* Implements MLflow Projects for reproducible fine-tuning workflows.",
    "L3": "* Designs end-to-end fine-tuning pipelines with MLflow for LLMs like LLaMA, Falcon, and Mistral.\n* Automates experiment tracking, model evaluation, and deployment using MLflow and CI/CD tools.\n* Combines MLflow with LangChain agents and RAG systems for domain-specific applications.\n* Implements custom MLflow plugins or extensions for advanced logging and visualization.\n* Benchmarks fine-tuned models across multiple domains and integrates with enterprise MLOps workflows.",
    "hashId": "020fd5db1504ace56c1d18e3a1d6b496a3ac3a513ec5c9f3ffa94a69edba748b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Fine-tuning LLMs on domain-specific data",
    "Tools": "Javascript, Typescript",
    "L1": "* Understands the concept of fine-tuning and its role in customizing LLMs.\n* Familiar with using JavaScript/TypeScript to interact with LLM APIs (e.g., OpenAI, Anthropic).\n* Can send basic prompts and receive responses using fetch, axios, or SDKs.\n* Uses pre-trained models for inference in web apps or Node.js environments.\n* Understands limitations of fine-tuning directly in JS/TS and the need for backend support.",
    "L2": "* Integrates domain-specific data into prompt engineering workflows using JS/TS.\n* Uses frameworks like LangChain.js or custom wrappers to build prompt pipelines.\n* Interfaces with backend services (Python, REST APIs) that handle actual fine-tuning.\n* Implements caching, logging, and versioning of prompts and responses.\n* Builds UI components to test and evaluate model outputs across different domains.",
    "L3": "* Designs full-stack systems where JS/TS frontends interact with fine-tuned models hosted via APIs.\n* Uses LangChain.js or custom agents to orchestrate multi-step reasoning and tool use.\n* Implements feedback loops from user interactions to inform future fine-tuning cycles.\n* Coordinates with backend systems (e.g., Python ML pipelines) for PEFT or full fine-tuning.\n* Builds dashboards for monitoring model performance, user feedback, and domain adaptation metrics.",
    "hashId": "8decc44c33106b6667a670fbeeb6e570529934cffadfe8838ac32ddba6a71809"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Transfer learning and adapter models (LoRA, PEFT)",
    "Tools": "Python (hugging Face Transformers, PEFT, OpenAI API,  Anthropic API, LLaMA / Mistral / Falcon, LangChain)",
    "L1": "* Understands the concept of transfer learning and adapter-based fine-tuning.\n* Familiar with Hugging Face Transformers and basic model loading in Python.\n* Can use OpenAI and Anthropic APIs for prompt-based customization.\n* Knows what LoRA and PEFT are and their role in efficient fine-tuning.\n* Explores LangChain for chaining prompts and basic model orchestration.",
    "L2": "* Applies LoRA and PEFT techniques to fine-tune models like LLaMA, Mistral, or Falcon using Hugging Face.\n* Uses Hugging Face\u2019s Trainer or PEFT library to run adapter-based training.\n* Understands how to freeze base model layers and train only adapters.\n* Integrates fine-tuned models into LangChain workflows for domain-specific tasks.\n* Evaluates model performance using metrics like accuracy, loss, and perplexity.",
    "L3": "* Designs scalable fine-tuning pipelines using PEFT for large-scale domain adaptation.\n* Implements LoRA and other adapter strategies across multiple architectures (e.g., Falcon, Mistral).\n* Combines adapter models with retrieval-augmented generation (RAG) and LangChain agents.\n* Benchmarks adapter-based models against full fine-tuning approaches.\n* Automates training, evaluation, and deployment using Python-based MLOps tools.\n* Contributes to internal libraries or frameworks for adapter-based fine-tuning.",
    "hashId": "dea6f61d45eb2b437bc94424b2beb8df5f463a0aba9ffee74d48b071c0a7fd4d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Transfer learning and adapter models (LoRA, PEFT)",
    "Tools": "Weights & Biases",
    "L1": "* Understands the basics of transfer learning and adapter models like LoRA and PEFT.\n* Familiar with Weights & Biases dashboard for tracking experiments.\n* Can log simple training metrics (e.g., loss, accuracy) from Python scripts.\n* Uses W&B to visualize training curves and compare runs.\n* Understands how W&B integrates with Hugging Face Transformers and PEFT workflows.",
    "L2": "* Tracks fine-tuning experiments for adapter models (LoRA, PEFT) using W&B.\n* Logs hyperparameters, model checkpoints, and evaluation metrics.\n* Uses W&B Artifacts to manage datasets and model versions.\n* Integrates W&B with Hugging Face Trainer, custom training loops, or LangChain workflows.\n* Collaborates with team members using W&B Reports and dashboards.",
    "L3": "* Designs automated fine-tuning pipelines with W&B for large-scale adapter training.\n* Implements advanced logging (e.g., attention maps, token-level metrics) for model interpretability.\n* Uses W&B Sweeps for hyperparameter optimization of LoRA/PEFT models.\n* Combines W&B with MLOps tools for CI/CD, model deployment, and monitoring.\n* Benchmarks adapter models across domains and architectures (e.g., LLaMA, Falcon, Mistral) using W&B.\n* Contributes to internal tooling or templates for standardized W&B integration in Gen AI workflows.",
    "hashId": "d2d6b75c6d538ab18249d2c07d070e57e4336960584753fcfc087e5c59e771ec"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Dataset preparation and augmentation",
    "Tools": "Python (label Studio, Prodigy, Numpy, Snorkel, GPT APIs, Text Attack, Hugging Face datasets)",
    "L1": "* Understands the importance of clean, labeled data for fine-tuning LLMs.\n* Can load and explore datasets using Hugging Face datasets library and NumPy.\n* Uses basic annotation tools like Label Studio or Prodigy for manual labeling.\n* Familiar with GPT APIs for generating synthetic examples or augmenting small datasets.\n* Understands basic data formats (CSV, JSON, JSONL) and preprocessing steps.",
    "L2": "* Applies data augmentation techniques using GPT APIs and libraries like TextAttack.\n* Uses Snorkel for weak supervision and programmatic labeling of unlabeled data.\n* Builds custom labeling workflows using Label Studio or Prodigy with Python integration.\n* Cleans and transforms datasets using NumPy and Hugging Face datasets for model training.\n* Combines multiple data sources and formats into unified training datasets.",
    "L3": "* Designs scalable data pipelines for domain-specific dataset preparation and augmentation.\n* Implements advanced augmentation strategies (e.g., adversarial examples, paraphrasing, contrast sets).\n* Uses Snorkel for building labeling functions and applying probabilistic labels.\n* Automates dataset versioning, validation, and quality checks using Python.\n* Integrates dataset workflows with MLOps tools and fine-tuning pipelines (e.g., PEFT, LangChain).\n* Contributes to internal data libraries or frameworks for reusable dataset components.",
    "hashId": "ed06c2a9db9fe7d51c3aa2eb8eb0da755f980181281452cb875feb8918b9d5ac"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Dataset preparation and augmentation",
    "Tools": "Scala",
    "L1": "* Understands the role of dataset preparation and augmentation in fine-tuning LLMs.\n* Familiar with basic Scala syntax and data structures (e.g., arrays, collections).\n* Can read and write structured data formats (CSV, JSON) using Scala libraries.\n* Uses Scala for basic data preprocessing tasks (e.g., cleaning, filtering, tokenization).\n* Understands how to interface Scala with Python-based ML tools if needed.",
    "L2": "* Uses Scala libraries (e.g., Spark, Breeze) for scalable data processing and transformation.\n* Implements basic augmentation techniques (e.g., synonym replacement, sentence shuffling).\n* Integrates Scala with data labeling tools or APIs for semi-automated annotation.\n* Prepares domain-specific datasets for downstream use in Python-based fine-tuning pipelines.\n* Applies functional programming paradigms to build reusable data transformation workflows.",
    "L3": "* Designs distributed data augmentation pipelines using Apache Spark with Scala.\n* Implements advanced augmentation strategies (e.g., adversarial examples, contrastive data) in Scala.\n* Builds Scala-based microservices for real-time data preprocessing and augmentation.\n* Interfaces Scala with Python (e.g., via Py4J or REST APIs) to support hybrid ML workflows.\n* Contributes to internal Scala libraries or frameworks for dataset preparation in Gen AI projects.",
    "hashId": "5e1a67f9517d9e197432ece0538e55633a5d872a0d622bc4c5e18cec6bfeb902"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Dataset preparation and augmentation",
    "Tools": "MLFLow",
    "L1": "* Understands the importance of dataset quality for fine-tuning LLMs.\n* Familiar with MLflow\u2019s basic components: tracking, projects, models, and registry.\n* Can log simple dataset preprocessing steps and metadata using MLflow.\n* Uses MLflow to monitor basic data transformation scripts written in Python.\n* Understands how to visualize metrics related to dataset size, class balance, etc.",
    "L2": "* Tracks dataset preparation workflows (e.g., cleaning, tokenization, augmentation) using MLflow.\n* Logs dataset versions, preprocessing parameters, and augmentation strategies as MLflow artifacts.\n* Uses MLflow Projects to organize reproducible data pipelines.\n* Integrates MLflow with tools like Hugging Face datasets, Snorkel, or TextAttack for tracking data changes.\n* Collaborates with team members using MLflow dashboards and reports for dataset evolution.",
    "L3": "* Designs automated and scalable dataset preparation pipelines with MLflow integration.\n* Implements advanced augmentation strategies (e.g., adversarial examples, synthetic data generation) and tracks them in MLflow.\n* Uses MLflow Artifacts and Model Registry to link datasets with specific model versions and training runs.\n* Combines MLflow with MLOps tools for continuous dataset monitoring, validation, and feedback loops.\n* Builds custom MLflow plugins or extensions to support domain-specific dataset metrics and visualizations.",
    "hashId": "cbd4e3d754151033acb3527f9bda56f5891f1345262ed7ad8bcc0f00303c4de9"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Dataset preparation and augmentation",
    "Tools": "Java (open Refine)",
    "L1": "* Understands the importance of clean and structured data for fine-tuning LLMs.\n* Familiar with OpenRefine\u2019s interface and basic operations (e.g., filtering, faceting, clustering).\n* Can use OpenRefine to clean tabular data (CSV, TSV, Excel) and export it for ML use.\n* Understands basic data transformation using OpenRefine expressions (GREL).\n* Uses Java for basic data parsing and file handling tasks.",
    "L2": "* Applies OpenRefine for advanced data cleaning (e.g., regex transformations, reconciliation with external sources).\n* Uses Java to automate data ingestion, transformation, and export workflows.\n* Integrates OpenRefine with Java-based pipelines for dataset preparation.\n* Prepares domain-specific datasets for downstream fine-tuning tasks.\n* Understands how to convert OpenRefine outputs into formats compatible with Python ML tools (e.g., JSONL, CSV).",
    "L3": "* Designs scalable data preparation workflows combining OpenRefine and Java-based ETL systems.\n* Implements custom reconciliation services or extensions in OpenRefine using Java.\n* Builds Java microservices for real-time data cleaning and augmentation.\n* Integrates OpenRefine outputs with MLOps pipelines for continuous dataset updates.\n* Contributes to internal tooling or plugins for OpenRefine to support Gen AI dataset workflows.",
    "hashId": "b971f6077dff8aed992f88ebda8aceacbb0ebcbbbeb55e0ba996a3323d211ece"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Fine-Tuning & Customization",
    "Sub-Sub-Category": "Dataset preparation and augmentation",
    "Tools": "github, GitLab",
    "L1": "* Understands the role of version control in managing datasets for Gen AI projects.\n* Can clone, pull, and push dataset-related repositories using GitHub or GitLab.\n* Familiar with organizing datasets and preprocessing scripts in a Git repository.\n* Uses basic markdown documentation to describe dataset sources and structure.\n* Understands how to track changes in dataset files (e.g., CSV, JSONL) using Git.",
    "L2": "* Manages dataset versioning using Git branches, tags, and commits.\n* Collaborates with team members on dataset preparation workflows via GitHub/GitLab.\n* Uses GitHub Actions or GitLab CI/CD to automate data validation and preprocessing pipelines.\n* Integrates Git repositories with Python-based tools (e.g., Label Studio, Snorkel, Hugging Face Datasets).\n* Maintains structured documentation and changelogs for dataset updates.",
    "L3": "* Designs full MLOps workflows where dataset preparation and augmentation are tracked and automated via GitHub/GitLab.\n* Implements CI/CD pipelines for dataset ingestion, cleaning, augmentation, and deployment.\n* Uses GitHub/GitLab APIs to programmatically manage dataset repositories and metadata.\n* Coordinates dataset workflows across multiple environments (e.g., cloud storage, local dev, model training pipelines).\n* Contributes to internal Git-based frameworks or templates for standardized dataset management in Gen AI projects.",
    "hashId": "5c8a3ecfaa6edc8c747d53b26bc5240db034ef93898a2a108972958528343677"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Combining text, image, audio, and video inputs",
    "Tools": "Open AI GPT- 4",
    "L1": "* Understands the concept of multimodal AI and the capabilities of GPT-4 (Vision).\n* Can input and interpret basic text and image prompts using GPT-4.\n* Familiar with supported input formats (e.g., PNG, JPEG, text).\n* Uses GPT-4 for simple tasks like image captioning, OCR, and visual Q&A.\n* Understands limitations of GPT-4 in handling audio and video directly.",
    "L2": "* Combines text and image inputs for more complex tasks (e.g., document analysis, UI interpretation).\n* Uses external tools to convert audio/video into text or image formats for GPT-4 processing.\n* Designs multimodal workflows using GPT-4 Vision in Python or via API.\n* Applies GPT-4 to real-world use cases like accessibility, education, and content moderation.\n* Understands how to structure multimodal prompts for optimal performance.",
    "L3": "* Builds end-to-end multimodal systems integrating GPT-4 with audio (ASR), video (frame extraction), and image pipelines.\n* Implements multimodal reasoning tasks (e.g., combining visual and textual context for decision-making).\n* Fine-tunes multimodal workflows using GPT-4 in combination with other models (e.g., Whisper for audio, CLIP for image-text alignment).\n* Designs scalable applications (e.g., multimodal chatbots, assistive tech) using GPT-4 APIs.\n* Benchmarks GPT-4\u2019s multimodal capabilities against other models and optimizes input formatting for performance.",
    "hashId": "4389840c06dae0de3d643849f17800b3aa6f6fcea049358f7c2a808a668d9e15"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Combining text, image, audio, and video inputs",
    "Tools": "Google Gemini",
    "L1": "* Understands the concept of multimodal AI and Gemini\u2019s role in combining different data types.\n* Can use Gemini via the chatbot interface to process text and image inputs.\n* Familiar with basic tasks like image captioning, text generation, and speech recognition.\n* Explores Gemini Nano for on-device multimodal tasks (e.g., summarizing recordings, smart replies)",
    "L2": "* Uses Gemini Pro to combine text, image, audio, and video inputs for more complex tasks.\n* Applies Gemini to real-world use cases like document summarization, video analysis, and multilingual translation .\n* Integrates Gemini with Google Workspace tools (Docs, Slides, Meet) for multimodal productivity (e.g., generating images from text, summarizing Drive documents) .\n* Understands Gemini\u2019s ability to interpret sketches, respond to video frames, and generate interactive content .",
    "L3": "* Leverages Gemini Ultra for high-complexity multimodal reasoning tasks across domains like science, law, and coding.\n* Designs agentic workflows using Gemini 2.0 models (Pro, Flash, Flash-Lite) that interact with external tools and APIs.\n* Builds applications that combine real-time inputs from text, image, audio, and video for dynamic decision-making.\n* Benchmarks Gemini\u2019s multimodal performance using academic standards (e.g., MMLU, MMMU) and integrates it into enterprise-grade systems.\n* Uses Gemini\u2019s extended memory and long-context capabilities (up to 2 million tokens) for deep multimodal analysis .\n",
    "hashId": "751b52dec458a82b302208e60bb2209e70be8f67bafa082c0aa121ed84fc6d3f"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Combining text, image, audio, and video inputs",
    "Tools": "JSON, JSONL, MP4, WAV, PNG, JPG",
    "L1": "* Understands the role of different data types (text, image, audio, video) in multimodal AI.\n* Familiar with basic file formats:\n    * Text: JSON, JSONL\n    * Image: PNG, JPG\n    * Audio: WAV\n    * Video: MP4\n* Can load and preview these formats using simple Python or web tools.\n* Uses JSON/JSONL for storing structured text data and annotations.\n* Understands how to convert files between formats using basic tools.",
    "L2": "* Preprocesses and validates multimodal datasets using Python libraries (e.g., json, PIL, librosa, moviepy).\n* Combines text annotations (JSON/JSONL) with corresponding media files (PNG, MP4, WAV) for training multimodal models.\n* Uses tools like Hugging Face Datasets to manage and load multimodal data.\n* Applies basic transformations (e.g., resizing images, trimming audio, extracting video frames).\n* Understands how to structure multimodal datasets for input into models like GPT-4 Vision or Gemini.",
    "L3": "* Designs scalable pipelines for multimodal dataset ingestion, preprocessing, and augmentation.\n* Implements synchronization between modalities (e.g., aligning audio with video, text with image regions).\n* Uses advanced tools (e.g., FFmpeg, OpenCV, PyDub) to manipulate MP4, WAV, PNG, and JPG files.\n* Builds multimodal training datasets with rich metadata in JSON/JSONL formats.\n* Integrates multimodal data into Gen AI workflows (e.g., RAG, agentic systems, multimodal reasoning).\n* Benchmarks model performance across modalities and formats using custom evaluation scripts.",
    "hashId": "e18efdbc2d649f3b15487df33c99c980a9bcb08e7d72a5831f69ffa2071a8db3"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Combining text, image, audio, and video inputs",
    "Tools": "LangChain",
    "L1": "* Understands the concept of multimodal AI and LangChain\u2019s role in orchestrating LLM workflows.\n* Familiar with LangChain\u2019s basic components: PromptTemplate, LLMChain, and Tool.\n* Can process text inputs and generate responses using LangChain with models like GPT or Claude.\n* Begins exploring LangChain\u2019s support for image inputs via vision-enabled models (e.g., GPT-4 Vision).\n* Understands how to structure simple chains that include text and image prompts.",
    "L2": "* Builds multimodal chains that combine text with image, audio, or video-derived inputs.\n* Uses external tools (e.g., Whisper for audio transcription, OpenCV for video frame extraction) to convert non-text inputs into text for LangChain processing.\n* Integrates LangChain with APIs or local tools to handle image captioning, OCR, and audio transcription.\n* Designs workflows that use LangChain agents to reason across modalities (e.g., interpreting a chart and answering questions).",
    "L3": "* Develops complex multimodal pipelines using LangChain agents, tools, and memory modules.\n* Combines real-time inputs from text, image, audio, and video sources for dynamic decision-making.\n* Integrates LangChain with multimodal models (e.g., GPT-4 Vision, Gemini, Claude) and external libraries (e.g., PyDub, FFmpeg, PIL).\n* Implements retrieval-augmented generation (RAG) across modalities (e.g., retrieving relevant video frames or audio clips).\n* Benchmarks and optimizes multimodal workflows for latency, accuracy, and contextual coherence.\n* Contributes to custom LangChain components or plugins for multimodal orchestration.",
    "hashId": "7256c37884b985cf759a4ce8ddf57d693d1bfb0d5838ba095879429295a8a185"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Combining text, image, audio, and video inputs",
    "Tools": "AudioCraft (Meta)",
    "L1": "* Understands the concept of multimodal AI and the role of AudioCraft in audio generation.\n* Familiar with AudioCraft\u2019s core models:\n  * MusicGen \u2013 generates music from text prompts.\n  * AudioGen \u2013 creates sound effects from text (e.g., footsteps, dog barks).\n  * EnCodec \u2013 compresses and reconstructs audio with high fidelity .\n* Can run basic text-to-audio generation tasks using pre-trained models.\n* Explores open-source access to AudioCraft via GitHub for experimentation ",
    "L2": "* Uses AudioCraft to generate realistic audio for various domains (e.g., gaming, film, virtual environments).\n* Combines AudioCraft outputs with other modalities (e.g., syncing generated audio with video or image sequences).\n* Customizes text prompts to control style, tone, and duration of generated audio.\n* Integrates AudioCraft into Python-based workflows for multimodal applications.\n* Begins experimenting with personalized datasets for domain-specific audio generation ",
    "L3": "* Designs multimodal pipelines combining AudioCraft with image, video, and text models (e.g., GPT-4 Vision, CLIP).\n* Fine-tunes AudioCraft models using custom datasets for specialized sound design.\n* Implements real-time multimodal systems (e.g., audio narration for visual scenes, soundscapes for video).\n* Benchmarks and optimizes audio quality using metrics like fidelity, coherence, and temporal consistency.\n* Contributes to open-source enhancements or plugins for AudioCraft to support broader multimodal use cases ",
    "hashId": "e29b62016a3e5489f6642098946b54b9f935216562c683018bdcd5c8c2f5eb05"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Combining text, image, audio, and video inputs",
    "Tools": "Meta LLaVA / LLaVA-Next",
    "L1": "* Understands the concept of multimodal AI and LLaVA\u2019s role in combining vision and language.\n* Can use LLaVA for basic image-text tasks like captioning and visual Q&A .\n* Familiar with running LLaVA models via Hugging Face or Colab demos.\n* Understands the input formats (e.g., PNG, JPG for images; text prompts).\n* Explores LLaVA-NeXT\u2019s improved OCR and reasoning capabilities ",
    "L2": "* Uses LLaVA-NeXT for multi-image and video tasks via zero-shot modality transfer .\n* Applies interleaved image-text formats for tasks like visual storytelling and multi-turn conversations .\n* Integrates LLaVA with Python workflows for multimodal inference and evaluation.\n* Understands prompt formatting and tokenization strategies for different LLaVA checkpoints .\n* Combines LLaVA outputs with external tools (e.g., Whisper for audio, OpenCV for video frame extraction)",
    "L3": "* Builds multimodal pipelines using LLaVA-NeXT-Interleave for unified processing of text, image, video, and 3D inputs .\n* Fine-tunes LLaVA models using interleaved datasets (e.g., M4-Instruct) across multiple domains and modalities.\n* Designs agentic systems using LLaVA-Plus and LLaVA-Interactive for tool use, segmentation, and image editing .\n* Benchmarks LLaVA-NeXT against other multimodal models (e.g., Gemini Pro, GPT-4V) using custom evaluation suites.\n* Contributes to open-source enhancements or internal frameworks for multimodal reasoning and task transfer.",
    "hashId": "3c3e394f5cfb9bd0a6faad76ab6ccda6a230b532bed7abf704e3922dafe78b31"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Image captioning, text-to-image",
    "Tools": "Python (Blip, CLIP, LangChain, CVAT, DJango)",
    "L1": "* Understands the basic concepts of image captioning and text-to-image generation.\n* Can use pre-trained models like BLIP for image captioning and CLIP for image-text similarity.\n* Familiar with basic Python scripts to load images and run inference.\n* Uses CVAT for manual image annotation and labeling.\n* Understands how to serve simple image-captioning models via Django web apps.",
    "L2": "* Builds pipelines combining BLIP and CLIP for caption generation and relevance scoring.\n* Uses LangChain to orchestrate multimodal workflows (e.g., combining image input with LLM reasoning).\n* Implements text-to-image generation using external APIs or models (e.g., Stable Diffusion).\n* Integrates CVAT with Python for automated annotation and dataset preparation.\n* Develops Django-based interfaces for uploading images and displaying generated captions or visuals.",
    "L3": "* Designs end-to-end multimodal systems using LangChain agents that process text, image, and metadata.\n* Fine-tunes BLIP or CLIP models on domain-specific datasets using custom training loops.\n* Implements real-time captioning and text-to-image generation in scalable Django applications.\n* Combines CVAT outputs with model training pipelines for supervised multimodal learning.\n* Benchmarks and optimizes multimodal models for accuracy, latency, and contextual relevance.",
    "hashId": "67cafdcaada25afa522da97da7d5a32e88c6c08f9c670f4415a67b420877308f"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Image captioning, text-to-image",
    "Tools": "AWS (S3 (storage), SageMaker (training), EC2 (compute))",
    "L1": "* Understands the basics of image captioning and text-to-image generation.\n* Familiar with uploading and managing image datasets in Amazon S3.\n* Can launch and configure basic EC2 instances for running inference scripts.\n* Uses pre-trained models (e.g., BLIP, Stable Diffusion) locally or on EC2 for experimentation.\n* Understands how to use SageMaker Studio for basic model exploration.",
    "L2": "* Trains or fine-tunes image captioning or text-to-image models using SageMaker Training Jobs.\n* Automates dataset ingestion from S3 and logs outputs back to S3 buckets.\n* Uses EC2 GPU instances for accelerated training or inference.\n* Deploys models as SageMaker Endpoints for real-time captioning or image generation.\n* Integrates AWS services (e.g., Lambda, API Gateway) to build simple multimodal applications.",
    "L3": "* Designs scalable, production-grade pipelines for multimodal AI using SageMaker Pipelines.\n* Implements distributed training for large models (e.g., BLIP-2, Stable Diffusion XL) on EC2 or SageMaker.\n* Uses S3 versioning and SageMaker Model Registry to manage datasets and model lifecycles.\n* Builds end-to-end multimodal systems (e.g., captioning + retrieval + generation) using AWS-native tools.\n* Integrates monitoring, logging, and CI/CD workflows for multimodal models using CloudWatch, CodePipeline, and SageMaker Clarify.",
    "hashId": "40e0ff0b8e82929af13a673e5ab73249e84a000e80c3c493ee56df1e86f5f99c"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Image captioning, text-to-image",
    "Tools": "Azure\u00a0- AzureML Studio, Blob Storage",
    "L1": "* Understands the basics of image captioning and text-to-image generation.\n* Familiar with uploading and managing image datasets in Azure Blob Storage.\n* Can use AzureML Studio to explore pre-trained models or run basic notebooks.\n* Uses AzureML Designer for simple drag-and-drop workflows involving image data.\n* Understands how to connect Blob Storage to AzureML for dataset access.",
    "L2": "* Trains or fine-tunes image captioning models (e.g., BLIP) using AzureML Pipelines.\n* Uses Blob Storage to manage large-scale image datasets and generated outputs.\n* Deploys models as AzureML Endpoints for real-time captioning or text-to-image generation.\n* Integrates AzureML with Python SDKs to automate training, evaluation, and deployment.\n* Builds custom environments and compute clusters for GPU-based model training.",
    "L3": "* Designs scalable, production-ready multimodal pipelines using AzureML Pipelines, Datastores, and Compute Targets.\n* Implements distributed training for large models (e.g., BLIP-2, Stable Diffusion) using AzureML and Blob Storage.\n* Uses AzureML Model Registry and MLflow integration for versioning and tracking multimodal models.\n* Combines image captioning and text-to-image generation in unified workflows (e.g., caption \u2192 prompt \u2192 image).\n* Integrates Azure services (e.g., Logic Apps, Cognitive Services) for end-to-end multimodal applications.",
    "hashId": "c20581b8a383066783e1bf45d7eb1b15bff5d9612b0415471f37de888f720404"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "Image captioning, text-to-image",
    "Tools": "GCP\u00a0\u2013 Vertex AI, Cloud Storage",
    "L1": "* Understands the basics of image captioning and text-to-image generation.\n* Familiar with uploading and managing image datasets in Google Cloud Storage (GCS).\n* Can use Vertex AI Studio to explore pre-trained models or run basic notebooks.\n* Uses GCP\u2019s built-in tools to visualize and annotate image data.\n* Understands how to connect GCS buckets to Vertex AI for training and inference.",
    "L2": "* Trains or fine-tunes image captioning models (e.g., BLIP) using Vertex AI Custom Training Jobs.\n* Uses Cloud Storage to manage large-scale image datasets and generated outputs.\n* Deploys models as Vertex AI Endpoints for real-time captioning or text-to-image generation.\n* Integrates Python SDKs to automate data loading, model training, and evaluation.\n* Builds pipelines using Vertex AI Pipelines for repeatable multimodal workflows.",
    "L3": "* Designs scalable, production-grade multimodal systems using Vertex AI, Cloud Functions, and BigQuery.\n* Implements distributed training for large models (e.g., BLIP-2, Stable Diffusion) using TPUs or GPU-enabled VMs.\n* Uses Vertex AI Model Registry and ML Metadata to manage model and dataset versions.\n* Combines image captioning and text-to-image generation in unified workflows (e.g., caption \u2192 prompt \u2192 image).\n* Integrates GCP services (e.g., Pub/Sub, Cloud Run, Looker) for real-time, end-to-end multimodal applications.",
    "hashId": "36d1e97ae2cdd4976a7fd556d5b5ee1545a6f105ee96d914c05bc9162f0c4880"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "Python (Whisper, SpeechBrain, ESPnet, Deepgram SDK, Librosa, Torchaudio)",
    "L1": "* Understands the basics of speech-to-text (STT) and its role in multimodal AI.\n* Can transcribe short audio clips using pre-trained models like Whisper or Deepgram API.\n* Uses Librosa or Torchaudio to load and visualize audio waveforms.\n* Familiar with basic audio formats (e.g., WAV, MP3) and sampling rates.\n* Runs simple Python scripts to convert audio to text using open-source models.",
    "L2": "* Implements STT pipelines using Whisper, SpeechBrain, or ESPnet for longer or domain-specific audio.\n* Uses Deepgram SDK for real-time or streaming transcription in Python.\n* Applies audio preprocessing techniques (e.g., noise reduction, resampling) using Librosa or Torchaudio.\n* Evaluates transcription quality using metrics like WER (Word Error Rate).\n* Integrates STT outputs into multimodal workflows (e.g., audio \u2192 text \u2192 LLM reasoning).",
    "L3": "* Fine-tunes STT models (e.g., Whisper, ESPnet) on domain-specific datasets.\n* Designs scalable, real-time transcription systems using Deepgram, SpeechBrain, or ESPnet with streaming support.\n* Combines STT with other modalities (e.g., aligning transcribed text with video frames or images).\n* Implements speaker diarization, language detection, and punctuation restoration.\n* Benchmarks and optimizes STT models for latency, accuracy, and multilingual support.",
    "hashId": "7204c290f7b1333c2f22aacfb772d9435e33833d6294e284d41f038c66477d97"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "JavaScript (Web front-end for annotation tools, API clients)",
    "L1": "* Understands the basics of speech-to-text (STT) and its role in multimodal AI.\n* Can build simple web interfaces using HTML/CSS/JavaScript to upload and play audio files.\n* Uses JavaScript to call STT APIs (e.g., Deepgram, AssemblyAI, Whisper API) and display transcriptions.\n* Familiar with basic audio formats (WAV, MP3) and browser APIs like AudioContext or MediaRecorder.\n* Understands how to send audio files to a backend or cloud API for transcription.",
    "L2": "* Builds interactive annotation tools for reviewing and editing transcriptions in the browser.\n* Implements real-time transcription using WebSockets or streaming APIs (e.g., Deepgram SDK).\n* Integrates audio waveform visualizations using libraries like Wavesurfer.js or Tone.js.\n* Manages audio chunking, buffering, and playback synchronization with transcribed text.\n* Uses JavaScript frameworks (e.g., React, Vue) to create modular, responsive STT interfaces.",
    "L3": "* Designs full-featured web-based STT annotation platforms with speaker diarization, timestamp alignment, and multilingual support.\n* Implements custom audio preprocessing (e.g., silence trimming, noise detection) in-browser using Web Audio API.\n* Integrates with backend services for model selection (e.g., Whisper, ESPnet) and fine-tuning feedback loops.\n* Builds collaborative tools with real-time editing, version control, and user roles.\n* Optimizes performance for large audio files and long transcription sessions using efficient data handling and UI rendering strategies.",
    "hashId": "b8fef7e52ad13a202d6b804bd880c3885a9e8778625af6b058022daa2ba33cf2"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "PyTorch (Whisper, SpeechBrain, ESPnet)",
    "L1": "* Understands the fundamentals of speech-to-text (STT) and its applications in Gen AI.\n* Can run pre-trained models like Whisper using PyTorch for basic transcription tasks.\n* Familiar with loading and preprocessing audio data using PyTorch-compatible libraries.\n* Understands basic audio formats (WAV, MP3) and sampling rates.\n* Uses simple scripts to transcribe short audio clips and print outputs.",
    "L2": "* Implements STT pipelines using SpeechBrain or ESPnet with PyTorch backends.\n* Applies audio preprocessing (e.g., normalization, silence trimming) using torchaudio.\n* Fine-tunes smaller STT models on domain-specific datasets using PyTorch.\n* Evaluates transcription quality using metrics like WER (Word Error Rate) and CER (Character Error Rate).\n* Integrates STT models into multimodal workflows (e.g., audio \u2192 text \u2192 LLM reasoning).",
    "L3": "* Designs scalable, real-time STT systems using PyTorch-based models (e.g., Whisper large, ESPnet streaming).\n* Fine-tunes large-scale models with custom datasets using distributed training and mixed precision.\n* Implements advanced features like speaker diarization, multilingual transcription, and punctuation restoration.\n* Combines STT with other modalities (e.g., aligning transcribed text with video frames or visual context).\n* Contributes to open-source STT model development or builds internal frameworks for enterprise use.",
    "hashId": "972b9e6d7b7e1c0cf7c1cc6fb86e710d0f9b0cf0672874104153edfc3d2390e9"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "MLflow / W&B",
    "L1": "* Understands the basics of speech-to-text (STT) and the importance of experiment tracking.\n* Can log basic training metrics (e.g., loss, accuracy) from STT models (e.g., Whisper, ESPnet) using MLflow or W&B.\n* Uses MLflow UI or W&B dashboard to visualize training curves and compare runs.\n* Logs simple artifacts like model checkpoints and audio samples.\n* Understands how to integrate MLflow or W&B with PyTorch-based STT training scripts.",
    "L2": "* Tracks full STT training pipelines including preprocessing, model training, and evaluation.\n* Logs audio-specific metrics such as Word Error Rate (WER) and Character Error Rate (CER).\n* Uses W&B Artifacts or MLflow Model Registry to manage dataset and model versions.\n* Implements W&B Sweeps or MLflow Projects for hyperparameter tuning.\n* Integrates STT workflows with other modalities (e.g., audio \u2192 text \u2192 LLM) and tracks them end-to-end.",
    "L3": "* Designs scalable, reproducible STT pipelines with automated logging and evaluation using MLflow or W&B.\n* Implements advanced logging (e.g., attention maps, audio spectrograms, transcription overlays).\n* Combines STT with multimodal systems and tracks cross-modal performance (e.g., audio-text alignment).\n* Uses MLflow or W&B for collaborative model development, version control, and deployment tracking.\n* Builds custom dashboards or plugins for domain-specific STT evaluation and visualization.",
    "hashId": "445754cabdcfd1973029998493f4f2a34fb6338a1e122b260398ca92d6918e44"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "JSON / TXT",
    "L1": "* Understands basic concepts of Gen AI and multimodal systems.\n* Can explain what speech-to-text generation is and its common applications.\n* Familiar with tools like Whisper, Google Speech-to-Text, or Azure Speech Services.\n* Able to transcribe simple audio using pre-built APIs or platforms.\n* Knows basic terminology: audio input, transcription, confidence score, etc.\nCan perform basic troubleshooting (e.g., audio clarity, file format issues).",
    "L2": "* Understands architecture of speech-to-text models (e.g., encoder-decoder, transformers).\n* Can fine-tune or customize pre-trained models for specific domains or accents.\n* Familiar with evaluation metrics like Word Error Rate (WER), BLEU score.\n* Able to integrate speech-to-text into multimodal pipelines (e.g., audio + image + text).\n* Can handle noisy data and apply preprocessing techniques (e.g., denoising, segmentation).\n* Understands limitations and ethical considerations (bias, privacy, etc.).",
    "L3": "* Designs and trains custom speech-to-text models using deep learning frameworks (e.g., PyTorch, TensorFlow).\n* Implements multimodal fusion strategies (e.g., combining audio, visual, and textual data).\n* Optimizes models for real-time transcription and low-latency applications.\n* Conducts advanced error analysis and model interpretability.\n* Contributes to research or development of novel architectures (e.g., self-supervised learning for speech).\n* Leads deployment of scalable, production-grade multimodal AI systems.",
    "hashId": "f11d63058aacb5f3d0b6af8d5c860ede1f1cbf3176c31ac6520497f8fa552e4d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "OpenAI API",
    "L1": "* Understands basic concepts of Generative AI and multimodal systems.\n* Can describe speech-to-text generation and its use cases.\n* Familiar with OpenAI\u2019s Whisper API or similar speech models.\n* Able to transcribe short audio clips using OpenAI API.\n* Knows basic terms: audio input, transcription, confidence score.\n* Can troubleshoot simple issues like audio format or clarity.",
    "L2": "* Understands model architecture (e.g., encoder-decoder, transformers).\n* Can fine-tune OpenAI models for specific accents or domains.\n* Familiar with evaluation metrics like Word Error Rate (WER).\n* Integrates speech-to-text into multimodal workflows (e.g., audio + image + text).\n* Applies preprocessing techniques (e.g., noise reduction, segmentation).\n* Aware of ethical concerns: bias, privacy, and data sensitivity.",
    "L3": "* Designs and trains custom speech-to-text models using OpenAI tools and APIs.\n* Implements multimodal fusion strategies (e.g., combining audio, visual, and textual data).\n* Optimizes models for real-time transcription and low latency.\n* Performs advanced error analysis and model interpretability.\n* Contributes to research or development of novel speech architectures.\n* Leads deployment of scalable, production-grade multimodal AI systems.",
    "hashId": "e5b1dec0b22deeaf4d150ce21888d0d92a19b9a82ce3a39bb74bb8dac84dcc26"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Multimodal AI",
    "Sub-Sub-Category": "speech-to-text generation",
    "Tools": "WAV / MP3 / FLAC",
    "L1": "* Understands basic concepts of Gen AI and multimodal systems.\n* Can describe speech-to-text generation and its applications.\n* Familiar with audio formats like WAV, MP3, FLAC and their differences.\n* Able to use tools (e.g., OpenAI Whisper API) to transcribe audio files.\n* Knows basic terminology: sample rate, bit depth, audio codec, transcription.\n* Can troubleshoot simple issues like unsupported formats or poor audio quality.",
    "L2": "* Understands how audio formats affect transcription accuracy and processing speed.\n* Can preprocess audio (e.g., convert formats, normalize volume, remove noise).\n* Familiar with OpenAI API parameters for handling different audio types.\n* Able to integrate speech-to-text into multimodal workflows (e.g., audio + image + text).\n* Evaluates transcription quality using metrics like Word Error Rate (WER).\n* Understands ethical concerns around voice data (e.g., consent, privacy).",
    "L3": "* Designs robust pipelines for handling diverse audio formats in real-time transcription.\n* Implements format-specific optimizations (e.g., compression handling, streaming).\n* Fine-tunes models for specific audio characteristics (e.g., low bitrate, noisy environments).\n* Develops multimodal systems that fuse audio with other modalities (e.g., video, text).\n* Conducts advanced error analysis and model interpretability across formats.\n* Leads deployment of scalable, format-agnostic speech-to-text systems using Gen AI.",
    "hashId": "4a61eb8f04fb7015e8d2ee254fde4ab49904c6f0c43047de77851aa31dc71119"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "Python (Hugging Face evaluate, NLTK, SacreBLEU, Scikit-learn, NumPy, Pandas)",
    "L1": "* Understands the importance of evaluating Gen AI outputs for quality and safety.\n* Can describe basic evaluation metrics like BLEU and ROUGE.\n* Familiar with Python libraries like NLTK and basic usage for text processing.\n* Able to run simple evaluation scripts using pre-built functions.\n* Knows how to load and compare reference vs generated outputs.\n* Can interpret basic metric scores (e.g., BLEU score out of 100).",
    "L2": "* Understands how BLEU, ROUGE, METEOR, and other metrics differ and when to use them.\n* Uses Hugging Face evaluate and SacreBLEU for standardized scoring.\n* Applies Scikit-learn for classification metrics (e.g., precision, recall, F1).\n* Can conduct human evaluations and aggregate feedback systematically.\n* Uses NumPy and Pandas for data manipulation and metric analysis.\n* Understands limitations of automated metrics and complements with qualitative review.\n",
    "L3": "* Designs custom evaluation pipelines combining automated and human metrics.\n* Implements advanced statistical analysis for output quality and consistency.\n* Fine-tunes evaluation strategies for different Gen AI tasks (e.g., summarization, translation).\n* Develops dashboards or reports using Python for continuous model monitoring.\n* Applies safety filters and bias detection using evaluation frameworks.\n* Leads research or audits on Gen AI model performance and ethical compliance.",
    "hashId": "f3399682351e0022b2783317aead4bb69abf0c393c687299f2b9cd176de426c4"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "BLEU / ROUGE / METEOR / CIDEr / BERTScore",
    "L1": "* Understands the purpose of evaluating Gen AI outputs for quality and relevance.\n* Can describe basic metrics like BLEU and ROUGE.\n* Familiar with how reference and generated outputs are compared.\n* Able to run simple evaluations using pre-built tools or platforms.\n* Knows the general interpretation of scores (e.g., higher BLEU = better match).\n* Understands the limitations of automated metrics.",
    "L2": "* Understands the differences and use cases for BLEU, ROUGE, METEOR, CIDEr, and BERTScore.\n* Can apply multiple metrics to evaluate Gen AI outputs across tasks (e.g., summarization, translation).\n* Uses Python libraries (e.g., Hugging Face evaluate, SacreBLEU) to compute scores.\n* Combines automated metrics with human evaluations for more robust assessment.\n* Analyzes metric trends across datasets using Pandas and NumPy.\nUnderstands how evaluation impacts model tuning and deployment decisions.",
    "L3": "* Designs comprehensive evaluation frameworks combining multiple metrics and human feedback.\n* Implements custom scoring logic tailored to specific Gen AI tasks.\n* Conducts statistical analysis to validate metric reliability and consistency.\n* Uses BERTScore and CIDEr for semantic and contextual evaluation of outputs.\n* Develops tools or dashboards for continuous evaluation and monitoring.\n* Leads audits and safety reviews of Gen AI systems based on evaluation outcomes.",
    "hashId": "c6782af5655bf1c11737b7c200ee8b3227252d0f50c569f308764cb9b6639a98"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "LangSmith",
    "L1": "* Understands the purpose of evaluating Gen AI outputs for quality and reliability.\n* Can describe basic metrics like BLEU and ROUGE.\n* Familiar with LangSmith\u2019s interface and basic evaluation workflows.\n* Able to run simple evaluations using LangSmith\u2019s built-in tools.\n* Knows how to compare reference vs generated outputs using standard metrics.\n* Understands basic score interpretation and limitations of automated metrics.\\m",
    "L2": "* Understands the differences and use cases for BLEU, ROUGE, METEOR, CIDEr, and BERTScore.\n* Uses LangSmith to run multi-metric evaluations and analyze model performance.\n* Can configure LangSmith for task-specific evaluations (e.g., summarization, translation).\n* Combines LangSmith\u2019s automated metrics with human feedback workflows.\n* Applies LangSmith\u2019s filtering and tagging features to segment evaluation data.\n* Understands how evaluation results inform model tuning and safety checks.",
    "L3": "* Designs custom evaluation pipelines in LangSmith integrating multiple metrics and human annotations.\n* Implements advanced scoring logic and metric combinations for nuanced analysis.\n* Uses LangSmith to monitor Gen AI models in production for quality and safety.\n* Conducts longitudinal studies and error analysis using LangSmith\u2019s analytics tools.\n* Leads audits and compliance reviews using LangSmith\u2019s evaluation framework.\n* Contributes to evolving evaluation standards and best practices in Gen AI safety.",
    "hashId": "848ea18275bf16193d6519b6889e885ca304371870757d8a62c2bfed1f7f1b36"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "EvalLM",
    "L1": "* Understands the role of evaluation in Gen AI output quality and safety.\n* Can describe basic metrics like BLEU and ROUGE.\n* Familiar with EvalLM\u2019s interface and basic evaluation workflows.\n* Able to run simple evaluations using EvalLM\u2019s default configurations.\n* Knows how to compare reference vs generated outputs using EvalLM.\n* Understands basic score interpretation and metric limitations.",
    "L2": "* Understands the differences and applications of BLEU, ROUGE, METEOR, CIDEr, and BERTScore.\n* Uses EvalLM to evaluate outputs across multiple Gen AI tasks (e.g., summarization, translation).\n* Configures EvalLM for custom evaluation setups and metric combinations.\n* Combines EvalLM\u2019s automated metrics with human feedback for richer insights.\n* Analyzes evaluation results using EvalLM\u2019s reporting features.\n* Understands how evaluation outcomes influence model refinement and safety protocols.",
    "L3": "* Designs advanced evaluation pipelines using EvalLM with custom metrics and scoring logic.\n* Integrates EvalLM into continuous evaluation workflows for production Gen AI systems.\n* Conducts deep error analysis and semantic evaluation using BERTScore and CIDEr.\n* Uses EvalLM to monitor model performance over time and across datasets.\n* Leads audits and compliance reviews using EvalLM\u2019s structured evaluation framework.\n* Contributes to evolving best practices for Gen AI evaluation and safety using EvalLM.",
    "hashId": "3adad7480511e782556b321bd452656eb1e315b2985e323f4a9556804563ba82"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "Weights & Biases (W&B)",
    "L1": "* Understands the importance of evaluating Gen AI outputs for quality and safety.\n* Can describe basic metrics like BLEU and ROUGE.\n* Familiar with the W&B dashboard and basic logging features.\n* Able to visualize simple evaluation results using W&B.\n* Knows how to track model outputs and metric scores over time.\n* Understands basic score interpretation and metric limitations.",
    "L2": "* Uses W&B to log and compare multiple evaluation metrics (e.g., BLEU, ROUGE, METEOR).\n* Integrates W&B with Python libraries (e.g., Hugging Face evaluate, Scikit-learn) for automated logging.\n* Configures W&B projects to track experiments across Gen AI tasks.\n* Combines automated metrics with human evaluation results in W&B reports.\n* Analyzes trends and performance across datasets using W&B visualizations.\n* Understands how evaluation insights guide model tuning and safety improvements.",
    "L3": "* Designs comprehensive evaluation pipelines with W&B for Gen AI systems.\n* Implements custom metric tracking and visualizations (e.g., BERTScore, CIDEr).\n* Uses W&B for real-time monitoring of deployed Gen AI models.\n* Conducts deep error analysis and performance audits using W&B tools.\n* Leads safety reviews and compliance reporting using W&B dashboards.\n* Contributes to best practices in Gen AI evaluation using W&B integrations.",
    "hashId": "dd01b1dafeb5315a40de774988902d357b77d33727d79a04a3c2c6e75bc694b9"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "Matplotlib / Seaborn / Plotly",
    "L1": "* Understands the purpose of evaluating Gen AI outputs using metrics like BLEU and ROUGE.\n* Familiar with basic Python plotting libraries: Matplotlib and Seaborn.\n* Can create simple bar or line charts to visualize evaluation scores.\n* Able to interpret basic visualizations of model performance.\n* Knows how to compare reference vs generated outputs using plotted metrics.",
    "L2": "* Uses Matplotlib, Seaborn, and Plotly to create multi-metric visualizations.\n* Can customize plots (e.g., labels, legends, color schemes) for clearer insights.\n* Applies visualizations to compare performance across models or datasets.\n* Integrates metric outputs (e.g., BLEU, ROUGE, METEOR) into Pandas DataFrames for plotting.\n* Understands how visual trends inform model tuning and evaluation strategies.\n* Combines automated metrics with human evaluation data in visual reports.",
    "L3": "* Designs interactive dashboards using Plotly for real-time evaluation monitoring.\n* Implements advanced visualizations (e.g., heatmaps, distribution plots, semantic similarity graphs).\n* Uses visual analytics to conduct deep error analysis and model interpretability.\n* Automates evaluation pipelines with integrated visualization reporting.\n* Leads development of visual tools for Gen AI safety audits and performance reviews.\n* Contributes to best practices in visualizing Gen AI evaluation metrics.",
    "hashId": "74b6a16fd4a51da01131f48ffc431fd75604a4d15992b3fab8d4650673cd2a81"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Evaluating output quality (BLEU, ROUGE, human evals)",
    "Tools": "OpenAI Evals",
    "L1": "* Understands the purpose of evaluating Gen AI outputs for quality and reliability.\n* Can describe basic metrics like BLEU and ROUGE.\n* Familiar with the concept of OpenAI Evals and its role in structured evaluation.\n* Able to run basic evaluation tasks using OpenAI Evals templates.\n* Knows how to compare reference vs generated outputs using standard metrics.\n* Understands basic score interpretation and metric limitations.",
    "L2": "* Uses OpenAI Evals to evaluate outputs across multiple Gen AI tasks (e.g., summarization, translation).\n* Configures evaluation scenarios using YAML templates and Python scripts.\n* Integrates BLEU, ROUGE, METEOR, CIDEr, and BERTScore into OpenAI Evals workflows.\n* Combines automated metrics with human evaluation for richer insights.\n* Analyzes evaluation results using structured logs and reports.\n* Understands how evaluation outcomes influence model tuning and safety protocols.",
    "L3": "* Designs custom evaluation pipelines using OpenAI Evals with advanced metrics and logic.\n* Implements task-specific evaluators and scoring functions for nuanced analysis.\n* Uses OpenAI Evals to monitor model performance across datasets and deployment stages.\n* Conducts deep error analysis and semantic evaluation using BERTScore and CIDEr.\n* Leads audits and compliance reviews using OpenAI Evals frameworks.\n* Contributes to evolving best practices in Gen AI evaluation and safety using OpenAI Evals.",
    "hashId": "a990209ef52a84420227ef38c085e575a7043934fa5522e8dcbae507e566a860"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "Python (Detoxify, TruLens, AI Fairness 360, Fairness Indicators, Hugging Face evaluate)",
    "L1": "* Understands the importance of detecting bias, toxicity, and hallucinations in Gen AI outputs.\n* Can define basic terms: bias, toxicity, hallucination, fairness.\n* Familiar with Python libraries like Detoxify and Hugging Face evaluate.\n* Able to run basic toxicity detection on text outputs using pre-trained models.\n* Understands the concept of fairness indicators and their role in evaluation.\n* Can interpret simple outputs (e.g., toxicity scores, flagged terms).",
    "L2": "* Uses Detoxify and TruLens to evaluate and visualize toxicity and hallucination risks.\n* Applies AI Fairness 360 and Fairness Indicators to assess demographic bias.\n* Integrates multiple tools to evaluate Gen AI outputs across different dimensions.\n* Understands how hallucinations manifest in LLMs and how to detect them.\n* Combines automated detection with manual review for nuanced evaluation.\n* Analyzes trends in bias/toxicity across datasets using Pandas and visualization tools.",
    "L3": "* Designs custom pipelines for detecting and mitigating bias, toxicity, and hallucinations.\n* Fine-tunes detection models for domain-specific or multilingual use cases.\n* Implements real-time monitoring and alerting systems using TruLens or similar tools.\n* Conducts in-depth audits and fairness assessments using AI Fairness 360.\n* Leads development of ethical evaluation frameworks for Gen AI systems.\n* Contributes to research or policy on responsible AI and model safety.",
    "hashId": "c8a0660729dab54dee88b0cb90d21aeb55d278f1d03691d3beacf89434335b5d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "TensorFlow / PyTorch",
    "L1": "* Understands the importance of detecting bias, toxicity, and hallucinations in Gen AI outputs.\n* Familiar with TensorFlow and PyTorch environments and basic model usage.\n* Can use pre-trained models for basic toxicity or bias detection.\n* Understands basic concepts like fairness, representation, and harmful content.\n* Able to run simple inference scripts to flag toxic or biased outputs.",
    "L2": "* Implements bias and toxicity detection pipelines using TensorFlow or PyTorch models.\n* Fine-tunes pre-trained models for specific domains or languages.\n* Integrates detection models into Gen AI workflows for real-time evaluation.\n* Uses explainability tools (e.g., SHAP, LIME) to interpret model behavior.\n* Applies data preprocessing and augmentation to reduce bias in training data.\n* Understands how hallucinations arise in LLMs and uses model checkpoints to analyze them.",
    "L3": "* Designs and trains custom models for detecting nuanced bias, toxicity, and hallucinations.\n* Develops scalable, production-ready detection systems using TensorFlow or PyTorch.\n* Implements adversarial testing and robustness checks for safety validation.\n* Conducts in-depth audits and fairness evaluations across demographic groups.\n* Leads research or development of novel architectures for ethical Gen AI.\n* Contributes to open-source or enterprise-grade safety frameworks using deep learning.",
    "hashId": "4a73e980fe4e2f31946fc5f6a5069418aeaf6d49e81f1b0110c2b46642f6a805"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "REST APIs",
    "L1": "* Understands the importance of detecting bias, toxicity, and hallucinations in Gen AI outputs.\n* Can explain what REST APIs are and how they are used in AI evaluation.\n* Familiar with basic API tools (e.g., Postman, curl) to send requests and receive responses.\n* Able to call simple REST APIs for toxicity detection (e.g., Perspective API, Detoxify API).\n* Understands basic JSON response parsing and interpretation of scores.",
    "L2": "* Integrates REST APIs into Python or web-based Gen AI evaluation workflows.\n* Uses APIs like Perspective API, Hugging Face Inference API, or custom endpoints for bias/toxicity detection.\n* Handles authentication, rate limits, and error handling in API calls.\n* Combines multiple API responses (e.g., toxicity + hallucination detection) for richer evaluation.\n* Visualizes API-based evaluation results using tools like Pandas and Matplotlib.\n* Understands how to evaluate and compare API-based vs local model performance.",
    "L3": "* Designs and deploys custom REST APIs for internal bias, toxicity, or hallucination detection models.\n* Implements scalable, production-grade API pipelines for real-time Gen AI safety monitoring.\n* Integrates REST APIs into CI/CD pipelines for automated evaluation and alerts.\n* Conducts audits and compliance checks using API-based evaluation logs.\n* Leads development of API-first safety frameworks for Gen AI systems.\n* Contributes to open-source or enterprise-grade REST API tools for responsible AI.",
    "hashId": "87d187d4fe913143cc01c872df5753e6e908fcd37858dc9ad3cf8447ebca0c6b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "JSON, CSV, Parquet",
    "L1": "* Understands the role of structured data (JSON, CSV, Parquet) in evaluating Gen AI outputs.\n* Can read and write basic JSON and CSV files for storing evaluation results.\n* Familiar with tools like Excel, Python (Pandas), or online viewers to inspect data.\n* Able to log simple toxicity or bias scores in tabular format.\n* Understands the importance of consistent data formatting for analysis.",
    "L2": "* Uses Python (Pandas, PyArrow) to process and analyze evaluation data in JSON, CSV, and Parquet formats.\n* Merges model outputs with evaluation scores (e.g., toxicity, hallucination flags) for analysis.\n* Automates data cleaning, transformation, and export for reporting.\n* Applies filters and aggregations to identify patterns in bias or hallucination occurrences.\n* Understands trade-offs between formats (e.g., Parquet for large-scale, CSV for portability).",
    "L3": "* Designs scalable pipelines to log and store Gen AI evaluation data in optimized formats (e.g., Parquet for big data).\n* Integrates JSON/CSV/Parquet-based logs into dashboards or monitoring systems.\n* Implements schema validation and versioning for evaluation datasets.\n* Conducts longitudinal analysis of bias and hallucination trends across model versions.\n* Leads audits and compliance reporting using structured evaluation datasets.\n* Contributes to best practices in data governance and traceability for Gen AI safety.",
    "hashId": "3867431d8e2df7e7b59e187b403f3de2517aeec10b24380a4bf0834d6f17eee7"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "Matplotlib, Seaborn, Plotly",
    "L1": "* Understands the importance of visualizing bias, toxicity, and hallucination metrics.\n* Familiar with basic plotting using Matplotlib and Seaborn.\n* Can create simple bar and line charts to visualize detection scores.\n* Able to plot frequency of toxic terms or hallucinated entities.\n* Understands how to interpret basic visual patterns in evaluation data.",
    "L2": "* Uses Seaborn and Plotly to create comparative visualizations across models or datasets.\n* Visualizes distributions of toxicity scores, bias indicators, or hallucination rates.\n* Combines multiple metrics (e.g., toxicity + bias) in a single plot for richer insights.\n* Applies heatmaps, boxplots, and scatter plots to explore correlations.\n* Integrates visualizations into evaluation reports or dashboards.\n* Understands how visual trends inform model tuning and safety decisions.",
    "L3": "* Designs interactive dashboards using Plotly for real-time safety monitoring.\n* Implements advanced visualizations (e.g., multi-dimensional plots, semantic clustering).\n* Automates visualization pipelines for continuous evaluation workflows.\n* Uses visual analytics to support audits, fairness reviews, and compliance reporting.\n* Leads development of custom visualization tools for Gen AI safety metrics.\n* Contributes to best practices in visual storytelling for responsible AI.",
    "hashId": "c8a508fc5dd2660b0e29aa7fcbf95e8cbcee996e635c2dc2d60f8035d52cce62"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "AWS",
    "L1": "* Understands the importance of detecting bias, toxicity, and hallucinations in Gen AI outputs.\n* Familiar with AWS services like Amazon Comprehend and Amazon SageMaker JumpStart.\n* Can use pre-built models via AWS to detect sentiment, key phrases, and basic toxicity.\n* Able to upload and analyze text data using AWS Console or simple SDK scripts.\n* Understands basic IAM roles and permissions for accessing AI services securely.",
    "L2": "* Uses Amazon SageMaker to deploy and evaluate custom or pre-trained bias/toxicity detection models.\n* Integrates AWS services (e.g., Comprehend, Rekognition, Textract) into Gen AI pipelines.\n* Applies Amazon SageMaker Clarify to detect and visualize bias in datasets and model predictions.\n* Automates evaluation workflows using AWS Lambda, Step Functions, and CloudWatch.\n* Understands how to manage data securely and ethically in AWS environments.\n",
    "L3": "* Designs and deploys scalable, production-grade Gen AI safety pipelines using AWS.\n* Implements real-time bias and toxicity detection using SageMaker Clarify and custom endpoints.\n* Conducts in-depth fairness audits and hallucination detection using AWS-native and third-party tools.\n* Integrates AWS services with external APIs or on-prem systems for hybrid evaluation workflows.\n* Leads governance, compliance, and monitoring strategies using AWS CloudTrail, Config, and Security Hub.\n* Contributes to enterprise-wide responsible AI frameworks using AWS infrastructure.",
    "hashId": "b1cfa14c9ee5668d576ba6639799fcb878a4943eec2d278db84f2883dd8aa58b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "Azure",
    "L1": "* Understands the importance of detecting bias, toxicity, and hallucinations in Gen AI outputs.\n* Familiar with Azure AI services like Azure Content Safety, Azure OpenAI, and Text Analytics.\n* Can use Azure Studio or REST APIs to run basic content moderation and sentiment analysis.\n* Able to interpret simple toxicity or bias scores from Azure services.\n* Understands basic Azure concepts like resource groups, keys, and endpoints.",
    "L2": "* Uses Azure Content Safety to detect harmful content, bias, and offensive language in Gen AI outputs.\n* Integrates Azure OpenAI Service with evaluation pipelines for hallucination detection.\n* Applies Azure Machine Learning to deploy and monitor custom detection models.\n* Automates evaluation workflows using Logic Apps, Azure Functions, or Data Factory.\n* Analyzes trends in bias/toxicity using Azure Monitor, Log Analytics, and Power BI.\n* Understands how to manage data privacy and compliance within Azure environments.",
    "L3": "* Designs and deploys scalable, enterprise-grade safety pipelines using Azure AI and ML services.\n* Implements real-time detection and mitigation of bias, toxicity, and hallucinations using Azure Content Safety APIs.\n* Conducts fairness audits and model interpretability using Responsible AI dashboard in Azure ML.\n* Integrates Azure services with third-party tools (e.g., Hugging Face, Detoxify) for hybrid safety evaluations.\n* Leads governance, compliance, and ethical AI initiatives using Azure Policy, Security Center, and Purview.\n* Contributes to organizational frameworks for responsible Gen AI using Azure infrastructure.",
    "hashId": "4851948bb5062793555d90b898415fefae9a6d9a1d06b16ca8e24ac95374cdb3"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Bias, toxicity, and hallucination detection",
    "Tools": "GCP",
    "L1": "* Understands the importance of detecting bias, toxicity, and hallucinations in Gen AI outputs.\n* Familiar with GCP services like Cloud Natural Language API and Vertex AI.\n* Can use pre-trained models via GCP to analyze sentiment, entity recognition, and content moderation.\n* Able to run basic evaluations using the GCP Console or simple API calls.\n* Understands basic GCP concepts like projects, billing, and IAM roles.\n",
    "L2": "* Uses Vertex AI to deploy and evaluate custom models for bias and toxicity detection.\n* Integrates Cloud Natural Language API and Perspective API (via GCP) into Gen AI pipelines.\n* Applies Explainable AI tools in Vertex AI to interpret model predictions and detect hallucinations.\n* Automates evaluation workflows using Cloud Functions, Cloud Composer, or Dataflow.\n* Analyzes trends in bias/toxicity using BigQuery, Looker, and Data Studio.\n* Understands how to manage data privacy, access control, and compliance in GCP.",
    "L3": "* Designs and deploys scalable, production-grade safety pipelines using Vertex AI Pipelines and GCP-native tools.\n* Implements real-time detection and mitigation of bias, toxicity, and hallucinations using custom models and APIs.\n* Conducts fairness audits and model interpretability using What-If Tool and Explainable AI SDK.\n* Integrates GCP services with third-party tools (e.g., Detoxify, AI Fairness 360) for hybrid safety evaluations.\n* Leads governance, compliance, and ethical AI initiatives using Cloud Audit Logs, Security Command Center, and Policy Intelligence.\n* Contributes to responsible AI frameworks and best practices using GCP infrastructure.",
    "hashId": "8aac2cf379ddf4549114ffd8de26cfbf6c53135c21046f4b0f630b9261eb26d4"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "Python (TruLens, Hugging Face transformers, Presidio)",
    "L1": "* Understands the purpose of guardrails and content moderation in Gen AI systems.\n* Familiar with basic Python usage and libraries like Hugging Face Transformers.\n* Can use pre-trained models to detect inappropriate or unsafe content.\n* Able to run simple moderation checks using TruLens or Hugging Face pipelines.\n* Understands basic concepts like PII (Personally Identifiable Information), toxicity, and prompt injection.",
    "L2": "* Uses TruLens to evaluate and visualize Gen AI outputs for safety, relevance, and factuality.\n* Applies Presidio for detecting and anonymizing PII in text data.\n* Integrates Hugging Face moderation models into Gen AI pipelines for real-time filtering.\n* Customizes moderation thresholds and rules based on use case or audience.\n* Combines multiple tools to enforce layered safety checks (e.g., PII + toxicity + hallucination).\n* Understands how to log and audit moderation decisions for compliance.",
    "L3": "* Designs and deploys robust guardrail systems using TruLens, Presidio, and custom moderation logic.\n* Fine-tunes moderation models for domain-specific or multilingual applications.\n* Implements real-time content filtering and response rewriting in production environments.\n* Conducts audits and safety evaluations using structured logs and dashboards.\n* Leads development of ethical AI frameworks and safety protocols using open-source and proprietary tools.\n* Contributes to research or policy on responsible Gen AI deployment and content governance.",
    "hashId": "456e522abec70d7f77bf402593f0899f419e0ed96ce7a76f6bb2cb78ed92688d"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "REST APIs",
    "L1": "* Understands the role of guardrails and content moderation in Gen AI systems.\n* Can explain what REST APIs are and how they are used to enforce safety.\n* Familiar with calling basic moderation APIs (e.g., Perspective API, OpenAI Moderation API).\n* Able to send simple requests and interpret JSON responses for content safety.\n* Understands basic concepts like rate limits, API keys, and response codes.",
    "L2": "* Integrates REST APIs into Gen AI pipelines for real-time content filtering.\n* Uses multiple APIs (e.g., toxicity, PII, profanity) to build layered moderation systems.\n* Handles authentication, retries, and error handling in API workflows.\n* Combines REST API outputs with custom logic for dynamic response handling.\n* Logs and visualizes moderation results using tools like Pandas and dashboards.\n* Understands how to evaluate and compare moderation APIs for different use cases.",
    "L3": "* Designs and deploys custom REST APIs for internal moderation and safety enforcement.\n* Implements scalable, production-ready moderation pipelines using RESTful architecture.\n* Integrates REST APIs with CI/CD pipelines for automated safety checks.\n* Conducts audits and compliance reporting using API logs and analytics.\n* Leads development of enterprise-grade safety frameworks using REST APIs.\n* Contributes to open-source or proprietary moderation API ecosystems.",
    "hashId": "bb99d55fca98420d1d4bf36571edbc9815a35d3bdc90f26cb5ce8773a6645613"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "JSON, CSV, YAML",
    "L1": "* Understands the role of structured data in implementing and managing content moderation rules.\n* Can read and write basic JSON and CSV files for storing moderation outputs or flagged content.\n* Familiar with using YAML for configuration of moderation rules or pipelines.\n* Able to log simple moderation results (e.g., toxicity scores, PII flags) in tabular or structured formats.\n* Understands the importance of consistent formatting for traceability and audits.",
    "L2": "* Uses Python (Pandas, PyYAML, json) to parse, transform, and analyze moderation data.\n* Maintains rule sets and thresholds for guardrails in YAML configuration files.\n* Combines multiple data sources (e.g., model outputs + human feedback) in CSV/JSON for evaluation.\n* Automates logging and exporting of moderation results for dashboards or reports.\n* Applies filters and aggregations to identify trends in flagged content or violations.",
    "L3": "* Designs scalable pipelines that log and store moderation data in JSON, CSV, or Parquet for large-scale analysis.\n* Implements schema validation and version control for moderation rule sets and outputs.\n* Integrates structured data into real-time monitoring systems and compliance dashboards.\n* Conducts longitudinal analysis of moderation effectiveness using structured logs.\n* Leads audits and reporting using structured datasets for regulatory or ethical reviews.\n* Contributes to best practices in data governance and transparency for Gen AI safety.",
    "hashId": "83a182010b8e21638c1f88ecd36c2df2cd12b8584552be56641a783d31a39d00"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "AWS",
    "L1": "* Understands the need for guardrails and content moderation in Gen AI applications.\n* Familiar with AWS services like Amazon Comprehend, Amazon Rekognition, and AWS Content Moderation APIs.\n* Can use AWS Console or SDKs to run basic moderation tasks (e.g., detect sentiment, inappropriate content).\n* Understands basic IAM roles and permissions for accessing moderation services.\n* Able to interpret simple moderation outputs (e.g., labels, confidence scores).",
    "L2": "* Uses Amazon Comprehend for detecting sentiment, PII, and targeted content in Gen AI outputs.\n* Integrates Amazon Rekognition for multimodal moderation (e.g., image + text).\n* Implements Amazon SageMaker Clarify to detect bias and explain model predictions.\n* Automates moderation workflows using AWS Lambda, Step Functions, and API Gateway.\n* Logs and analyzes moderation results using CloudWatch, Athena, and QuickSight.\n* Understands how to manage moderation thresholds and customize rule sets.",
    "L3": "* Designs and deploys scalable, real-time moderation pipelines using SageMaker, Comprehend, and custom APIs.\n* Implements advanced guardrails using SageMaker Clarify, Amazon Macie (for sensitive data), and AWS WAF.\n* Conducts audits and compliance reporting using CloudTrail, AWS Config, and Security Hub.\n* Integrates AWS moderation services with third-party tools (e.g., Hugging Face, TruLens) for hybrid safety systems.\n* Leads development of enterprise-grade responsible AI frameworks using AWS infrastructure.\n* Contributes to policy and governance strategies for Gen AI safety and content control.",
    "hashId": "bbf3c0064ab7b14d9b25453f81843aa2e29d1a64299faa6512944b35c50c85d2"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "Azure",
    "L1": "* Understands the importance of guardrails and content moderation in Gen AI systems.\n* Familiar with Azure services like Azure Content Safety, Azure AI Language, and Azure OpenAI.\n* Can use Azure Portal or REST APIs to run basic moderation tasks (e.g., detect harmful content, sentiment).\n* Understands basic Azure concepts like resource groups, API keys, and endpoints.\n* Able to interpret simple moderation outputs (e.g., severity scores, flagged categories).",
    "L2": "* Uses Azure Content Safety to detect and classify content risks (e.g., hate, violence, sexual content).\n* Integrates Azure OpenAI Service with moderation APIs for real-time filtering of Gen AI outputs.\n* Applies Azure AI Language for PII detection, sentiment analysis, and content classification.\n* Automates moderation workflows using Azure Functions, Logic Apps, or Data Factory.\n* Analyzes moderation trends using Azure Monitor, Log Analytics, and Power BI.\n* Understands how to manage moderation thresholds and customize safety policies.",
    "L3": "* Designs and deploys enterprise-grade moderation pipelines using Azure AI, Content Safety, and custom APIs.\n* Implements real-time guardrails for Gen AI applications using Azure OpenAI + Content Safety integration.\n* Conducts audits and compliance reporting using Azure Policy, Security Center, and Purview.\n* Integrates Azure moderation services with third-party tools (e.g., TruLens, Presidio) for hybrid safety systems.\n* Leads development of responsible AI frameworks and governance strategies using Azure infrastructure.\n* Contributes to organizational policies and tooling for ethical Gen AI deployment.",
    "hashId": "d6dbd65b9db29d09e01af6936216b6dc1b3733b8b60e620891031482d85393ac"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "GCP",
    "L1": "* Understands the role of guardrails and content moderation in Gen AI systems.\n* Familiar with GCP services like Cloud Natural Language API, Cloud DLP, and Vertex AI.\n* Can use GCP Console or APIs to detect sentiment, PII, and basic content risks.\n* Understands basic GCP concepts like projects, service accounts, and API keys.\n* Able to interpret simple moderation outputs (e.g., toxicity labels, confidence scores).\n",
    "L2": "* Uses Cloud DLP to detect and redact sensitive information (e.g., PII, financial data) in Gen AI outputs.\n* Integrates Cloud Natural Language API and Perspective API for content classification and toxicity detection.\n* Deploys moderation models using Vertex AI and configures pipelines for real-time filtering.\n* Automates moderation workflows using Cloud Functions, Pub/Sub, and Cloud Scheduler.\n* Analyzes moderation trends using BigQuery, Looker, or Data Studio.\n* Understands how to manage moderation thresholds, logging, and alerting.",
    "L3": "* Designs and deploys scalable, production-grade moderation pipelines using Vertex AI Pipelines and Cloud Run.\n* Implements real-time guardrails using custom models and GCP-native services.\n* Conducts audits and compliance reporting using Cloud Audit Logs, Security Command Center, and Policy Intelligence.\n* Integrates GCP moderation tools with third-party APIs (e.g., Hugging Face, TruLens) for hybrid safety systems.\n* Leads development of responsible AI frameworks and governance strategies using GCP infrastructure.\n* Contributes to enterprise-wide safety and ethical AI practices using GCP tooling.",
    "hashId": "e8cb730f6c06dcb0df0ddb5e5ef0d00db7b6ee7fe939f0d5637861bb6ef1f332"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Evaluation & Safety",
    "Sub-Sub-Category": "Guardrails and content moderation",
    "Tools": "Matplotlib, Seaborn, Plotly",
    "L1": "* Understands the role of visualizations in monitoring Gen AI safety and moderation.\n* Familiar with basic plotting using Matplotlib and Seaborn.\n* Can create simple bar or line charts to show flagged content counts or severity levels.\n* Able to visualize moderation results (e.g., toxic vs non-toxic outputs).\n* Understands how to interpret basic visual patterns in moderation data.",
    "L2": "* Uses Seaborn and Plotly to create comparative visualizations across models, categories, or time periods.\n* Visualizes distributions of flagged content types (e.g., hate speech, PII, hallucinations).\n* Combines multiple metrics (e.g., toxicity + bias + hallucination) in a single plot.\n* Applies heatmaps, boxplots, and time series plots to explore moderation trends.\n* Integrates visualizations into evaluation reports or dashboards for stakeholders.",
    "L3": "* Designs interactive dashboards using Plotly Dash or Streamlit for real-time moderation monitoring.\n* Implements advanced visualizations (e.g., multi-dimensional plots, clustering of flagged outputs).\n* Automates visualization pipelines for continuous safety evaluation.\n* Uses visual analytics to support audits, compliance reviews, and model retraining decisions.\n* Leads development of custom visualization tools for Gen AI safety and governance.",
    "hashId": "4ed5ed36b091eb704c2fc04de9aab6f3b7249f7c39266d388963ef3c97be8eef"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "Python(LangChain, LlamaIndex, FastAPI, Flask, OpenAI SDK, Hugging Face SDK)",
    "L1": "* Understands the basics of APIs and how they enable Gen AI integration.\n* Familiar with Python and basic usage of OpenAI SDK or Hugging Face SDK.\n* Can make simple API calls to LLMs and retrieve responses.\n* Able to build basic web apps using Flask or FastAPI for Gen AI demos.\n* Understands JSON request/response formats and basic error handling.",
    "L2": "* Uses LangChain or LlamaIndex to orchestrate multi-step Gen AI workflows.\n* Builds modular APIs with FastAPI or Flask to serve Gen AI models.\n* Integrates multiple APIs (e.g., OpenAI + search + database) into a single pipeline.\n* Implements caching, logging, and basic authentication in API services.\n* Understands how to manage rate limits, retries, and latency in orchestration.\n* Deploys Gen AI apps using containers or cloud platforms (e.g., Heroku, AWS Lambda).",
    "L3": "* Designs scalable, production-grade Gen AI systems using LangChain agents, LlamaIndex retrievers, and custom APIs.\n* Implements advanced orchestration logic (e.g., tool calling, memory, routing) using LangChain.\n* Builds robust, asynchronous APIs with FastAPI, including background tasks and streaming.\n* Integrates Gen AI APIs with enterprise systems (e.g., CRMs, knowledge bases, analytics).\n* Leads deployment using CI/CD pipelines, container orchestration (e.g., Docker, Kubernetes), and monitoring tools.\n* Contributes to open-source or internal frameworks for Gen AI API orchestration and governance.",
    "hashId": "ac7637420f60d48a2812c09223088f2a2e455a6a0c4b83a9719d68d5169461d7"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "JavaScript / TypeScript (Node.js, Express, React (for front-end integration))",
    "L1": "* Understands the basics of REST APIs and how they connect Gen AI models to applications.\n* Familiar with JavaScript or TypeScript syntax and basic usage of Node.js and Express.\n* Can make simple API calls to services like OpenAI or Hugging Face using fetch or axios.\n* Able to build basic front-end interfaces in React to display Gen AI outputs.\n* Understands JSON request/response structure and basic error handling.",
    "L2": "* Builds modular back-end services using Express to orchestrate Gen AI workflows.\n* Integrates multiple APIs (e.g., OpenAI + search + database) into a unified pipeline.\n* Uses React to create dynamic front-end components that interact with Gen AI APIs.\n* Implements middleware for logging, authentication, and rate limiting.\n* Manages environment variables, API keys, and secure configuration.\n* Understands asynchronous programming and error propagation in API chains.",
    "L3": "* Designs scalable, production-ready Gen AI applications using Node.js microservices and React front-ends.\n* Implements advanced orchestration logic (e.g., multi-agent coordination, tool calling) using custom APIs.\n* Builds real-time, streaming interfaces for Gen AI using WebSockets or Server-Sent Events.\n* Integrates Gen AI APIs with enterprise systems (e.g., CRMs, analytics, databases).\n* Leads deployment using CI/CD pipelines, containerization (Docker), and cloud platforms (e.g., Vercel, AWS, GCP).\n* Contributes to reusable SDKs, component libraries, or orchestration frameworks for Gen AI integration.",
    "hashId": "1677922bdb90f0ed73dcceb41cf2ef7adfead54585f127071fbd3e7eb50a7e04"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "AWS (SageMaker, Bedrock)",
    "L1": "* Understands the basics of API integration and orchestration in Gen AI workflows.\n* Familiar with AWS services like Amazon SageMaker and Amazon Bedrock.\n* Can use AWS Console or SDKs to invoke pre-trained foundation models via Bedrock.\n* Able to deploy basic models using SageMaker Studio or JumpStart.\n* Understands IAM roles, API keys, and basic request/response handling.",
    "L2": "* Uses Amazon Bedrock to integrate foundation models (e.g., Anthropic, Cohere, Meta) into applications via APIs.\n* Builds and deploys custom Gen AI models using SageMaker Pipelines and SageMaker Inference Endpoints.\n* Orchestrates multi-step workflows using AWS Step Functions or Lambda.\n* Implements logging, monitoring, and error handling using CloudWatch and X-Ray.\n* Manages API rate limits, retries, and secure access using API Gateway and IAM policies.",
    "L3": "* Designs scalable, production-grade Gen AI systems using SageMaker, Bedrock, and custom APIs.\n* Implements advanced orchestration logic (e.g., multi-agent coordination, tool calling) using Step Functions, EventBridge, and Lambda.\n* Integrates Bedrock APIs with enterprise systems (e.g., CRMs, databases, analytics platforms).\n* Leads deployment using CI/CD pipelines, CloudFormation, and SageMaker Model Registry.\n* Conducts performance optimization, cost analysis, and compliance audits for Gen AI APIs.\n* Contributes to enterprise-wide frameworks for secure, scalable Gen AI integration on AWS.",
    "hashId": "ebb52196c297d9cbb9be544fd1414235f29249cc002954fb8018944c490e3cd6"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "Azure (AI Studio)",
    "L1": "* Understands basic concepts of RESTful APIs and JSON.\n* Can explore and test Gen AI APIs via Azure AI Studio interface.\n* Uses tools like Postman or curl to make simple API calls.\n* Sets up basic Azure resources (e.g., AI services, keys, endpoints).\n* Reads and interprets API documentation for Gen AI services.",
    "L2": "* Integrates Gen AI APIs into applications using SDKs (Python, C#, etc.).\n* Implements authentication (API keys, OAuth) securely.\n* Orchestrates multiple API calls for workflows (e.g., chaining prompts).\n* Uses Azure Functions or Logic Apps for basic automation.\n* Adds error handling, logging, and retry mechanisms.\n* Understands cost implications and manages quotas/throttling.",
    "L3": "* Architects scalable Gen AI solutions using Azure AI Studio and complementary services (e.g., Azure Kubernetes Service, API Management).\n* Designs advanced orchestration using Durable Functions or Logic Apps.\n* Integrates Gen AI APIs with enterprise systems (CRM, ERP) via middleware or custom connectors.\n* Implements CI/CD pipelines for Gen AI deployments.\n* Applies advanced security (RBAC, managed identities, network isolation).\n* Monitors performance and reliability using Azure Monitor and Application Insights.\n* Coaches teams on best practices for API lifecycle and governance.",
    "hashId": "d62862d5ac7e0c480d99d55c27ff784a4ab1bb146be172edb1d20a635cf511e8"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "GCP (Vertex AI)",
    "L1": "* Understands basic API concepts (REST, JSON) and how Vertex AI exposes endpoints.\n* Can access and test Vertex AI APIs using tools like curl or Postman.\n* Navigates Vertex AI console to locate models and endpoints.\n* Sets up basic GCP resources (e.g., enabling APIs, IAM roles).\n* Reads and interprets Vertex AI API documentation.",
    "L2": "* Integrates Vertex AI APIs into applications using client libraries (Python, Java, etc.).\n* Implements authentication using service accounts and OAuth 2.0.\n* Orchestrates multiple API calls for tasks like prompt chaining or multimodal workflows.\n* Uses Cloud Functions or Workflows for automation and orchestration.\n* Implements logging, error handling, and retries for API calls.\n* Understands quota limits and manages resource usage effectively.\n",
    "L3": "* Architects scalable Gen AI solutions using Vertex AI Pipelines and GCP services (e.g., Cloud Run, API Gateway).\n* Designs advanced orchestration using Cloud Composer (Airflow) or Workflows.\n* Integrates Gen AI APIs with enterprise systems (e.g., BigQuery, Firestore, CRM) using Pub/Sub or Eventarc.\n* Implements CI/CD pipelines for Gen AI deployments using Cloud Build.\n* Applies advanced security practices (IAM fine-grained roles, VPC Service Controls).\n* Monitors and optimizes API performance using Cloud Monitoring and Logging.\n* Leads governance and lifecycle management of Gen AI APIs across teams.",
    "hashId": "6ba66c7ae952d3dcb442f6770ace53e5a4d5e047855d4c5628078f1dfdcb41a8"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "Docker, Kubernetes",
    "L1": "* Understands basic containerization concepts and Dockerfile syntax.\n* Can build and run simple Docker containers for Gen AI applications.\n* Knows how to expose Gen AI APIs via containerized services.\n* Familiar with basic Kubernetes concepts (pods, deployments, services).\n* Can deploy a Gen AI container to a local or cloud-based Kubernetes cluster.",
    "L2": "* Creates multi-container setups using Docker Compose for Gen AI workflows.\n* Deploys Gen AI APIs to Kubernetes using Helm charts or manifests.\n* Implements basic orchestration patterns (e.g., chaining containers, service discovery).\n* Configures environment variables, secrets, and volumes for Gen AI containers.\n* Uses Kubernetes features like autoscaling and rolling updates for Gen AI services.\n* Monitors container health and logs using Kubernetes tools (e.g., kubectl, Grafana).",
    "L3": "* Designs scalable, production-grade Gen AI architectures using Kubernetes (e.g., microservices, event-driven pipelines).\n* Implements advanced orchestration using Kubernetes Operators or Argo Workflows.\n* Integrates Gen AI APIs with CI/CD pipelines using Docker and Kubernetes (e.g., GitOps).\n* Applies advanced security practices (e.g., PodSecurityPolicies, RBAC, network policies).\n* Optimizes container performance and resource allocation for Gen AI workloads.\n* Leads container lifecycle management and governance for Gen AI deployments across teams.",
    "hashId": "48036435709287ec6713d270231cbb23a3841f2f0167f0f85c951d43c2436d40"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "API integration and orchestration",
    "Tools": "Apache Airflow",
    "L1": "* Understands basic concepts of DAGs (Directed Acyclic Graphs) and task scheduling.\n* Can install and configure Apache Airflow locally or on cloud platforms.\n* Creates simple DAGs to trigger Gen AI API calls (e.g., prompt generation).\n* Uses basic operators (e.g., PythonOperator, BashOperator) to interact with Gen AI services.\n* Familiar with Airflow UI for monitoring and managing workflows.",
    "L2": "* Designs modular DAGs to orchestrate multi-step Gen AI workflows (e.g., data preprocessing \u2192 prompt \u2192 postprocessing).\n* Uses advanced operators (e.g., HttpOperator, BranchPythonOperator) for conditional logic and API integration.\n* Implements retries, timeouts, and error handling for Gen AI API calls.\n* Integrates Airflow with cloud services (e.g., GCP, Azure, AWS) for scalable Gen AI deployments.\n* Uses Airflow variables, connections, and secrets for secure API access.",
    "L3": "* Architects complex Gen AI pipelines using dynamic DAG generation and custom plugins.\n* Integrates Airflow with CI/CD tools for automated Gen AI workflow deployment.\n* Implements scalable orchestration using Airflow on Kubernetes or managed services (e.g., Cloud Composer).\n* Applies advanced monitoring and alerting using Airflow metrics and external tools (e.g., Prometheus, Grafana).\n* Leads governance, versioning, and lifecycle management of Gen AI workflows across teams.",
    "hashId": "ccec251328f19b275dda06a9c107eb1baac6e45d2dd4867a4684a3eab5b00302"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "Python (LangChain, LlamaIndex, FastAPI, Flask, Gradio, Streamlit)",
    "L1": "* Understands basic Python programming and virtual environment setup.\n* Can build simple GenAI apps using pre-trained models and basic UI frameworks (e.g., Gradio, Streamlit).\n* Uses LangChain or LlamaIndex for basic prompt chaining or document querying.\n* Deploys GenAI apps locally using Flask or FastAPI.\n* Familiar with basic concepts of agents, chains, and embeddings.",
    "L2": "* Builds modular GenAI apps with LangChain or LlamaIndex using custom tools, chains, and memory.\n* Integrates external APIs and data sources (e.g., PDFs, websites, databases) into GenAI workflows.\n* Deploys GenAI apps using FastAPI or Flask with Docker for containerization.\n* Implements user interfaces with Gradio or Streamlit for interactive GenAI experiences.\n* Applies basic security (e.g., API keys, input validation) and performance optimization.\n* Uses callbacks and tracing tools to monitor GenAI app behavior.",
    "L3": "* Designs scalable GenAI agents with advanced LangChain constructs (e.g., multi-agent systems, custom toolkits).\n* Implements retrieval-augmented generation (RAG) pipelines using LlamaIndex with vector databases.\n* Deploys production-grade GenAI apps using FastAPI with Kubernetes or serverless platforms.\n* Integrates CI/CD pipelines for automated testing and deployment of GenAI apps.\n* Applies advanced security (e.g., OAuth, RBAC), observability, and logging.\n* Leads architecture and governance of GenAI agent ecosystems across teams.",
    "hashId": "7e148262ddf5e1d39d948320e95901e9469cf35bbbed1dba1a538b7129b906fe"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "JavaScript / TypeScript (Node.js, React, Express)",
    "L1": "* Understands basic JavaScript/TypeScript syntax and project setup.\n* Can build simple GenAI frontends using React or vanilla JS.\n* Uses Node.js and Express to create basic backend services that call GenAI APIs.\n* Familiar with HTTP requests (e.g., fetch, axios) to interact with GenAI endpoints.\n* Deploys GenAI apps locally or on platforms like Vercel or Render.",
    "L2": "* Builds full-stack GenAI apps with React frontend and Express/Node.js backend.\n* Implements prompt engineering and basic chaining logic in the backend.\n* Uses environment variables and secure storage for API keys.\n* Integrates GenAI APIs with external data sources (e.g., databases, files).\n* Implements routing, error handling, and middleware in Express.\n* Uses TypeScript for type safety and better maintainability.",
    "L3": "* Designs scalable GenAI agent architectures using modular Node.js services and React micro frontends.\n* Implements advanced orchestration logic (e.g., multi-agent workflows, context-aware prompts).\n* Deploys GenAI apps using Docker, Kubernetes, or serverless platforms (e.g., AWS Lambda, GCP Cloud Functions).\n* Integrates CI/CD pipelines for automated testing and deployment.\n* Applies advanced security (e.g., OAuth, RBAC, rate limiting) and observability (e.g., logging, tracing).\n* Leads development of reusable GenAI components and SDKs for team-wide adoption.",
    "hashId": "794031d301282b53d66f540a66bd5a96fdb164ce242aae0643145108b6e70a21"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "REST / GraphQL APIs",
    "L1": "* Understands basic concepts of REST and GraphQL (e.g., endpoints, queries, mutations).\n* Can make simple API calls to GenAI services using tools like Postman or fetch/axios.\n* Builds basic GenAI apps that consume REST APIs for prompt generation or text completion.\n* Familiar with GraphQL playgrounds and basic query structures.\n* Deploys simple GenAI apps with static or minimal dynamic content.",
    "L2": "* Designs and implements RESTful or GraphQL APIs to serve GenAI capabilities (e.g., prompt chaining, RAG).\n* Integrates GenAI APIs with frontend frameworks (React, Vue) or backend services (Node.js, Python).\n* Implements authentication and authorization (e.g., API keys, OAuth) for secure access.\n* Uses GraphQL resolvers and schema stitching to combine GenAI with other data sources.\n* Applies error handling, caching, and rate limiting for API performance and reliability.",
    "L3": "* Architects scalable GenAI microservices using REST/GraphQL APIs with advanced orchestration logic.\n* Implements dynamic agent workflows exposed via GraphQL subscriptions or REST webhooks.\n* Integrates GenAI APIs into enterprise-grade systems with CI/CD, monitoring, and observability.\n* Applies advanced security protocols (e.g., RBAC, JWT, API gateways) and governance practices.\n* Leads development of reusable GenAI API modules and SDKs for internal or external use.\n* Optimizes API performance using tools like Apollo Federation, GraphQL batching, or RESTful caching strategies.",
    "hashId": "2db8eb2bda56776e0506259ff3c0f1f72a3850eca02c96979a126d5684e7daaf"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "AWS (Bedrock, SageMaker)",
    "L1": "* Understands basic AWS concepts (IAM, regions, endpoints).\n* Can access and test GenAI models via Amazon Bedrock console.\n* Uses SageMaker Studio for basic model exploration and inference.\n* Familiar with invoking Bedrock APIs using SDKs (e.g., Boto3).\n* Deploys simple GenAI apps using pre-built models and minimal configuration.",
    "L2": "* Builds GenAI apps using Bedrock APIs with custom prompts and chaining logic.\n* Trains and deploys custom models using SageMaker (e.g., fine-tuning, inference endpoints).\n* Integrates GenAI services with other AWS components (e.g., Lambda, API Gateway).\n* Implements secure access using IAM roles, policies, and environment variables.\n* Uses SageMaker Pipelines for basic automation and workflow orchestration.\n* Applies logging and monitoring using CloudWatch.",
    "L3": "* Architects scalable GenAI agent-based systems using Bedrock and SageMaker with advanced orchestration.\n* Implements retrieval-augmented generation (RAG) using SageMaker with vector databases (e.g., Amazon OpenSearch).\n* Deploys production-grade GenAI apps using CI/CD pipelines (e.g., CodePipeline, CodeBuild).\n* Applies advanced security (e.g., VPC endpoints, KMS encryption, fine-grained IAM policies).\n* Monitors and optimizes model performance using SageMaker Model Monitor and CloudWatch metrics.\n* Leads development of reusable GenAI modules and infrastructure-as-code (e.g., Terraform, CloudFormation).",
    "hashId": "3539cdb99d966a09b8738bd1c8f82724d07bf074fc48bee51f79aa1485446606"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "Azure (AI Studio)",
    "L1": "* Understands Azure AI Studio interface and basic GenAI model capabilities.\n* Can create and test GenAI prompts using Azure AI Studio playground.\n* Deploys simple GenAI apps using pre-built models and endpoints.\n* Familiar with basic Azure services (e.g., Azure OpenAI, Azure Functions).\n* Uses REST APIs from Azure AI Studio in basic applications (e.g., Postman, Python scripts).",
    "L2": "* Builds GenAI apps using Azure AI Studio with custom prompts and chaining logic.\n* Integrates GenAI APIs with other Azure services (e.g., Logic Apps, Power Automate).\n* Implements secure access using Azure Identity (e.g., API keys, managed identities).\n* Uses Azure AI Studio to manage multiple models and endpoints.\n* Deploys GenAI apps using Azure App Service or Azure Container Instances.\n* Applies basic monitoring and logging using Azure Monitor.",
    "L3": "* Designs scalable GenAI agent-based systems using Azure AI Studio and orchestration tools (e.g., Durable Functions, Azure Kubernetes Service).\n* Implements retrieval-augmented generation (RAG) pipelines using Azure Cognitive Search or external vector databases.\n* Integrates GenAI apps into enterprise systems (e.g., Dynamics 365, SharePoint) using connectors and APIs.\n* Implements CI/CD pipelines for GenAI app deployment using GitHub Actions or Azure DevOps.\n* Applies advanced security and governance (e.g., RBAC, network isolation, data compliance).\n* Leads architecture, optimization, and lifecycle management of GenAI apps across teams.",
    "hashId": "71859b6b5265e738bf40fc385659748c9d1850a90491130d481c5f7c106deaf2"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "GCP (Vertex AI)",
    "L1": "* Understands basic GCP concepts (projects, IAM, billing).\n* Can access and test GenAI models via Vertex AI Studio or APIs.\n* Uses pre-trained models for simple tasks like text generation or classification.\n* Familiar with basic SDK usage (e.g., Python client libraries) to invoke Vertex AI endpoints.\n* Deploys simple GenAI apps using Vertex AI with minimal configuration.",
    "L2": "* Builds GenAI apps using custom prompts and chaining logic via Vertex AI APIs.\n* Integrates Vertex AI with other GCP services (e.g., Cloud Functions, Pub/Sub, BigQuery).\n* Implements secure access using service accounts and OAuth 2.0.\n* Uses Vertex AI Workbench or Pipelines for workflow automation.\n* Applies logging, monitoring, and error handling using Cloud Logging and Monitoring.\n* Deploys GenAI apps using Cloud Run or App Engine for scalability.",
    "L3": "* Designs scalable GenAI agent-based systems using Vertex AI with advanced orchestration (e.g., Vertex AI Pipelines, Cloud Composer).\n* Implements retrieval-augmented generation (RAG) using Vertex AI with vector databases (e.g., Pinecone, FAISS, or BigQuery).\n* Integrates GenAI apps into enterprise systems (e.g., CRM, ERP) using Eventarc, API Gateway, or custom connectors.\n* Implements CI/CD pipelines using Cloud Build and Artifact Registry.\n* Applies advanced security (e.g., VPC Service Controls, IAM fine-grained roles, encryption).\n* Leads architecture, optimization, and lifecycle management of GenAI apps across teams.",
    "hashId": "dbafef3b040337102af37984e4a2ac9085d441014cf41270d218b2d0b5815c66"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "Docker, Kubernetes",
    "L1": "* Understands basic containerization concepts and Dockerfile syntax.\n* Can build and run GenAI apps in Docker containers locally.\n* Familiar with basic Kubernetes concepts (pods, deployments, services).\n* Deploys simple GenAI containers to a local Kubernetes cluster (e.g., Minikube).\n* Uses basic CLI tools (docker, kubectl) to manage containers and clusters.",
    "L2": "* Builds multi-container GenAI apps using Docker Compose.\n* Deploys GenAI apps to cloud-based Kubernetes clusters (e.g., AKS, GKE, EKS).\n* Implements orchestration patterns for GenAI workflows (e.g., chaining services, parallel execution).\n* Uses Helm charts for managing GenAI app deployments.\n* Configures secrets, environment variables, and persistent volumes for GenAI services.\n* Monitors container health and logs using Kubernetes-native tools (e.g., kubectl, Grafana, Prometheus).",
    "L3": "* Designs scalable, production-grade GenAI agent architectures using Kubernetes (e.g., microservices, event-driven pipelines).\n* Implements advanced orchestration using Kubernetes Operators or Argo Workflows.\n* Integrates GenAI apps with CI/CD pipelines using tools like Jenkins, GitHub Actions, or GitLab CI.\n* Applies advanced security practices (e.g., PodSecurityPolicies, RBAC, network policies, service meshes).\n* Optimizes container performance and resource allocation for GenAI workloads.\n* Leads governance, lifecycle management, and observability of GenAI deployments across teams.",
    "hashId": "bb945dc50bade3b1100d47b92150d2fe4f49c9c428f051afb61482fa38f8d52f"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Building GenAI apps and agents",
    "Tools": "React",
    "L1": "* Understands basic React concepts (components, props, state).\n* Can build simple GenAI frontends that consume GenAI APIs (e.g., OpenAI, Azure AI).\n* Uses fetch or axios to make API calls from React components.\n* Familiar with basic UI/UX design for GenAI interactions (e.g., chat interfaces).\n* Deploys GenAI React apps using platforms like Vercel or Netlify.",
    "L2": "* Builds modular GenAI apps with reusable React components and hooks.\n* Implements prompt engineering and dynamic input/output handling in the UI.\n* Integrates GenAI APIs with backend services (e.g., Node.js, Python Flask).\n* Applies state management (e.g., Redux, Context API) for complex GenAI workflows.\n* Implements basic security (e.g., token handling, input sanitization).\n* Uses component libraries (e.g., Material UI, Chakra UI) for polished GenAI interfaces.\n",
    "L3": "* Designs scalable GenAI agent-based frontends with advanced React architecture (e.g., micro frontends, server-side rendering).\n* Implements real-time GenAI interactions using WebSockets or GraphQL subscriptions.\n* Integrates GenAI apps with enterprise systems and APIs (e.g., CRM, analytics).\n* Applies advanced performance optimization (e.g., lazy loading, memoization).\n* Implements CI/CD pipelines for automated deployment and testing of GenAI React apps.\n* Leads development of reusable GenAI UI components and design systems across teams.",
    "hashId": "689bfb8d6ca5f04f144d65aea128660bdbdea46d219faea9bb9e0263fd3aabd6"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "Python (W&B SDK, MLflow SDK, TruLens, LangSmith)",
    "L1": "* Understands the importance of monitoring GenAI app performance and user interactions.\n* Can log basic metrics (e.g., latency, token usage) using W&B or MLflow.\n* Uses TruLens or LangSmith to visualize prompt-response pairs and basic feedback.\n* Familiar with setting up tracking for GenAI experiments locally.\n* Can install and configure SDKs (W&B, MLflow, TruLens, LangSmith) in Python environments.",
    "L2": "* Implements structured logging and tracking of GenAI model inputs, outputs, and metadata.\n* Uses W&B or MLflow to compare multiple GenAI model runs and prompt variations.\n* Integrates TruLens or LangSmith for real-time feedback collection and evaluation (e.g., relevance, coherence).\n* Builds dashboards to monitor GenAI agent behavior and performance trends.\n* Applies tagging, versioning, and experiment grouping for organized tracking.\n* Uses feedback data to refine prompts, chains, or model selection.",
    "L3": "* Designs comprehensive monitoring pipelines for GenAI agents using W&B, MLflow, TruLens, and LangSmith.\n* Implements automated feedback loops for continuous improvement (e.g., reinforcement learning from human feedback).\n* Integrates monitoring tools with CI/CD workflows and production-grade GenAI systems.\n* Applies advanced evaluation metrics (e.g., factuality, toxicity, helpfulness) using custom evaluators.\n* Leads governance and compliance efforts by tracking data lineage, model versions, and audit trails.\n* Coaches teams on best practices for GenAI observability, feedback-driven optimization, and ethical monitoring.",
    "hashId": "c6c12db92bd18b97bdda1041de2ac0ed9dfe31ee6500ee60191b5085f21d1a3b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "JavaScript / TypeScript",
    "L1": "* Understands basic logging and monitoring concepts in web applications.\n* Can capture GenAI API responses and user interactions in the frontend (e.g., React, Vue).\n* Uses browser-based tools (e.g., console logs, network tab) to debug GenAI workflows.\n* Familiar with sending logs to backend or third-party services via REST APIs.\n* Implements basic feedback forms or rating widgets for GenAI outputs.",
    "L2": "* Implements structured logging and telemetry in GenAI apps using tools like LogRocket, Sentry, or custom logging services.\n* Captures and stores user feedback (e.g., thumbs up/down, comments) for GenAI responses.\n* Uses TypeScript for type-safe feedback and monitoring data structures.\n* Integrates frontend feedback with backend analytics or monitoring pipelines.\n* Applies performance tracking (e.g., response time, token usage) for GenAI API calls.\n* Builds dashboards to visualize GenAI usage and feedback trends.",
    "L3": "* Designs full-stack feedback loop systems for GenAI agents using custom telemetry and analytics frameworks.\n* Implements real-time monitoring and alerting using tools like Grafana, Prometheus, or ELK stack.\n* Integrates feedback data into GenAI model refinement workflows (e.g., prompt tuning, RAG optimization).\n* Applies advanced observability practices (e.g., distributed tracing, session replay) for GenAI interactions.\n* Leads development of reusable monitoring components and feedback modules across GenAI apps.\n* Ensures compliance and governance in feedback collection (e.g., data privacy, consent management).",
    "hashId": "09db63849ec4abf1017588a8e8012d692481706daaabd7ac843f3220f8dac038"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "REST APIs",
    "L1": "* Understands basic REST API concepts (endpoints, methods, status codes).\n* Can send feedback data (e.g., user ratings, comments) to a backend via POST requests.\n* Uses tools like Postman or browser-based fetch/axios to test feedback APIs.\n* Familiar with basic logging of GenAI responses and user interactions via REST endpoints.\n* Implements simple feedback forms connected to REST APIs.",
    "L2": "* Designs RESTful APIs to collect and store GenAI feedback (e.g., relevance, accuracy, user satisfaction).\n* Integrates GenAI apps with monitoring services via REST (e.g., sending logs to ELK stack, Datadog).\n* Implements structured feedback schemas and validation for consistent data capture.\n* Uses REST APIs to trigger evaluation workflows (e.g., prompt tuning, model comparison).\n* Applies authentication and authorization (e.g., API keys, JWT) for secure feedback submission.",
    "L3": "* Architects scalable feedback loop systems using REST APIs integrated with GenAI pipelines.\n* Implements real-time feedback ingestion and processing using event-driven architectures (e.g., REST + Webhooks + Message Queues).\n* Applies advanced observability practices (e.g., distributed tracing, telemetry) via REST endpoints.\n* Integrates REST APIs with model monitoring platforms (e.g., MLflow, LangSmith, TruLens) for continuous evaluation.\n* Leads governance and compliance efforts in feedback collection (e.g., consent management, audit logging).\n* Designs reusable REST API modules for feedback and monitoring across GenAI applications.",
    "hashId": "8a941e394411c5b8420b934f94e715a4e832ff8422280f6a70be13f365cbf136"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "AWS (CloudWatch)",
    "L1": "* Understands basic AWS CloudWatch concepts (logs, metrics, dashboards).\n* Can view and interpret basic logs and metrics for GenAI applications.\n* Sets up simple alarms for GenAI API usage (e.g., invocation count, error rate).\n* Familiar with CloudWatch integration with AWS services like Lambda and SageMaker.\n* Uses CloudWatch Logs to track GenAI responses and user interactions.",
    "L2": "* Implements structured logging and custom metrics for GenAI apps using CloudWatch SDKs.\n* Creates dashboards to monitor GenAI performance (e.g., latency, token usage, throughput).\n* Configures CloudWatch alarms and notifications for anomalies in GenAI workflows.\n* Integrates CloudWatch with other AWS services (e.g., EventBridge, SNS) for automated feedback loops.\n* Applies log filtering and metric extraction for targeted GenAI monitoring.",
    "L3": "* Designs comprehensive observability pipelines for GenAI agents using CloudWatch Logs, Metrics, and Traces.\n* Implements real-time feedback loops using CloudWatch + Lambda + DynamoDB or S3.\n* Applies advanced monitoring strategies (e.g., anomaly detection, predictive alerts) for GenAI performance.\n* Integrates CloudWatch with third-party tools (e.g., Datadog, Grafana) for enhanced visualization and analysis.\n* Leads governance, compliance, and audit logging for GenAI systems using CloudWatch and AWS security best practices.\n",
    "hashId": "a92630337eaf08d5fb7b4dd333dd60758c594e963fd1ac3e1126709ca301d505"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "Azure (Monitor)",
    "L1": "* Understands basic Azure Monitor components (metrics, logs, alerts).\n* Can view and interpret basic logs and metrics for GenAI services (e.g., Azure OpenAI, Azure Functions).\n* Sets up simple alerts for GenAI API usage (e.g., error rates, latency).\n* Uses Azure Monitor to track resource health and performance.\n* Familiar with integrating Azure Monitor with Azure AI Studio endpoints.",
    "L2": "* Implements custom logging and telemetry for GenAI apps using Azure Monitor SDKs.\n* Creates dashboards to visualize GenAI performance (e.g., token usage, response time).\n* Configures alerts and action groups for automated responses to GenAI anomalies.\n* Integrates Azure Monitor with other services (e.g., Log Analytics, Application Insights) for deeper insights.\n* Applies structured logging and diagnostic settings for GenAI workflows.",
    "L3": "* Designs end-to-end observability pipelines for GenAI agents using Azure Monitor, Application Insights, and Log Analytics.\n* Implements real-time feedback loops using Azure Monitor + Event Grid + Azure Functions.\n* Applies advanced monitoring strategies (e.g., anomaly detection, predictive alerts) for GenAI performance.\n* Integrates Azure Monitor with external tools (e.g., Grafana, Power BI) for enterprise-grade visualization.\n* Leads governance, compliance, and audit logging for GenAI systems using Azure Monitor and security best practices.\n",
    "hashId": "bdbedda61456a00737825e4fe6110eb6986f14144c8a959cacf1a75af322ba48"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "GCP (Operations Suite)",
    "L1": "* Understands basic components of GCP Operations Suite (Logging, Monitoring, Error Reporting).\n* Can view logs and metrics for GenAI services deployed on GCP (e.g., Vertex AI, Cloud Functions).\n* Sets up basic alerts for GenAI API usage (e.g., error rates, latency).\n* Uses GCP Console to explore logs and create simple dashboards.\n* Familiar with integrating GenAI apps with Cloud Logging via SDKs or REST APIs.",
    "L2": "* Implements structured logging and custom metrics for GenAI workflows using Cloud Logging and Monitoring APIs.\n* Creates dashboards to visualize GenAI performance (e.g., token usage, response time, throughput).\n* Configures alerting policies and notification channels (e.g., email, Slack, Pub/Sub).\n* Integrates feedback collection mechanisms into GenAI apps and logs them for analysis.\n* Uses Error Reporting to track and resolve issues in GenAI pipelines.\n",
    "L3": "* Designs comprehensive observability pipelines for GenAI agents using GCP Operations Suite.\n* Implements real-time feedback loops using Cloud Monitoring + Eventarc + Cloud Functions.\n* Applies advanced monitoring strategies (e.g., anomaly detection, predictive alerts) for GenAI performance.\n* Integrates GCP Operations Suite with external tools (e.g., Grafana, BigQuery) for advanced analytics and visualization.\n* Leads governance, compliance, and audit logging for GenAI systems using GCP best practices.",
    "hashId": "3104929aa078cb996d5aba62150d36fe86a5dfeb28d9a39b5b5daf39502571bd"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "PostgreSQL",
    "L1": "* Understands basic PostgreSQL concepts (tables, queries, schema).\n* Can store GenAI API responses and user feedback in a PostgreSQL database.\n* Uses simple SQL queries to retrieve and analyze GenAI interaction data.\n* Familiar with connecting GenAI apps to PostgreSQL using ORMs or direct SQL.\n* Implements basic feedback forms that write to a PostgreSQL table.\n",
    "L2": "* Designs normalized schemas for storing GenAI logs, metrics, and feedback.\n* Implements structured logging and feedback tracking using PostgreSQL triggers or stored procedures.\n* Uses indexing and query optimization for efficient retrieval of GenAI performance data.\n* Integrates PostgreSQL with backend services (e.g., Flask, FastAPI, Node.js) for real-time feedback ingestion.\n* Applies data validation and constraints to ensure feedback quality and consistency.",
    "L3": "* Architects scalable feedback loop systems using PostgreSQL with advanced analytics (e.g., window functions, CTEs).\n* Implements automated feedback processing pipelines using PostgreSQL + event-driven systems (e.g., Kafka, Celery).\n* Applies advanced monitoring strategies using PostgreSQL views, materialized views, and time-series extensions (e.g., TimescaleDB).\n* Integrates PostgreSQL with visualization tools (e.g., Metabase, Grafana) for GenAI performance dashboards.\n* Leads governance, audit logging, and compliance efforts using PostgreSQL security features (e.g., row-level security, encryption).",
    "hashId": "8567e44185b042243ca405435d062516fd7de0f3600ace665cc93a110e5e0b6e"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Deployment & Integration",
    "Sub-Sub-Category": "Monitoring and feedback loops",
    "Tools": "MongoDB",
    "L1": "* Understands basic MongoDB concepts (documents, collections, CRUD operations).\n* Can store GenAI responses and user feedback in MongoDB using simple schemas.\n* Uses basic queries to retrieve GenAI interaction data (e.g., prompt-response pairs).\n* Connects GenAI apps to MongoDB using drivers (e.g., PyMongo, Mongoose).\n* Implements simple feedback forms that write to MongoDB collections.",
    "L2": "* Designs flexible schemas for logging GenAI metrics, feedback, and metadata.\n* Implements indexing and query optimization for efficient feedback retrieval.\n* Uses aggregation pipelines to analyze GenAI performance trends (e.g., response quality, user ratings).\n* Integrates MongoDB with GenAI apps for real-time feedback ingestion and monitoring.\n* Applies data validation and schema enforcement using MongoDB tools (e.g., MongoDB Atlas, Mongoose).",
    "L3": "* Architects scalable feedback loop systems using MongoDB with advanced analytics and real-time processing.\n* Implements change streams for real-time feedback monitoring and triggers.\n* Integrates MongoDB with observability tools (e.g., Grafana, Metabase) for GenAI performance dashboards.\n* Applies advanced security and compliance practices (e.g., field-level encryption, role-based access control).\n* Leads governance, audit logging, and lifecycle management of GenAI feedback data across teams.",
    "hashId": "051ec9850dd4050e2d53028ffaad6ef738e9af24a893ce6f11216be8ecbaaf59"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Python (AI Fairness 360, Fairlearn, TruLens, Presidio)",
    "L1": "* Understands basic ethical principles in GenAI (e.g., fairness, transparency, privacy).\n* Can install and run simple demos using AI Fairness 360 or Fairlearn.\n* Uses TruLens to visualize GenAI responses and identify potential bias or hallucinations.\n* Applies Presidio for basic PII detection and redaction in GenAI outputs.\n* Familiar with evaluating GenAI models for fairness and privacy risks using pre-built tools.",
    "L2": "* Applies fairness metrics (e.g., demographic parity, equalized odds) using AI Fairness 360 or Fairlearn.\n* Uses TruLens to log and analyze GenAI interactions for ethical evaluation (e.g., relevance, toxicity).\n* Integrates Presidio into GenAI pipelines for real-time PII detection and masking.\n* Builds custom evaluators to assess GenAI outputs for bias, safety, and factuality.\n* Implements feedback loops to improve GenAI behavior based on ethical assessments.",
    "L3": "* Designs comprehensive ethical evaluation pipelines using AI Fairness 360, Fairlearn, TruLens, and Presidio.\n* Develops custom fairness mitigation strategies and integrates them into GenAI training or inference workflows.\n* Implements privacy-preserving GenAI systems using advanced techniques (e.g., differential privacy, secure data handling).\n* Leads governance initiatives for GenAI systems, including audit logging, compliance tracking, and ethical reporting.\n* Coaches teams on responsible GenAI development practices and tool adoption across the lifecycle.",
    "hashId": "01e4791d880bfb2c626141f1b2ed49e0d1c89374a336a842a2b012b8144cc264"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Jupyter Notebooks",
    "L1": "* Understands the role of Jupyter Notebooks in documenting and experimenting with GenAI workflows.\n* Uses notebooks to run basic fairness and privacy checks using pre-built Python libraries (e.g., AI Fairness 360, Presidio).\n* Documents GenAI model inputs, outputs, and evaluation results in a reproducible format.\n* Familiar with markdown cells for annotating ethical considerations and observations.\n* Can visualize simple metrics (e.g., bias scores, PII detection) using plots and tables.",
    "L2": "* Builds interactive notebooks to evaluate GenAI models for fairness, bias, and privacy risks.\n* Integrates tools like Fairlearn, TruLens, and Presidio into GenAI pipelines within notebooks.\n* Uses widgets and visualizations to explore ethical metrics and feedback data.\n* Applies version control and experiment tracking using tools like MLflow or W&B within notebooks.\n* Collaborates with stakeholders by sharing annotated notebooks for ethical reviews.",
    "L3": "* Designs comprehensive ethical evaluation frameworks in Jupyter Notebooks combining multiple tools (e.g., Fairlearn + TruLens + Presidio).\n* Implements automated feedback loops and ethical audits using notebook-based workflows.\n* Uses advanced visualization techniques (e.g., interactive dashboards, heatmaps) to communicate ethical risks.\n* Leads governance and compliance documentation using reproducible notebook templates.\n* Coaches teams on using Jupyter Notebooks for responsible GenAI development, experimentation, and reporting.\n",
    "hashId": "2a60d192646febaff615e49d6897aa3013b895c22af4b407510d8e66ea0bdd10"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Azure (Responsible AI)",
    "L1": "* Understands basic principles of responsible AI (fairness, transparency, accountability, privacy).\n* Familiar with Azure AI Studio\u2019s built-in tools for model evaluation and prompt testing.\n* Can access and interpret basic documentation on Azure\u2019s Responsible AI standards.\n* Uses Azure AI Content Safety to detect harmful or inappropriate outputs.\n* Understands the role of Azure Responsible AI dashboard in model assessment.",
    "L2": "* Applies Azure Responsible AI tools to evaluate GenAI models for bias, safety, and fairness.\n* Uses Azure AI Content Safety and model monitoring features to flag and mitigate risks.\n* Implements responsible data handling practices using Azure services (e.g., Data Masking, DLP).\n* Integrates Azure Responsible AI principles into GenAI workflows and app development.\n* Documents ethical considerations and mitigations using Azure\u2019s governance templates.",
    "L3": "* Designs and governs GenAI systems using Azure Responsible AI framework across the lifecycle (design, development, deployment).\n* Implements advanced safety and fairness evaluations using Azure tools (e.g., Responsible AI dashboard, interpretability tools).\n* Leads ethical risk assessments and mitigation planning for GenAI deployments.\n* Integrates Responsible AI practices into CI/CD pipelines and enterprise governance workflows.\n* Coaches teams on Azure\u2019s Responsible AI policies, compliance standards, and ethical development practices.",
    "hashId": "45d3363a73309d914569ee44f7f4ba25e84a82ea7a1da3542cd31e555c5a7ea0"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "AWS (SageMaker Clarify)",
    "L1": "* Understands basic ethical principles in GenAI (e.g., fairness, bias, transparency).\n* Familiar with SageMaker Clarify\u2019s purpose in detecting bias and explaining model predictions.\n* Can run basic bias detection jobs using pre-configured datasets in SageMaker Studio.\n* Uses built-in visualizations to interpret bias metrics and feature importance.\n* Understands the role of Clarify in responsible AI workflows within AWS.",
    "L2": "* Configures SageMaker Clarify jobs for custom GenAI models and datasets.\n* Applies bias metrics (e.g., disparate impact, statistical parity) to evaluate GenAI outputs.\n* Uses Clarify to generate SHAP-based explanations for model predictions.\n* Integrates Clarify with SageMaker Pipelines for automated ethical evaluation.\n* Documents and interprets Clarify results to inform GenAI model adjustments.",
    "L3": "* Designs comprehensive responsible AI workflows using SageMaker Clarify across the GenAI lifecycle.\n* Implements continuous bias monitoring and mitigation strategies using Clarify + SageMaker Model Monitor.\n* Leads governance and compliance efforts by integrating Clarify results into audit and reporting systems.\n* Applies advanced fairness techniques and custom metrics to GenAI models.\n* Coaches teams on ethical AI development using AWS tools and best practices.",
    "hashId": "2112a23b92295dc83eca9040d017cfc13bbf9072679b0420a16ceb03145052b1"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "GCP (Vertex AI Explainable AI)",
    "L1": "* Understands basic ethical principles in GenAI (e.g., fairness, transparency, accountability).\n* Familiar with GCP Vertex AI Explainable AI\u2019s purpose in interpreting model predictions.\n* Can access and view basic feature attribution reports in Vertex AI.\n* Uses built-in tools to visualize model behavior and identify potential biases.\n* Understands the role of explainability in responsible GenAI development.",
    "L2": "* Configures Vertex AI Explainable AI for custom GenAI models (e.g., tabular, image, text).\n* Applies feature attribution techniques (e.g., integrated gradients, XRAI) to analyze model decisions.\n* Integrates explainability reports into GenAI evaluation workflows.\n* Uses Vertex AI to assess fairness and transparency across different user segments.\n* Documents explainability insights to inform prompt design and model selection.",
    "L3": "* Designs comprehensive ethical evaluation pipelines using Vertex AI Explainable AI across GenAI lifecycle stages.\n* Implements continuous monitoring of GenAI outputs for fairness, bias, and interpretability.\n* Integrates explainability insights into governance frameworks and compliance reporting.\n* Leads development of custom explainability strategies for complex GenAI models and agents.\n* Coaches teams on responsible GenAI development using GCP tools and ethical best practices.",
    "hashId": "8ae11453e10b11852b9ae91b84e16d734b91c2d1f39bc890344b871b7dc8cca6"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Plotly",
    "L1": "* Understands basic data visualization principles and how they relate to ethical AI (e.g., transparency, clarity).\n* Can use Plotly to create simple charts (e.g., bar, line, scatter) to visualize GenAI outputs and feedback.\n* Uses Plotly in Jupyter or Python scripts to display model performance metrics.\n* Familiar with visualizing fairness or bias scores using basic plots.",
    "L2": "* Builds interactive dashboards using Plotly Dash to monitor GenAI ethical metrics (e.g., bias, toxicity, PII detection).\n* Visualizes feature attribution and model interpretability results (e.g., SHAP values) using Plotly.\n* Integrates Plotly with ethical evaluation tools (e.g., AI Fairness 360, TruLens) for dynamic reporting.\n* Applies best practices in ethical data visualization (e.g., avoiding misleading scales, ensuring accessibility).",
    "L3": "* Designs comprehensive ethical monitoring dashboards using Plotly Dash for real-time GenAI evaluation.\n* Implements advanced visualizations (e.g., heatmaps, multi-dimensional plots) to communicate fairness, privacy, and safety insights.\n* Integrates Plotly with feedback loops and governance systems for continuous ethical oversight.\n* Leads development of reusable visualization templates for GenAI ethics reporting across teams.\n* Coaches teams on using Plotly for transparent and responsible AI communication.",
    "hashId": "de951a1440b6fc7f9f2723d3437a312f540edac427ad25dd0436734bd4bbf373"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Seaborn",
    "L1": "* Understands basic data visualization concepts and how they relate to ethical AI (e.g., clarity, transparency).\n* Uses Seaborn to create simple plots (e.g., bar, line, scatter) to visualize GenAI outputs and feedback.\n* Can plot basic fairness metrics or bias scores using Seaborn in Jupyter Notebooks or Python scripts.\n* Familiar with customizing plots for readability and interpretability.",
    "L2": "* Builds visualizations to explore ethical dimensions of GenAI (e.g., distribution of responses across demographics).\n* Uses Seaborn to compare GenAI model performance across different user groups or prompt types.\n* Integrates Seaborn with tools like AI Fairness 360 or Fairlearn to visualize fairness and bias metrics.\n* Applies best practices in ethical data visualization (e.g., avoiding misleading representations, ensuring accessibility).",
    "L3": "* Designs comprehensive visual reports using Seaborn to communicate ethical risks in GenAI systems.\n* Implements advanced plots (e.g., heatmaps, violin plots, pair plots) to analyze complex ethical metrics.\n* Integrates Seaborn visualizations into automated GenAI evaluation pipelines and dashboards.\n* Leads development of standardized visualization templates for ethical reporting across GenAI projects.\n* Coaches teams on using Seaborn for transparent and responsible AI communication.",
    "hashId": "df5fb1508a682509bf2b5d1faf14b82d5cc2f56fe0db59e617e4724674cc4ca5"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Matplotlib",
    "L1": "* Basic understanding of Bias Detection & Mitigation and Privacy & Data Protection\n* Limited awareness of Transparency, Fairness, and Human Oversight\n* Minimal exposure to Accountability and Ethical Risk Assessment\n* Introductory knowledge of Regulatory Compliance",
    "L2": "* Moderate proficiency in identifying and addressing Bias, Privacy, and Fairness\n* Growing capability in Transparency, Human Oversight, and Regulatory Compliance\n* Able to apply Accountability Mechanisms in structured environments\n* Beginning to assess Ethical Risks in GenAI applications",
    "L3": "* High-level expertise in Bias Mitigation, Privacy Protection, and Fairness\n* Strong command of Transparency, Explainability, and Human Oversight\n* Proficient in implementing Accountability Mechanisms and conducting Ethical Risk Assessments\n* Deep understanding of Regulatory Compliance and its integration into GenAI systems",
    "hashId": "857060b046a893fb09182e11e8e49dc029688f4c8290a7f467a985882f1471cb"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Ethical considerations in GenAI use",
    "Tools": "Datasheets for Datasets",
    "L1": "* Understand the purpose of datasheets in documenting datasets\n* Recognize basic metadata (source, format, intended use)\n* Awareness of ethical concerns like bias and privacy",
    "L2": "* Ability to create and interpret datasheets for datasets\n* Evaluate datasets for fairness, inclusivity, and transparency\n* Understand implications of dataset choices on model behavior",
    "L3": "* Design comprehensive datasheets with ethical risk assessments\n* Integrate datasheet practices into governance workflows\n* Lead audits and reviews of datasets for regulatory compliance\n",
    "hashId": "4e34c5a6a9709ca50300b163ffafab97f01b3abf7267fd4e24a7da3af69cedae"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "Python (AI Fairness 360, Fairlearn, TruLens, Presidio)",
    "L1": "* Understand basic principles of GDPR and HIPAA\n* Use Presidio for simple PII detection and anonymization\n* Awareness of fairness and bias concepts",
    "L2": "* Apply Fairlearn and AI Fairness 360 to evaluate model fairness\n* Use TruLens for GenAI output evaluation\n* Implement privacy-preserving techniques in data pipelines",
    "L3": "* Design and audit systems for full GDPR/HIPAA compliance\n* Customize fairness mitigation strategies using AI Fairness 360 and Fairlearn\n* Integrate TruLens and Presidio into enterprise-grade GenAI workflows",
    "hashId": "ce561c781c3e22143af0c3e8a5bb52a746a585967fad6c0065d2ec53f3971646"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "Jupyter Notebooks",
    "L1": "* Run basic privacy checks using Presidio in Jupyter\n* Explore fairness metrics with Fairlearn\n* Document simple compliance workflows interactively",
    "L2": "* Build and test fairness mitigation pipelines using AI Fairness 360\n* Use TruLens to evaluate GenAI outputs for privacy and bias\n* Create reproducible notebooks for GDPR/HIPAA audit trails",
    "L3": "* Develop enterprise-grade privacy and fairness dashboards in Jupyter\n* Integrate multiple tools (e.g., Presidio + Fairlearn) for end-to-end compliance workflows\n* Automate ethical risk assessments and reporting within notebooks",
    "hashId": "2ca3c77fdaa773c4366fc3930501318b12f894478a9bddd60e8557f42c91f092"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "Azure",
    "L1": "* Understand basic concepts of PII, data privacy, and Azure governance\n* Recognize key Azure tools: Azure AD, Key Vault, Azure Policy, and RBAC\n* Learn how Azure uses identities to control access and protect resources\n* Awareness of Microsoft Privacy Statement, Online Service Terms, and Data Protection Amendment\n* Navigate Azure security options and identify firewall configurations",
    "L2": "* Implement row-level and column-level security and dynamic data masking in Azure SQL\n* Classify sensitive data and apply Azure Defender for SQL for protection\n* Use Azure Monitor Logs and Policy Compliance to track and enforce GDPR/HIPAA standards\n* Apply RBAC, MFA, and Conditional Access for secure identity management\n* Hands-on experience with data classification, audit trails, and policy enforcement workflows",
    "L3": "* Design and deploy Azure Blueprints tailored to GDPR and HIPAA compliance\n* Automate compliance reporting and remediation using Azure Compliance Manager\n* Integrate Azure Key Vault, TDE, and Storage Service Encryption for full data protection\n* Lead implementation of GDPR rights management (access, erasure, portability)\n* Conduct risk assessments, manage incident response plans, and enforce least privilege principles across environments",
    "hashId": "763d8328fb6d2a4780e7567d8148f21831603a35d4a5db313000d329f1671fa5"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "AWS (SageMaker Clarify)",
    "L1": "* Understand the basics of GDPR and HIPAA compliance in cloud environments\n* Learn the purpose of SageMaker Clarify for bias detection and explainability \n* Explore AWS\u2019s shared responsibility model for compliance \n* Use pre-built notebooks to run simple bias metrics and generate reports\n* Awareness of AWS tools like IAM, Macie, and CloudTrail for privacy and monitoring",
    "L2": "* Apply SageMaker Clarify to detect bias in datasets and model predictions\n* Integrate Clarify with SageMaker Pipelines for automated fairness checks \n* Use Amazon Macie for PII detection and classification in S3 buckets\n* Configure IAM roles and least privilege access for secure AI workflows\n* Monitor model behavior and data access using CloudTrail and Model Monitor",
    "L3": "* Design and implement full compliance workflows using AWS services\n* Automate bias detection, explainability, and drift monitoring across ML lifecycle\n* Conduct compliance audits using AWS Audit Manager and Security Hub\n* Lead governance strategy using tools like SageMaker Clarify, Macie, and CloudTrail\n* Ensure continuous alignment with GDPR/HIPAA through custom alerts, reports, and policy enforcement",
    "hashId": "47e64642ffba037ea569bd5425d821e1d143dee2232c24b995e513de0a2f8f52"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "GCP (Vertex AI Explainable AI)",
    "L1": "* Understand basic principles of GDPR and HIPAA in cloud environments\n* Learn the purpose of Explainable AI (xAI) in promoting transparency and fairness\n* Explore Vertex AI\u2019s model cards and basic bias detection tools \n* Awareness of Google Cloud\u2019s privacy-by-design approach and default data protection policies",
    "L2": "* Use SHAP and LIME in Vertex AI to interpret model predictions and identify bias \n* Apply model monitoring and evaluation tools to track fairness and compliance\n* Implement data governance workflows using Vertex AI and Google Cloud\u2019s integrated tools (e.g., Data Catalog, DLP API) \n* Configure IAM roles, VPC Service Controls, and Security Command Center for secure access and data protection",
    "L3": "* Design and deploy compliance-ready AI systems using Vertex AI\u2019s full suite of tools\n* Conduct ethical risk assessments and integrate responsible AI principles into model lifecycle \n* Lead enterprise-wide governance strategies using custom model cards, bias mitigation pipelines, and regulatory audit trails\n* Ensure continuous alignment with GDPR/HIPAA through automated monitoring, documentation, and stakeholder transparency",
    "hashId": "6820a0fd29894c79f30ea6a1bfb710d6bcef9fa35d888cac3e17f52629c9c085"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "Plotly",
    "L1": "* Understand foundational concepts of GDPR and HIPAA\n* Use basic tools for PII detection (e.g., Presidio)\n* Explore introductory bias detection and explainability techniques\n* Apply simple access control using IAM\n* Limited exposure to audit trails and compliance reporting",
    "L2": "* Apply Presidio, Clarify, and Vertex AI for privacy and fairness checks\n* Implement IAM policies, monitoring, and data governance workflows\n* Generate basic compliance reports and interpret audit logs\n* Use Plotly to visualize privacy metrics and fairness evaluations",
    "L3": "* Design and lead compliance strategies across GenAI systems\n* Integrate multiple tools for automated privacy, fairness, and explainability\n* Conduct ethical risk assessments and ensure full regulatory alignment\n* Build interactive dashboards in Plotly for real-time compliance monitoring",
    "hashId": "697c7d4a08f5a20142383c702d8a93f58c81a52080b1af6db9c9804a25663794"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "Seaborn",
    "L1": "* Understand basic principles of GDPR and HIPAA\n* Use Seaborn to visualize simple privacy-related metrics\n* Explore introductory tools for PII detection and access control\n* Limited experience with bias detection and explainability",
    "L2": "* Apply Seaborn to compare fairness and privacy metrics across models\n* Use tools like Presidio, Clarify, and Vertex AI for compliance checks\n* Implement IAM policies and monitor audit trails\n* Generate visual summaries for compliance reporting",
    "L3": "* Design Seaborn dashboards for real-time monitoring of privacy and fairness\n* Integrate multiple tools into Seaborn visual workflows for GDPR/HIPAA audits\n* Lead governance strategy with advanced visual analytics\n* Automate ethical risk assessments and reporting using Seaborn",
    "hashId": "237143c0f91c635a4a43be4c24626d1881d7e78278ff4834b7c83f8d41d5386b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Data privacy and compliance (GDPR, HIPAA)",
    "Tools": "Matplotlib",
    "L1": "* Understand basic principles of GDPR and HIPAA\n* Use simple tools for PII detection and access control\n* Limited exposure to bias detection and explainability\n* Minimal experience with compliance reporting and governance",
    "L2": "* Apply tools like Presidio, Clarify, and Vertex AI for privacy and fairness\n* Implement IAM policies and monitor audit trails\n* Generate basic compliance reports and contribute to governance workflows",
    "L3": "* Lead comprehensive compliance strategies across GenAI systems\n* Integrate privacy, fairness, and explainability into model lifecycle\n* Conduct ethical risk assessments and ensure full regulatory alignment\n* Build and manage governance frameworks for enterprise AI\n",
    "hashId": "7dc18d27de47ffdb4303b56808d533d78aae5a20aea3002ecc996c8d5360b46b"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Python (AI Fairness 360, Fairlearn, TruLens, Presidio, SHAP, LIME, Captum )",
    "L1": "* Basic understanding of fairness and privacy concepts\n* Able to run simple bias detection with AI Fairness 360 and Fairlearn\n* Initial exposure to SHAP, LIME, and Captum for model interpretability\n* Use Presidio for basic PII detection",
    "L2": "* Apply SHAP and LIME for feature-level explanations\n* Use Fairlearn and AI Fairness 360 for fairness evaluation\n* Implement TruLens for GenAI output monitoring\n* Configure Presidio for structured PII workflows",
    "L3": "* Design comprehensive transparency pipelines using SHAP, LIME, and Captum\n* Integrate TruLens and Fairlearn into enterprise GenAI systems\n* Lead ethical audits and explainability reviews\n* Customize tools for domain-specific fairness and privacy needs",
    "hashId": "a2db8b0d7ade725c17e5a77428675015cfb18a16af29c5d1300f0c43ba8b250c"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Jupyter Notebooks",
    "L1": "* Use Jupyter Notebooks to run basic fairness and explainability checks\n* Explore pre-built examples using tools like SHAP, LIME, and AI Fairness 360\n* Document simple model outputs and visualizations interactively\n* Understand the role of transparency in GenAI systems",
    "L2": "* Apply Fairlearn, TruLens, and Captum within Jupyter for model evaluation\n* Build reproducible workflows for bias detection and interpretability\n* Use notebooks to compare model behaviors and visualize feature importance\n* Collaborate on ethical reviews using annotated notebook outputs",
    "L3": "* Design comprehensive transparency pipelines using multiple tools in Jupyter\n* Automate explainability reporting and fairness audits\n* Integrate Jupyter with enterprise governance systems for traceability\n* Lead responsible AI initiatives with interactive, shareable notebooks",
    "hashId": "f39b072fb7336f201246e7109d6f2c2d2045b3f90cfb0c03e17401daf63fe988"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Azure (Responsible AI)",
    "L1": "* Understand the importance of transparency in AI systems\n* Use basic features of the Azure Responsible AI Dashboard to visualize model behavior\n* Explore built-in interpretability tools in Azure Machine Learning for simple models\n* Learn how to generate feature importance plots using the Interpretability SDK",
    "L2": "* Apply TabularExplainer, SHAP, and LIME within Azure ML to interpret model predictions \n* Use the Responsible AI Toolbox to debug models and compare cohorts \n* Evaluate both global and local explanations for fairness and trust\n* Document and share explainability insights using Azure ML notebooks and dashboards",
    "L3": "* Design and operationalize full transparency workflows using the Responsible AI Dashboard\n* Integrate multiple explainability techniques across model lifecycle for compliance and governance\n* Customize Azure tools to support domain-specific transparency needs (e.g., healthcare, finance)\n* Lead ethical audits and deploy explainable AI systems at scale using Azure ML and Responsible AI tooling",
    "hashId": "dc32955a55cf228a8b4d216e9a279d6c1bf5edae4fa9f92f2be32437ccf7ea26"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "AWS (SageMaker Clarify)",
    "L1": "* Understand the purpose of transparency and fairness in AI systems\n* Use SageMaker Clarify to detect basic bias in datasets and model predictions \n* Generate simple feature importance reports using built-in tools\n* Explore SHAP-based visualizations for tabular data explainability",
    "L2": "* Apply bias detection and mitigation techniques across sensitive attributes (e.g., gender, age) \n* Use partial dependence plots (PDPs) to understand feature impact on predictions \n* Integrate Clarify with SageMaker Experiments and Model Monitor for ongoing evaluation \n* Customize evaluations using curated datasets and scoring algorithms (e.g., BertScore, Rouge",
    "L3": "* Design and operationalize end-to-end explainability workflows using Clarify and SageMaker Pipelines \n* Conduct human-based evaluations for nuanced model outputs (e.g., tone, helpfulness, brand alignment) \n* Monitor semantic robustness and toxicity risks in generative AI outputs\n* Lead compliance audits and build governance dashboards using Clarify\u2019s integration with MLOps tools",
    "hashId": "c0dab24ea723d38fec49ec0e096d02cc41a0858f333838e2591001f8a9e82948"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "GCP (Vertex AI Explainable AI)",
    "L1": "* Understand the concept of Explainable AI (XAI) and its role in building trust \n* Use Vertex AI Workbench to explore basic feature attribution techniques like Integrated Gradients \n* Run pre-built notebooks to visualize model behavior and interpret predictions\n* Learn how to enable explainability during model training or prediction phases",
    "L2": "* Apply Vertex Explainable AI to tabular, image, and text models for real-time explanations \n* Use AutoML Tables and TensorFlow models with built-in explainability features\n* Evaluate model outputs using feature attribution dashboards and model cards\n* Implement workflows that include pairwise and pointwise evaluations to assess LLM outputs",
    "L3": "* Design and deploy custom explainability pipelines using Vertex AI Evaluation Service \n* Automate selection of optimal LLM responses based on quality metrics like accuracy, groundedness, and helpfulness\n* Extend explainability to multimodal inputs and outputs across industries\n* Lead governance initiatives by integrating explainability into enterprise AI systems and compliance workflows",
    "hashId": "f8ad493f52ab0025220e94fdb62fe85a5845fad7eecb6e0320bba5712789a1c4"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Tensorflow What if Tool",
    "L1": "* Understand the purpose of transparency and fairness in AI models\n* Use the What-If Tool (WIT) in TensorBoard or Jupyter Notebooks to visualize model predictions without writing code \n* Explore datasets and model outputs using built-in visualizations like Facets\n* Run basic counterfactual scenarios to see how small changes affect predictions ",
    "L2": "* Manually edit datapoints and observe prediction changes to understand model sensitivity\n* Use partial dependence plots to analyze feature impact on predictions\n* Evaluate model fairness across different subgroups (e.g., gender, age) using WIT\u2019s fairness metrics \n* Integrate WIT with TensorFlow Serving for real-time model analysis\n",
    "L3": "* Design comprehensive transparency workflows using WIT for model debugging, fairness auditing, and performance evaluation\n* Conduct counterfactual analysis to understand decision boundaries and improve model robustness \n* Combine WIT with other explainability tools (e.g., SHAP, LIME) for multi-layered insights\n* Lead governance initiatives by embedding WIT into enterprise AI pipelines for compliance and ethical oversight\n",
    "hashId": "85ff276d32a8cbe65c930088cb9e96485721c5ea27f186c98c697709d7dbb367"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Plotly",
    "L1": "* Basic understanding of fairness and explainability concepts\n* Use tools like Presidio and AI Fairness 360 for simple evaluations\n* Explore SHAP, LIME, and Captum through pre-built examples",
    "L2": "* Apply SHAP, LIME, and Captum for detailed model interpretation\n* Use Fairlearn and TruLens for fairness and GenAI output evaluation\n* Implement structured workflows for transparency in AI systems",
    "L3": "* Design and lead enterprise-grade transparency pipelines\n* Integrate multiple tools for comprehensive explainability and governance\n* Conduct ethical audits and ensure regulatory compliance",
    "hashId": "873e9b90c46e916a774d60e83e586aceb86a6e2132bfa78ca2512a4deef7f6a6"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Seaborn",
    "L1": "* Basic understanding of transparency and fairness in AI systems\n* Use Seaborn to visualize simple metrics from tools like Presidio, AI Fairness 360, and Fairlearn\n* Explore introductory interpretability techniques with SHAP, LIME, and Captum\n* Limited experience with model debugging or ethical evaluation",
    "L2": "* Apply Seaborn to compare fairness and feature attribution across models\n* Use SHAP, LIME, and Captum for detailed interpretability workflows\n* Integrate TruLens and Fairlearn for GenAI output evaluation\n* Build reproducible visualizations for transparency reporting and ethical reviews",
    "L3": "* Design Seaborn dashboards for real-time transparency monitoring\n* Combine multiple tools (e.g., SHAP + Fairlearn + TruLens) into comprehensive governance pipelines\n* Lead ethical audits and explainability reviews using Seaborn visual analytics\n* Automate reporting and integrate Seaborn into enterprise AI governance frameworks",
    "hashId": "95963ccae7b80c2cc36c4282bc5929cf9c2afee40e41ead1c63877050cc739eb"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Matplotlib",
    "L1": "* Basic understanding of fairness and interpretability concepts\n* Use Presidio and AI Fairness 360 for simple evaluations\n* Explore SHAP, LIME, and Captum through guided examples",
    "L2": "* Apply SHAP, LIME, and Captum for detailed model interpretation\n* Use Fairlearn and TruLens for fairness and GenAI output evaluation\n* Build structured workflows for transparency in AI systems",
    "L3": "* Design and lead enterprise-grade transparency pipelines\n* Integrate multiple tools for comprehensive explainability and governance\n* Conduct ethical audits and ensure regulatory compliance",
    "hashId": "afc7ec7006c775b2b9f26a212e9dfea24eb22cedacfbb5c9e13fa755d4d28fb6"
  },
  {
    "Category": "Gen AI",
    "Sub-Category": "Ethics, Governance & Responsible AI",
    "Sub-Sub-Category": "Transparency and explainability",
    "Tools": "Datasheets for Datasets",
    "L1": "* Understand the purpose of datasheets in documenting datasets\n* Recognize basic metadata fields (e.g., source, format, intended use)\n* Awareness of ethical concerns like bias, privacy, and consent\n* Use datasheets to support transparency in dataset selection",
    "L2": "* Create and interpret datasheets for datasets used in GenAI models\n* Evaluate datasets for fairness, inclusivity, and transparency\n* Document data collection methods, annotation processes, and limitations\n* Use datasheets to support model explainability and ethical reviews",
    "L3": "* Design comprehensive datasheets with embedded ethical risk assessments\n* Integrate datasheet practices into organizational governance workflows\n* Lead audits and reviews of datasets for regulatory compliance (e.g., GDPR, HIPAA)\n* Use datasheets to drive responsible AI development and stakeholder trust",
    "hashId": "4338d3c9237a5a1896fb8b05321a684453ec58f880706eeb0f89577d8a0b3ce2"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Requirements analysis , Modelling ",
    "Tools": "Java (MagicDraw + SysML Plugin,AnyLogic,Modelio  )",
    "L1": "* Understand basic concepts of systems modeling and requirements gathering\n* Use Modelio for simple UML diagrams and SysML blocks\n* Explore MagicDraw interface and SysML plugin for basic model creation\n* Run basic simulations in AnyLogic using pre-built templates\n",
    "L2": "* Apply SysML in MagicDraw for requirement traceability and system behavior modeling\n* Use AnyLogic for discrete event and agent-based simulations\n* Model system architecture and interactions using Modelio with Java integration\n* Perform requirement validation and consistency checks across models",
    "L3": "* Design and manage complex system models using MagicDraw + SysML Plugin\n* Integrate AnyLogic simulations with real-time data and optimization logic\n* Customize Modelio with Java extensions for domain-specific modeling\n* Lead requirement analysis workshops and model-based systems engineering (MBSE) initiatives",
    "hashId": "6063c3fd054fd5fd9c857afcea3340112d44007c1229e52850cdfdd915164e3e"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Requirements analysis , Modelling ",
    "Tools": "Web-based, REST API (Jama COnnect)",
    "L1": "* Understand basic concepts of requirements management and traceability\n* Navigate Jama Connect\u2019s web interface to view and organize requirements\n* Learn how REST APIs can be used to access and retrieve requirement data\n* Use Jama\u2019s built-in templates for simple requirement documentation",
    "L2": "* Create and manage requirement hierarchies and relationships in Jama Connect\n* Use REST APIs to automate requirement extraction and reporting workflows\n* Integrate Jama Connect with modeling tools and version control systems\n* Perform impact analysis and requirement validation using built-in features",
    "L3": "* Design and implement end-to-end requirement lifecycle workflows using Jama Connect and REST APIs\n* Develop custom integrations with external systems (e.g., PLM, ALM, MBSE tools)\n* Lead model-based systems engineering (MBSE) initiatives with real-time requirement synchronization\n* Use Jama\u2019s analytics and API capabilities to support compliance, traceability, and stakeholder reporting",
    "hashId": "d1d23f946b36bf965c3b691d6612edcfbab649aef33d76b70a510a12725cf8a3"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Requirements analysis , Modelling ",
    "Tools": ".NET, XML",
    "L1": "* Understand basic concepts of requirements documentation and system modeling\n* Use XML for simple data structuring and configuration files\n* Explore .NET tools like Visual Studio for basic form-based requirement input\n* Learn how XML schemas (XSD) define data formats for requirement exchange",
    "L2": "* Model requirements using .NET frameworks and serialize them into XML for interoperability\n* Use XML with XSLT for transforming and presenting requirement data\n* Integrate .NET applications with requirements management systems via XML-based APIs\n* Validate and manage requirement data using XML parsers and schema validation",
    "L3": "* Design and implement custom requirements modeling tools using .NET and XML\n* Develop RESTful services in .NET that consume and produce XML for system integration\n* Lead enterprise-level requirement workflows using XML-based standards (e.g., ReqIF)\n* Automate requirement traceability and reporting using .NET and XML technologies\n",
    "hashId": "0629276d652a092053e67b3531a4bc686b267fa4a58a92e5c7000734d19e6232"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Requirements analysis , Modelling ",
    "Tools": "C++, Python (ANSYS)",
    "L1": "* Understand basic principles of requirements analysis and system modeling\n* Use ANSYS Workbench with Python scripting for simple automation tasks\n* Explore C++ integration for basic simulation control and data handling\n* Familiarity with ANSYS file formats and model setup workflows",
    "L2": "* Develop Python scripts to automate parametric studies, data extraction, and report generation in ANSYS\n* Use C++ for extending ANSYS capabilities via custom plugins or solver modules\n* Model system requirements and link them to simulation parameters\n* Perform validation and verification of models against requirements",
    "L3": "* Design and implement complex simulation workflows using Python and C++ in ANSYS\n* Integrate ANSYS simulations with external systems for real-time data exchange\n* Lead model-based systems engineering (MBSE) initiatives with full traceability from requirements to simulation results\n* Optimize system architecture using advanced scripting and solver customization",
    "hashId": "7ccd2dbceedc9ce07068ff8fdd78b61a640be627649724e312794878e43556cc"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Requirements analysis , Modelling ",
    "Tools": "Web UI",
    "L1": "* Understand basic principles of requirements gathering and user interface design\n* Use simple form-based web UIs to capture and organize requirements\n* Familiarity with HTML/CSS for static UI layouts\n* Explore low-code/no-code platforms for visual requirement modeling",
    "L2": "* Design interactive web-based requirement dashboards using JavaScript frameworks (e.g., React, Angular)\n* Integrate web UIs with backend systems to store and retrieve requirement data\n* Use REST APIs to connect web UIs with modeling tools or databases\n* Implement basic validation and traceability features in the UI",
    "L3": "* Develop custom web applications for end-to-end requirements lifecycle management\n* Integrate real-time collaboration, versioning, and model visualization into the UI\n* Lead development of domain-specific modeling interfaces using advanced web technologies\n* Ensure compliance and governance through secure, scalable, and audit-ready web UI systems",
    "hashId": "ff1b956fbf5f257dbe6f980c8ed3558df355a98f2938a9311235b145aa00c615"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Integration Hardware and Software",
    "Tools": "C / C++ / Assembly (CMSIS, FreeRTOS, Zephyr RTOS, STM32Cube, AVR Libc, mbed OS, U-Boot, OpenOCD, GDB, JTAG)",
    "L1": "* Understand basic embedded systems concepts and microcontroller architecture\n* Write simple programs in C/C++ for microcontroller peripherals (GPIO, UART, etc.)\n* Use STM32Cube or mbed OS for basic configuration and code generation\n* Flash firmware using OpenOCD or JTAG tools\n* Debug simple applications using GDB or IDE-integrated debuggers",
    "L2": "* Develop multitasking applications using FreeRTOS or Zephyr RTOS\n* Use CMSIS for hardware abstraction and portability across ARM Cortex-M devices\n* Integrate AVR Libc for low-level control on AVR microcontrollers\n* Customize bootloaders using U-Boot for embedded Linux systems\n* Perform real-time debugging and trace analysis using JTAG, GDB, and OpenOCD",
    "L3": "* Architect and optimize full embedded systems with RTOS, bootloader, and hardware abstraction layers\n* Develop and port device drivers across platforms using CMSIS, Zephyr, or mbed OS\n* Implement secure boot and firmware update mechanisms using U-Boot and custom protocols\n* Lead hardware-software co-design for complex systems (e.g., automotive, industrial IoT)\n* Conduct performance profiling, memory analysis, and fault diagnostics using advanced debugging tools",
    "hashId": "ed9849c1a532e444af6db5998fb0f860afd536ac9f421c6868de161e3a30590e"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Integration Hardware and Software",
    "Tools": "Python (PySerial, PyVisa, PyMeasure, Robot Framework, Behave, SimPy, RPi.GPIO, Adafruit Blinka, NumPy, matplotlib, pandas)",
    "L1": "* Understands basic concepts of hardware-software integration.\n* Uses:\n  * PySerial for simple serial communication.\n  * PyVisa for basic instrument control.\n  * PyMeasure for straightforward measurement tasks.\n  * RPi.GPIO for basic GPIO pin control on Raspberry Pi.\n  * Adafruit Blinka for simple sensor interfacing.\n  * SimPy for basic process simulations.\n  * Robot Framework to run simple test cases.\n  * Behave for basic BDD scenarios.\n  * NumPy, pandas for basic data manipulation.\n  * matplotlib for simple plotting and visualization.",
    "L2": "* Applies multiple libraries to build integrated systems.\n* Capable of:\n  * Automating tests using Robot Framework and Behave.\n  * Interfacing with lab instruments using PyVisa and PyMeasure.\n  * Simulating systems with SimPy including resource constraints.\n  * Handling asynchronous communication with PySerial.\n  * Controlling hardware with RPi.GPIO and Adafruit Blinka.\n  * Analyzing and visualizing data using NumPy, pandas, and matplotlib.\n* Understands:\n  * Communication protocols (I2C, SPI, UART, SCPI).\n  * Integration of test automation with CI/CD pipelines.",
    "L3": "* Designs and architects robust hardware-software integrated systems.\n* Proficient in:\n  * Building modular test frameworks using Robot Framework and Behave.\n  * Developing custom drivers using PySerial, PyVisa, PyMeasure.\n  * Optimizing simulations with SimPy for performance.\n  * Architecting sensor networks using RPi.GPIO, Adafruit Blinka.\n  * Advanced data analytics and preprocessing with NumPy, pandas.\n  * Creating dynamic and interactive visualizations with matplotlib.\n* Leads:\n   * System architecture decisions and integration strategies.\n   * Debugging, performance tuning, and mentoring in hardware-software systems.",
    "hashId": "8feb24989044f1d21fa4bfbf1668a0e55d1d0a388e83e2ca1ae42b6577c2fb99"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Integration Hardware and Software",
    "Tools": "AWS IoT Core",
    "L1": "* Understands basic concepts of IoT and cloud integration.\n* Able to:\n  * Set up AWS IoT Core and register devices.\n  * Use MQTT protocol for simple message publishing/subscribing.\n  * Connect a basic sensor or microcontroller to AWS IoT Core.\n  * View device messages in AWS IoT Core console.\n* Use AWS IoT Core with simple Python scripts (e.g., using paho-mqtt).",
    "L2": "* Capable of building functional IoT systems with AWS services.\n* Able to:\n   * Implement secure communication using X.509 certificates.\n   * Create and manage IoT policies and roles in AWS IAM.\n   * Integrate AWS IoT Core with other services (e.g., Lambda, DynamoDB).\n   * Develop Python-based applications for real-time data streaming.\n   * Use Device Shadows for state management.\n   * Monitor and troubleshoot device connectivity and message flow.",
    "L3": "* Designs and manages scalable, secure IoT architectures.\n* Proficient in:\n  * Architecting multi-device systems with AWS IoT Core and edge computing.\n  * Implementing advanced analytics pipelines using AWS IoT Analytics or Kinesis.\n  * Designing fault-tolerant and secure device provisioning workflows.\n  * Integrating AWS IoT Core with ML models for predictive maintenance or anomaly detection.\n  * Optimizing performance and cost across AWS IoT services.\n  * Leading architecture reviews and mentoring teams on IoT best practices.",
    "hashId": "7d03f1fe463da5855a7621a8b74c0b73227697e734c3b083806ae178eba9cca0"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Integration Hardware and Software",
    "Tools": "Azure IoT Hub",
    "L1": "* Understands basic IoT concepts and Azure ecosystem.\n* Able to:\n  * Set up Azure IoT Hub and register devices.\n  * Send and receive messages using MQTT or HTTPS protocols.\n  * Use Azure Portal to monitor device-to-cloud communication.\n  * Connect simple devices (e.g., Raspberry Pi) using SDKs (Python, Node.js).\n  * Use Device Explorer or Azure CLI for basic operations.\n",
    "L2": "* Builds functional IoT solutions using Azure services.\n* Able to:\n  * Implement secure device authentication using symmetric keys or X.509 certificates.\n  * Use Device Twins for state synchronization and configuration.\n  * Route messages to other Azure services (e.g., Event Hub, Service Bus, Blob Storage).\n  * Develop Python applications using Azure IoT SDKs.\n  * Monitor and troubleshoot device connectivity and message flow.\n  * Use IoT Hub Metrics, Diagnostics, and Monitoring tools.",
    "L3": "* Designs and manages scalable, secure IoT architectures on Azure.\n* Proficient in:\n   * Implementing IoT Edge for local processing and offline capabilities.\n   * Designing automated device provisioning using DPS (Device Provisioning Service).\n   * Integrating IoT Hub with Azure Functions, Stream Analytics, and Time Series Insights.\n   * Architecting solutions for predictive analytics and ML integration.\n   * Optimizing performance, cost, and security across Azure IoT services.\n   * Leading architecture decisions and mentoring teams on Azure IoT best practices.",
    "hashId": "897a32f3a87484543711c557ad41c6c53a4ef4a103a2aced1dcad643751f28fd"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Integration Hardware and Software",
    "Tools": "Docker, Kubernetes",
    "L1": "* Understands containerization basics and orchestration concepts.\n* Able to:\n   * Install and run simple containers using Docker CLI.\n   * Write basic Dockerfiles to containerize Python or IoT applications.\n   * Use Docker Compose for multi-container setups.\n   * Understand basic Kubernetes components: Pods, Deployments, Services.\n   * Deploy simple applications to Minikube or local Kubernetes clusters.",
    "L2": "* Builds and manages containerized applications in development environments.\n* Able to:\n   * Optimize Dockerfiles for performance and size.\n   * Use Docker volumes and networks for persistent and connected services.\n   * Create and manage Kubernetes resources using kubectl and YAML manifests.\n   * Implement ConfigMaps, Secrets, and Health Probes in Kubernetes.\n   * Monitor container performance using tools like Docker Stats or Kubernetes Dashboard.\n   * Integrate containers with CI/CD pipelines.",
    "L3": "* Designs and operates scalable, secure container orchestration systems.\n* Proficient in:\n   * Architecting microservices using Docker and Kubernetes.\n   * Managing production-grade clusters with Helm, Ingress Controllers, and RBAC.\n   * Implementing auto-scaling, rolling updates, and blue-green deployments.\n   * Securing containers and clusters using network policies, image scanning, and role-based access.\n   * Integrating Kubernetes with cloud platforms (e.g., AKS, EKS, GKE).\n   * Leading DevOps practices and mentoring teams on container orchestration.",
    "hashId": "793dfc0e2ed3ac37426832640761851a8e58083d7259234b850446083856d4f7"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Systems  & Architecture",
    "Sub-Sub-Category": "Integration Hardware and Software",
    "Tools": "Java / JavaScript (Apache Camel, Spring Integration, Node-RED, Eclipse IoT, Swagger, Postman, REST Assured, MQTT.js, Socket.IO)",
    "L1": "* Understands basic integration and messaging concepts.\n* Able to:\n  * Use Postman to test REST APIs.\n  * Create simple flows in Node-RED for device communication.\n  * Use Swagger UI to explore and document APIs.\n  * Send/receive MQTT messages using MQTT.js.\n  * Build basic REST clients using JavaScript or Java.\n  * Understand simple message routing using Apache Camel or Spring Integration.\n  * Use Socket.IO for basic real-time communication in web apps.",
    "L2": "* Builds and tests integrated systems using Java/JavaScript frameworks.\n* Able to:\n   * Develop RESTful services and automate API testing using REST Assured.\n   * Design and deploy integration flows using Apache Camel and Spring Integration.\n    * Use Node-RED for edge-level automation and device orchestration.\n    * Implement real-time communication using Socket.IO with event-driven architecture.\n    * Use Swagger for API versioning and documentation.\n    * Integrate Eclipse IoT components for device management.\n    * Secure and monitor MQTT communication using MQTT.js and broker tools.",
    "L3": "* Architects and optimizes scalable integration platforms.\n* Proficient in:\n   * Designing enterprise-grade integration pipelines using Apache Camel and Spring Integration.\n   * Building microservices with full API lifecycle management using Swagger, Postman, and REST Assured.\n   * Creating complex automation and data flows in Node-RED integrated with cloud services.\n   * Implementing secure, scalable real-time systems using Socket.IO and MQTT.js.\n   * Integrating Eclipse IoT with cloud platforms for device provisioning and analytics.\n   * Leading architecture decisions for hybrid integration (cloud + edge).\n   * Mentoring teams on best practices in API design, testing, and IoT integration  .",
    "hashId": "ab3cdd35ea1c645c707b87f4393da926ec7727023e52049575002890675a56e3"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Front end ",
    "Tools": "HTML/CSS (Bootstrap, Tailwind CSS, Bulma, Sass, LESS)",
    "L1": "* Understands basic HTML and CSS structure and styling.\n* Able to:\n   * Create simple web pages using semantic HTML.\n   * Apply basic styling using Bootstrap, Tailwind CSS, or Bulma classes.\n   * Use pre-built components like buttons, forms, and grids.\n   * Write basic styles using Sass or LESS variables and nesting.\n   * Understand responsive design principles using framework utilities.\n",
    "L2": "* Builds responsive and modular front-end interfaces.\n* Able to:\n   * Customize themes and components in Bootstrap, Tailwind CSS, and Bulma.\n   * Use Sass or LESS for modular CSS architecture (mixins, functions, imports).\n   * Implement responsive layouts using grid and flexbox utilities.\n   * Optimize CSS for performance and maintainability.\n   * Integrate frameworks with JavaScript for dynamic UI behavior.\n   * Use build tools (e.g., Webpack, Vite) to compile Sass/LESS.",
    "L3": "* Architects scalable and maintainable front-end systems.\n* Proficient in:\n   * Designing custom UI systems using Tailwind CSS with configuration files.\n   * Extending Bootstrap or Bulma with custom components and utilities.\n   * Building design systems using Sass or LESS with reusable patterns.\n   * Implementing accessibility and cross-browser compatibility.\n   * Leading front-end architecture decisions and code reviews.\n   * Mentoring teams on CSS best practices and framework usage.",
    "hashId": "c6c894ba3a271dc410d3598a8c56e332e34de89c7d449cf06649599b3d4c16d4"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Front end ",
    "Tools": "ReactJS (Redux, React Router, Next.js, Material UI, Styled Components)",
    "L1": "* Understands basic ReactJS concepts and component structure.\n* Able to:\n   * Create simple React components using JSX.\n   * Use React Router for basic navigation between pages.\n   * Apply styles using Material UI components or Styled Components.\n   * Understand props, state, and component lifecycle.\n   * Build basic static pages using Next.js.",
    "L2": "* Builds dynamic and responsive React applications.\n* Able to:\n    * Manage application state using Redux with middleware (e.g., Thunk).\n    * Implement nested routing and dynamic routes using React Router.\n    * Use Next.js for server-side rendering and API routes.\n    * Customize themes and components using Material UI.\n    * Write modular and reusable styles using Styled Components.\n    * Optimize performance using memoization and lazy loading.",
    "L3": "* Architects scalable and maintainable front-end systems using React.\n* Proficient in:\n    * Designing complex state management flows using Redux Toolkit.\n    * Implementing advanced routing strategies and guards in React Router.\n    * Building full-stack applications with Next.js (SSR, ISR, API routes).\n    * Creating custom UI libraries using Material UI and Styled Components.\n    * Integrating React apps with CI/CD pipelines and testing frameworks.\n    * Leading front-end architecture decisions and mentoring teams on React best practices.",
    "hashId": "9fc0987a6ce4acd6846ce798eded07fcd7a783790c20aa462d0f99cc29584881"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Front end ",
    "Tools": "Angular (Angular CLI, RxJS, NgRx, Angular Material)",
    "L1": "* Understands basic Angular architecture and TypeScript syntax.\n* Able to:\n  * Create Angular projects using Angular CLI.\n  * Build simple components and templates.\n  * Use Angular Material for UI components like buttons, cards, and forms.\n  * Apply basic routing and navigation.\n  * Use RxJS for simple observables and subscriptions.",
    "L2": "* Builds modular and reactive Angular applications.\n* Able to:\n  * Structure applications using modules, services, and routing.\n  * Use RxJS operators for reactive programming (e.g., map, mergeMap, switchMap).\n   * Implement state management using NgRx (actions, reducers, effects).\n   * Customize and theme Angular Material components.\n   * Optimize performance using lazy loading and change detection strategies.\n   * Integrate REST APIs and handle asynchronous data flows.",
    "L3": "* Architects scalable and maintainable Angular applications.\n* Proficient in:\n  * Designing enterprise-grade applications with NgRx for complex state management.\n  * Creating reusable libraries and shared modules.\n  * Implementing advanced RxJS patterns and custom operators.\n  * Building responsive and accessible UIs with Angular Material.\n  * Leading architecture decisions, code reviews, and performance audits.\n  * Mentoring teams on Angular best practices and advanced CLI tooling.",
    "hashId": "835c6ab479464f0bf682cd890cdc03edde7a88322bbfe37cc7b298160b61c09c"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Front end ",
    "Tools": "TypeScript (Angular, NestJS (for full-stack), RxJS)",
    "L1": "* Understands basic TypeScript syntax and usage.\n* Able to:\n   * Write simple TypeScript classes, interfaces, and functions.\n   * Use TypeScript in Angular components and services.\n   * Understand basic observables using RxJS.\n   * Set up a basic NestJS project for full-stack development.\n   * Use decorators and modules in Angular and NestJS.",
    "L2": "* Builds modular and reactive applications using TypeScript.\n* Able to:\n   * Implement strong typing and generics in TypeScript.\n   * Use RxJS operators for reactive data handling (map, filter, switchMap).\n   * Develop REST APIs and services using NestJS with TypeScript.\n   * Create reusable Angular components and services with TypeScript.\n   * Handle asynchronous operations and error handling effectively.\n   * Use dependency injection and modular architecture in NestJS.",
    "L3": "* Architects scalable and maintainable TypeScript-based applications.\n* Proficient in:\n   * Designing complex reactive flows using advanced RxJS patterns.\n   * Building enterprise-grade full-stack applications using NestJS and Angular.\n   * Implementing decorators, interceptors, guards, and middleware in NestJS.\n   * Creating shared TypeScript libraries for use across front-end and back-end.\n   * Leading architecture decisions and mentoring teams on TypeScript best practices.\n   * Ensuring code quality through linting, testing, and strict typing.  ",
    "hashId": "80eb1e0604b76460825be8569b1976923ce8884626f2ddeb6e24ba3b2c65052e"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Front end ",
    "Tools": "Cypress",
    "L1": "* Understands basic concepts of automated UI testing.\n* Able to:\n   * Install and configure Cypress in a front-end project.\n   * Write simple test cases to check page load, button clicks, and form submissions.\n   * Use Cypress Test Runner to execute and debug tests.\n   * Understand basic selectors and assertions (cy.get(), cy.contains(), should()).",
    "L2": "* Builds robust and reusable test suites.\n* Able to:\n   * Organize tests using fixtures, commands, and page objects.\n   * Test dynamic content and asynchronous behavior.\n   * Use Cypress intercepts to stub network requests.\n   * Integrate Cypress with CI/CD pipelines.\n   * Perform cross-browser testing and visual validations.\n   * Use Cypress Dashboard for test analytics and reporting.",
    "L3": "* Architects scalable and maintainable test automation frameworks.\n* Proficient in:\n   * Designing modular test architecture with custom commands and reusable components.\n   * Implementing advanced mocking and stubbing strategies.\n   * Integrating Cypress with other tools (e.g., Allure, Mocha, Jenkins, GitHub Actions).\n   * Optimizing test performance and parallel execution.\n   * Leading test strategy, coverage planning, and mentoring teams on Cypress best practices.\n   * Ensuring accessibility and compliance testing using Cypress plugins. ",
    "hashId": "0573b1f4b23c30c12818d31d7b047590d2d820b9609b310768f2649ebb580860"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Front end ",
    "Tools": "Playwright",
    "L1": "* Understands basic concepts of browser automation and UI testing.\n* Able to:\n   * Install and set up Playwright in a front-end project.\n   * Write simple test scripts to validate page elements and user interactions.\n   * Use basic selectors and assertions (page.locator(), expect()).\n   * Run tests across multiple browsers (Chromium, Firefox, WebKit).\n   * Capture screenshots and trace logs for debugging.\n",
    "L2": "* Builds reliable and maintainable test suites.\n* Able to:\n   * Use Playwright Test Runner for organizing and executing test cases.\n   * Implement test fixtures and hooks (beforeEach, afterEach) for setup/teardown.\n   * Handle dynamic content and asynchronous operations.\n   * Integrate Playwright with CI/CD pipelines (e.g., GitHub Actions, Jenkins).\n   * Use Playwright Inspector and Trace Viewer for debugging complex flows.\n   * Perform API testing alongside UI testing.",
    "L3": "* Architects scalable and efficient test automation frameworks.\n* Proficient in:\n   * Designing modular test architecture with custom fixtures and reusable components.\n   * Implementing parallel test execution and test sharding for performance.\n   * Integrating Playwright with other tools (e.g., Allure, TestRail, Azure DevOps).\n   * Performing accessibility, localization, and visual regression testing.\n   * Leading test strategy, coverage planning, and mentoring teams on Playwright best practices.\n   * Ensuring cross-browser and cross-platform compatibility in enterprise-grade applications.",
    "hashId": "e782881e97ba99156d8cab50b60da50370832141e43b87c77d14eb911e76fe41"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "Java (Spring Boot, Hibernate, JPA, Apache Maven, Gradle)",
    "L1": "* Understands basic Java syntax and object-oriented programming principles.\n* Able to:\n    * Set up a simple Spring Boot application using Spring Initializr.\n    * Use Apache Maven or Gradle to build and run Java projects.\n    * Create basic REST APIs with Spring Boot controllers.\n    * Perform simple CRUD operations using JPA and Hibernate.\n    * Configure application properties and connect to a local database. ",
    "L2": "* Builds modular and data-driven back-end applications.\n* Able to:\n   * Implement layered architecture (Controller, Service, Repository).\n   * Use Spring Boot annotations for dependency injection and configuration.\n   * Handle relational data using Hibernate with entity relationships (OneToMany, ManyToOne).\n   * Use JPA queries and custom repository methods.\n   * Manage project dependencies and lifecycle using Maven or Gradle.\n   * Integrate with external APIs and handle exceptions gracefully.",
    "L3": "* Architects scalable and maintainable enterprise back-end systems.\n* Proficient in:\n  * Designing microservices using Spring Boot with advanced configurations.\n  * Optimizing ORM performance with Hibernate (lazy loading, caching, transactions).\n  * Implementing complex data models and custom JPA specifications.\n  * Managing multi-module projects and CI/CD pipelines using Maven or Gradle.\n  * Securing APIs using Spring Security and OAuth2.\n  * Leading architecture decisions, performance tuning, and mentoring teams on Java best practices.",
    "hashId": "8d2fec68e50732fc86fcce031fd4f28f30c73b1cb1d77c7cc8be077c0390f104"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "Python (Django, Flask, FastAPI, SQLAlchemy, Celery)",
    "L1": "* Understands basic Python syntax and web development concepts.\n* Able to:\n  * Set up simple web applications using Flask or Django.\n  * Create basic REST APIs with FastAPI.\n  * Use SQLAlchemy for basic ORM operations (CRUD).\n  * Configure routing, templates, and static files in Flask/Django.\n  * Understand basic asynchronous task concepts with Celery.",
    "L2": "* Builds modular and scalable back-end applications.\n* Able to:\n   * Develop RESTful APIs with authentication and validation using FastAPI or Django REST Framework.\n   * Use SQLAlchemy for complex queries and relationships.\n   * Implement background tasks and queues using Celery with Redis or RabbitMQ.\n   * Structure applications using blueprints (Flask) or apps (Django).\n   * Handle middleware, error handling, and request lifecycle.\n   * Integrate with databases and third-party APIs.",
    "L3": "* Architects robust and high-performance back-end systems.\n* Proficient in:\n   * Designing microservices and asynchronous APIs using FastAPI and Celery.\n   * Optimizing ORM performance and managing migrations with SQLAlchemy or Django ORM.\n   * Implementing advanced security, caching, and scalability strategies.\n   * Building reusable modules and packages for enterprise applications.\n   * Leading architecture decisions, performance tuning, and mentoring teams on Python back-end best practices.\n   * Integrating CI/CD pipelines and containerization (Docker, Kubernetes) for deployment.",
    "hashId": "6379b11517c479693212bbdd0678571709d800da04c116650cb2daf61baaa06d"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "JavaScript / TypeScript (Node.js, Express.js, NestJS, Sequelize, Prisma)",
    "L1": "* Understands basic JavaScript/TypeScript syntax and server-side development.\n* Able to:\n   * Set up a basic Node.js server using Express.js.\n   * Create simple REST APIs with routing and middleware.\n   * Use Sequelize or Prisma for basic database operations (CRUD).\n   * Understand basic TypeScript types and decorators (in NestJS).\n   * Build simple modules and services using NestJS CLI.",
    "L2": "* Builds modular and scalable back-end applications.\n* Able to:\n   * Structure applications using controllers, services, and modules in NestJS.\n   * Use TypeScript features like generics, interfaces, and decorators effectively.\n   * Implement relational data models and associations using Sequelize or Prisma.\n   * Handle middleware, error handling, and request validation.\n   * Integrate REST APIs with authentication and authorization.\n   * Use environment variables and configuration management.",
    "L3": "* Architects robust and high-performance back-end systems.\n* Proficient in:\n   * Designing microservices and event-driven architectures using NestJS.\n   * Implementing advanced ORM features (transactions, migrations, custom queries).\n   * Optimizing performance and scalability of APIs.\n   * Securing APIs using JWT, OAuth2, and role-based access control.\n   * Integrating with GraphQL, WebSockets, and third-party services.\n   * Leading architecture decisions, code reviews, and mentoring teams on Node.js/TypeScript best practices.",
    "hashId": "c56fc30eeb6ad952405600be252ec0856d5b640a23ee9910dd19b3229ac801e5"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "C# / .NET (ASP.NET Core, Entity Framework, LINQ, NuGet)",
    "L1": "* Understands basic C# syntax and .NET project structure.\n* Able to:\n   * Create simple web APIs using ASP.NET Core.\n   * Use Entity Framework for basic CRUD operations.\n   * Write simple queries using LINQ.\n   * Manage dependencies using NuGet packages.\n   * Configure routing and middleware in ASP.NET Core.",
    "L2": "* Builds modular and data-driven back-end applications.\n* Able to:\n   * Implement layered architecture (Controllers, Services, Repositories).\n   * Use Entity Framework Core with migrations and relational mapping.\n   * Write complex LINQ queries for data filtering and transformation.\n   * Secure APIs using authentication and authorization (JWT, Identity).\n   * Optimize performance using asynchronous programming and caching.\n   * Integrate third-party libraries and tools via NuGet.",
    "L3": "* Architects scalable and enterprise-grade .NET applications.\n* Proficient in:\n   * Designing microservices and distributed systems using ASP.NET Core.\n   * Implementing advanced data access patterns with Entity Framework (e.g., Unit of Work, CQRS).\n   * Writing optimized and maintainable LINQ queries.\n   * Managing large-scale solutions with custom NuGet packages and CI/CD integration.\n   * Implementing advanced security, logging, and monitoring strategies.\n   * Leading architecture decisions, performance tuning, and mentoring teams on .NET best practices.",
    "hashId": "e7f10303d20a205a85f5aaa58d43b5f1c629d28419fe981bfac509c74c057579"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "PostgreSQL",
    "L1": "* Understands basic relational database concepts and SQL syntax.\n* Able to:\n   * Install and configure PostgreSQL locally or on cloud platforms.\n   * Create databases, tables, and perform basic CRUD operations.\n   * Use pgAdmin or CLI tools for database management.\n   * Write simple SQL queries with SELECT, INSERT, UPDATE, DELETE.\n* Understand basic data types and constraints (e.g., PRIMARY KEY, NOT NULL).",
    "L2": "* Builds and manages relational data models effectively.\n* Able to:\n   * Design normalized schemas with relationships (FOREIGN KEY, JOIN).\n   * Use advanced SQL features like GROUP BY, HAVING, CTE, and WINDOW FUNCTIONS.\n   * Implement indexing and query optimization techniques.\n   * Manage roles, permissions, and access control.\n   * Integrate PostgreSQL with back-end frameworks (e.g., Django ORM, SQLAlchemy, Sequelize, Prisma).\n  * Perform backups, restores, and migrations.",
    "L3": "* Architects scalable and high-performance database solutions.\n* Proficient in:\n   * Designing complex schemas for enterprise applications.\n   * Implementing stored procedures, triggers, and functions.\n   * Optimizing performance with advanced indexing, partitioning, and query tuning.\n   * Managing replication, high availability, and disaster recovery strategies.\n   * Securing databases with encryption, auditing, and role-based access.\n   * Leading database design decisions and mentoring teams on PostgreSQL best practices.",
    "hashId": "a8d81ab186534bdf13d822ca13b8a09a5b80ae3f840936e42c8615cb1d0076f7"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "MySQL",
    "L1": "* Understands basic relational database concepts and SQL syntax.\n* Able to:\n   * Install and configure MySQL locally or on cloud platforms.\n   * Create databases, tables, and perform basic CRUD operations.\n   * Use MySQL Workbench or CLI tools for database management.\n   * Write simple SQL queries (SELECT, INSERT, UPDATE, DELETE).\n   * Understand basic data types, constraints, and indexing.",
    "L2": "* Designs and manages relational schemas effectively.\n* Able to:\n  * Normalize data and define relationships using FOREIGN KEY, JOIN, etc.\n  * Write advanced SQL queries (GROUP BY, HAVING, UNION, SUBQUERIES).\n  * Implement stored procedures, triggers, and views.\n  * Optimize queries using indexing and query plans.\n  * Manage user roles, permissions, and access control.\n  * Integrate MySQL with back-end frameworks (e.g., Django ORM, SQLAlchemy, Sequelize).",
    "L3": "* Architects scalable and secure database solutions.\n* Proficient in:\n   * Designing complex schemas for enterprise applications.\n   * Implementing advanced performance tuning (e.g., query optimization, caching).\n   * Managing replication, clustering, and high availability setups.\n   * Securing databases with encryption, auditing, and role-based access.\n   * Leading database design decisions and mentoring teams on MySQL best practices.\n   * Automating backups, restores, and migrations using scripts or tools.",
    "hashId": "63e0d7f311a35db5590ef736cd9c51b2fe5bcdfe2d7f5d172b573c97910b81b1"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "GraphQL",
    "L1": "* Understands basic API concepts and GraphQL structure.\n* Able to:\n   * Set up a simple GraphQL server using libraries like Apollo Server or Express-GraphQL.\n   * Define basic schemas, queries, and mutations.\n   * Test GraphQL endpoints using tools like GraphQL Playground or Postman.\n   * Understand the difference between GraphQL and REST APIs.\n   * Use GraphQL client libraries (e.g., Apollo Client) for basic data fetching.\n",
    "L2": "* Builds modular and efficient GraphQL APIs.\n* Able to:\n   * Implement resolvers with nested queries and mutations.\n   * Use GraphQL SDL (Schema Definition Language) effectively.\n   * Integrate GraphQL with databases using ORMs (e.g., Prisma, Sequelize).\n   * Handle authentication and authorization in GraphQL APIs.\n   * Use Fragments, Variables, and Directives for reusable and dynamic queries.\n   * Optimize performance with query batching and caching.",
    "L3": "* Architects scalable and secure GraphQL systems.\n* Proficient in:\n    * Designing federated schemas and microservices using Apollo Federation.\n    * Implementing subscriptions for real-time data using WebSockets.\n    * Securing GraphQL APIs with depth limiting, query complexity analysis, and rate limiting.\n    * Integrating GraphQL with CI/CD pipelines and monitoring tools.\n    * Leading architecture decisions and mentoring teams on GraphQL best practices.\n    * Optimizing GraphQL performance with persisted queries and schema stitching.",
    "hashId": "243e9c7dc100042b9533d4cfd9d4e328be401fd2cb2f032b4415f568ce827559"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "Jenkins",
    "L1": "* Understands basic CI/CD concepts and Jenkins architecture.\n* Able to:\n  * Install and configure Jenkins locally or on a server.\n  * Create simple freestyle jobs to build and test code.\n  * Use basic Jenkins plugins (e.g., Git, Email Notification).\n  * Trigger builds manually or via source code changes.\n  * Understand basic pipeline syntax and job configuration.",
    "L2": "* Builds automated CI/CD pipelines for development workflows.\n* Able to:\n   * Write and manage Declarative Pipelines using Jenkinsfile.\n   * Integrate Jenkins with version control systems (e.g., GitHub, GitLab).\n   * Use build triggers, post-build actions, and environment variables.\n   * Implement parameterized builds and artifact archiving.\n   * Use Jenkins plugins for testing, code quality (e.g., JUnit, SonarQube), and deployment.\n  * Monitor and troubleshoot build failures and logs.",
    "L3": "* Architects scalable and secure CI/CD systems using Jenkins.\n* Proficient in:\n   * Designing complex Scripted Pipelines with shared libraries.\n   * Managing Jenkins in distributed environments with agents and nodes.\n   * Securing Jenkins with role-based access control and credential management.\n   * Integrating Jenkins with containerization tools (e.g., Docker, Kubernetes).\n   * Automating deployment workflows across multiple environments.\n   * Leading DevOps practices and mentoring teams on Jenkins best practices.",
    "hashId": "1727dc65eb257c0d047437304734c0f60846eadab73ec1ed4b51bad96625e29b"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "AWS Lambda",
    "L1": "* Understands basic serverless computing concepts.\n* Able to:\n   * Create and deploy simple AWS Lambda functions via the AWS Console.\n   * Trigger Lambda functions using basic event sources (e.g., S3, API Gateway).\n   * Write simple handler functions in Python, Node.js, or Java.\n   * Monitor executions using CloudWatch Logs.\n   * Understand basic IAM roles and permissions for Lambda.",
    "L2": "* Builds event-driven applications using AWS Lambda.\n* Able to:\n   * Use AWS CLI or SAM (Serverless Application Model) for deployments.\n   * Integrate Lambda with services like DynamoDB, SNS, SQS, and Step Functions.\n   * Handle environment variables, timeouts, and memory configurations.\n   * Implement error handling, retries, and dead-letter queues.\n   * Optimize cold start performance and manage concurrency.\n   * Use API Gateway to expose Lambda functions as RESTful APIs.",
    "L3": "* Architects scalable and secure serverless systems using AWS Lambda.\n* Proficient in:\n  * Designing microservices and event-driven architectures with Lambda.\n  * Implementing CI/CD pipelines using CodePipeline, CodeBuild, and SAM/Serverless Framework.\n  * Securing Lambda functions with fine-grained IAM policies and VPC integration.\n  * Monitoring and optimizing performance using CloudWatch Metrics, X-Ray, and custom logging.\n  * Managing large-scale deployments with versioning, aliases, and traffic shifting.\n  * Leading architecture decisions and mentoring teams on serverless best practices.",
    "hashId": "d3322e7b057ce522b49b425ef9180fc8d768e8e6d19474bb90001f31582e6602"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Back end ",
    "Tools": "Azure Functions",
    "L1": "* Understands basic serverless and event-driven architecture concepts.\n* Able to:\n   * Create and deploy simple Azure Functions using the Azure Portal.\n   * Write basic function code in supported languages (e.g., C#, Python, JavaScript).\n   * Trigger functions using HTTP requests or simple event sources (e.g., Blob Storage).\n   * Monitor executions using Azure Monitor and Application Insights.\n   * Understand basic function bindings and configuration.",
    "L2": "* Builds event-driven applications using Azure Functions.\n* Able to:\n   * Develop and deploy functions using Azure CLI, Visual Studio Code, or Azure Functions Core Tools.\n   * Use input/output bindings for services like Cosmos DB, Queue Storage, Event Grid, and Service Bus.\n   * Implement durable workflows using Durable Functions.\n   * Handle environment variables, retries, and error handling.\n   * Secure functions using API keys, Azure AD, and managed identities.\n   * Integrate Azure Functions with CI/CD pipelines (e.g., GitHub Actions, Azure DevOps).",
    "L3": "* Architects scalable and secure serverless solutions using Azure Functions.\n* Proficient in:\n   * Designing microservices and distributed systems with Durable Functions and Function Chaining.\n   * Implementing advanced monitoring, logging, and alerting strategies.\n   * Optimizing performance with cold start mitigation and scaling configurations.\n   * Securing functions with network isolation, private endpoints, and role-based access control.\n   * Managing large-scale deployments with slots, versioning, and traffic routing.\n   * Leading architecture decisions and mentoring teams on Azure serverless best practices.",
    "hashId": "65661f2769be5c45b8bc51545b7775c904817f935efe8e89d0729fe9a3334014"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Code version control",
    "Tools": "Git CLI, GitHub, GitLab",
    "L1": "* Understands basic version control concepts.\n* Able to:\n   * Initialize a Git repository and track changes using Git CLI (git init, git add, git commit).\n   * Clone repositories from GitHub or GitLab.\n   * Push and pull code using remote repositories.\n   * Resolve simple merge conflicts.\n   * Use basic branching (git branch, git checkout).",
    "L2": "* Manages collaborative workflows and branching strategies.\n* Able to:\n   * Use Git CLI for advanced operations (rebase, stash, cherry-pick, tag).\n   * Implement branching models like Git Flow or Feature Branching.\n   * Create and manage pull/merge requests on GitHub/GitLab.\n   * Use GitHub Actions or GitLab CI/CD for automated workflows.\n   * Configure repository settings, access controls, and webhooks.\n   * Review code and manage issues using GitHub/GitLab tools.",
    "L3": "* Architects scalable and secure version control systems.\n* Proficient in:\n   * Designing and enforcing branching strategies and code review policies.\n   * Managing large-scale repositories with submodules and monorepos.\n   * Automating workflows using GitHub Actions, GitLab CI/CD, and custom scripts.\n   * Securing repositories with signed commits, protected branches, and audit logs.\n   * Leading DevOps integration with Git-based workflows.\n   * Mentoring teams on Git best practices and version control strategies.",
    "hashId": "e053815f8926fb9fd3dd0d9e0bded963f1b955a1615ea0b6d9a7c041905bc4b7"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Code version control",
    "Tools": "Jenkins",
    "L1": "* Understands basic concepts of version control and Jenkins automation.\n* Able to:\n   * Connect Jenkins to a Git repository (GitHub/GitLab).\n   * Trigger builds on code commits using webhooks or polling.\n   * Use Jenkins to clone and build code from version control systems.\n   * View commit history and build logs in Jenkins.\n   * Understand basic job configuration for source control integration.\n",
    "L2": "* Manages automated workflows integrated with version control.\n* Able to:\n    * Configure multibranch pipelines to handle feature branches and PRs.\n    * Use Jenkinsfile to define CI/CD pipelines triggered by Git events.\n    * Implement build triggers based on tags, branches, or merge requests.\n    * Integrate Jenkins with GitHub/GitLab for status checks and notifications.\n    * Use credentials and SSH keys securely for repository access.\n    * Manage versioned artifacts and releases through Jenkins pipelines.\n",
    "L3": "* Architects robust CI/CD systems with deep version control integration.\n* Proficient in:\n   * Designing scalable pipelines with shared libraries and GitOps workflows.\n   * Implementing advanced branching strategies (e.g., Git Flow, trunk-based development).\n   * Automating release management and rollback strategies using Git tags and Jenkins.\n   * Securing version control access with fine-grained permissions and audit trails.\n   * Leading DevOps practices and mentoring teams on Jenkins + Git integration best practices.\n   * Integrating Jenkins with GitHub/GitLab APIs for dynamic pipeline behavior.",
    "hashId": "ca8b5123a61fc68208fdaf682ea757aa584489023a1f7f5aab13e68ec87dfeed"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Code version control",
    "Tools": "Python (pip)",
    "L1": "* Understands basic concepts of Python package management.\n* Able to:\n   * Install packages using pip install.\n   * Use requirements.txt to manage dependencies.\n   * Check installed packages using pip list or pip freeze.\n   * Upgrade or uninstall packages using pip install --upgrade and pip uninstall.\n   * Understand the role of virtual environments (e.g., venv, virtualenv)",
    "L2": "* Manages dependencies across projects and environments.\n* Able to:\n    * Create and manage virtual environments for isolated development.\n    * Use pip-tools or pipenv for enhanced dependency management.\n    * Resolve dependency conflicts and version mismatches.\n    * Automate environment setup using requirements.txt and setup.py.\n    * Integrate pip workflows into CI/CD pipelines.\n    * Audit and update outdated packages securely.",
    "L3": "* Architects scalable and secure Python environments.\n* Proficient in:\n    * Designing reproducible environments using lock files and dependency pinning.\n     * Managing private or internal Python package repositories.\n     * Implementing dependency scanning and vulnerability checks.\n     * Creating custom Python packages and publishing them to PyPI.\n     * Leading best practices for dependency hygiene and environment isolation.\n     * Mentoring teams on pip usage, environment setup, and secure package management.",
    "hashId": "70f520e2c95bb997b7c7bcd890f2d5a7e95a44f9ec7aa749d31e14b73226d0b0"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Code version control",
    "Tools": "VS Code Git Extension",
    "L1": "* Understands basic version control operations using the VS Code interface.\n* Able to:\n   * Initialize and clone Git repositories using the VS Code Git Extension.\n   * Stage, commit, and push changes via the Source Control panel.\n   * View file changes, diffs, and commit history.\n   * Resolve simple merge conflicts using the built-in merge editor.\n   * Switch between branches and pull updates from remote repositories.",
    "L2": "* Manages collaborative workflows and integrates Git features efficiently.\n* Able to:\n  * Create and manage branches and tags using the extension.\n  * Use GitLens or other VS Code extensions for enhanced Git insights.\n  * Review and manage pull requests directly from VS Code (GitHub/GitLab integration).\n  * Configure remotes and handle multiple repositories.\n  * Use interactive rebase and stash operations via the UI or terminal.\n  * Customize Git settings and workflows within VS Code.",
    "L3": "* Architects efficient Git workflows and mentors teams using VS Code.\n* Proficient in:\n   * Designing and enforcing branching strategies using VS Code tools.\n   * Integrating Git workflows with CI/CD tools and extensions.\n   * Automating version control tasks using VS Code tasks and scripts.\n   * Troubleshooting complex merge conflicts and repository issues.\n   * Leading code reviews and repository hygiene using VS Code Git tools.\n   * Training teams on best practices for Git usage within VS Code environments.",
    "hashId": "d2754de7dfd1f945e5aeb0716f3fb27d443dab910b26635dd2309058a90c8ead"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Code version control",
    "Tools": "JavaScript (npm)",
    "L1": "* Understands basic concepts of package management in JavaScript.\n* Able to:\n  * Install packages using npm install.\n  * Initialize a project with npm init and create a package.json file.\n  * Use npm install --save and --save-dev for dependencies.\n  * Run scripts defined in package.json using npm run.\n  * Understand semantic versioning and basic dependency structure",
    "L2": "* Manages dependencies and scripts across multiple environments.\n* Able to:\n  * Use npm audit to identify and fix vulnerabilities.\n  * Manage and update packages using npm outdated and npm update.\n  * Create and manage custom scripts for build, test, and deploy workflows.\n  * Use .npmrc for configuration and registry management.\n  * Work with peerDependencies, optionalDependencies, and bundledDependencies.\n  * Integrate npm workflows into CI/CD pipelines",
    "L3": "* Architects scalable and secure JavaScript environments.\n* Proficient in:\n   * Creating and publishing custom npm packages to public or private registries.\n   * Implementing strict version control and lock files (package-lock.json) for reproducibility.\n   * Managing monorepos using tools like Lerna or Nx with npm.\n   * Automating dependency updates and vulnerability scanning.\n   * Leading best practices for dependency hygiene, script management, and environment setup.\n  * Mentoring teams on npm usage and secure package management strategies.",
    "hashId": "784458e0a7848782c85ca9eb3a8c1b93f09cf70b10ae1171dc08686146a561ce"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Python (Fabric, Invoke, Ansible, Pyinfra)",
    "L1": "* Understands basic DevOps and automation concepts.\n* Able to:\n   * Use Fabric or Invoke to run simple shell commands remotely via Python scripts.\n  * Write basic tasks and command-line interfaces using Invoke.\n  * Install and configure Ansible and Pyinfra for local testing.\n  * Understand inventory files and basic playbook/task structure.\n  * Automate repetitive tasks like file transfers or service restarts.",
    "L2": "* Builds reusable automation scripts and manages infrastructure tasks.\n* Able to:\n  * Create modular task files and roles using Ansible.\n  * Use Fabric for SSH-based deployment workflows.\n  * Automate provisioning and configuration using Pyinfra with declarative syntax.\n  * Handle environment-specific variables and secrets securely.\n  * Integrate automation scripts into CI/CD pipelines.\n  * Use Invoke for task orchestration and chaining.",
    "L3": "* Architects scalable and secure automation frameworks.\n* Proficient in:\n   * Designing infrastructure-as-code solutions using Ansible or Pyinfra across environments.\n   * Implementing complex workflows with Fabric and Invoke, including error handling and logging.\n   * Managing dynamic inventories, templating, and role-based access in Ansible.\n   * Integrating Python automation tools with cloud platforms (AWS, Azure, GCP).\n   * Leading DevOps strategy, tool selection, and mentoring teams on Python-based automation best practices.",
    "hashId": "4b180d393dac1557bb6c1f04650d8e47b81339c7c3d177287670c841692cab1e"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "JavaScript / Node.js (Serverless Framework, npm scripts)",
    "L1": "* Understands basic DevOps and automation concepts in Node.js.\n* Able to:\n   * Use npm scripts to automate simple tasks (e.g., build, test, lint).\n   * Install and configure the Serverless Framework.\n   * Deploy basic serverless functions to AWS or other cloud providers.\n   * Understand the structure of serverless.yml and basic CLI commands.\n  * Run local development and testing using npm run commands.",
    "L2": "* Builds and manages automated workflows for deployment and operations.\n* Able to:\n   * Create custom npm scripts for multi-step automation (e.g., build \u2192 test \u2192 deploy).\n   * Use environment variables and configuration files in Serverless Framework.\n   * Integrate Serverless Framework with CI/CD pipelines (e.g., GitHub Actions, GitLab CI).\n   * Manage multiple services and stages (dev, test, prod) in serverless deployments.\n  * Handle permissions, secrets, and IAM roles securely.\n  * Monitor and troubleshoot deployments using logs and dashboards.",
    "L3": "* Architects scalable and secure DevOps systems using Node.js tools.\n* Proficient in:\n   * Designing complex serverless architectures with multiple functions, triggers, and resources.\n   * Implementing advanced deployment strategies (e.g., blue-green, canary) using Serverless Framework.\n   * Creating reusable npm scripts and CLI tools for team-wide automation.\n   * Managing infrastructure as code with plugins and extensions in Serverless Framework.\n   * Leading DevOps strategy, tool selection, and mentoring teams on best practices with Node.js automation.\n  * Integrating observability tools (e.g., Datadog, CloudWatch) into serverless workflows.",
    "hashId": "5fa562cc7865acbfcadd123e11b8c7ded18ed28b419ce1d700443c7acf026f45"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Jenkins",
    "L1": "* Understands basic CI/CD and Jenkins architecture.\n* Able to:\n   * Install and configure Jenkins locally or on a server.\n   * Create and run freestyle jobs for basic build tasks.\n   * Connect Jenkins to version control systems (e.g., GitHub, GitLab).\n   * Trigger builds manually or via source code changes.\n   * View build logs and basic job history.",
    "L2": "* Builds automated pipelines and integrates Jenkins with development workflows.\n* Able to:\n   * Write and manage Declarative Pipelines using Jenkinsfile.\n   * Use build triggers, post-build actions, and environment variables.\n   * Integrate Jenkins with tools like Docker, SonarQube, and Slack.\n   * Configure parameterized builds, artifact archiving, and test reporting.\n   * Manage Jenkins plugins for extended functionality.\n   * Implement basic security and access control.",
    "L3": "* Architects scalable and secure CI/CD systems using Jenkins.\n* Proficient in:\n   * Designing complex Scripted Pipelines and shared libraries.\n   * Managing distributed builds with agents and nodes.\n   * Securing Jenkins with role-based access control, credentials, and audit logging.\n   * Integrating Jenkins with cloud platforms (AWS, Azure, GCP) and Kubernetes.\n   * Automating deployment workflows across multiple environments.\n   * Leading DevOps strategy and mentoring teams on Jenkins best practices.",
    "hashId": "95ac22961226e777e1b54db129d10a758d2bbf37019f129cc1efdcfa92e17774"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Gitlab CI /CD",
    "L1": "* Awareness of CI/CD concepts: Understand what Continuous Integration and Continuous Deployment mean.\n* Basic GitLab usage: Familiarity with GitLab UI and basic Git operations.\n* Simple pipeline creation:\n  * Writing basic .gitlab-ci.yml files.\n  * Defining simple jobs and stages.\n* Using GitLab Runners: Understand what runners are and how they execute jobs.\n* Artifact handling: Produce and store build artifacts.\n* Merge Request Pipelines: Enable and view basic MR pipelines ",
    "L2": "* Pipeline optimization:\n  * Use before_script, after_script, needs, and allow_failure keywords.\n  * Implement conditional pipelines and workflow rules.\n* Testing integration:\n  * Define and run unit, integration, and end-to-end tests.\n  * Include code coverage jobs.\n* Security scanning:\n  * Integrate GitLab\u2019s security scanners.\n  * Manage secrets and sensitive data.\n* Component reuse:\n  * Use include, extends, and templates for modular pipeline design.\n* Troubleshooting:\n  * Read job outputs and debug common errors.\n  * Use GitLab Duo for root cause analysis",
    "L3": "* Complex pipeline orchestration:\n  * Design multi-project pipelines.\n  * Use YAML anchors and inheritance for DRY principles.\n* Deployment strategies:\n  * Configure deployment rules and environments.\n  * Tag builds and manage releases via GitLab UI.\n* CI/CD Catalog usage:\n  * Create, version, and share reusable CI/CD components.\n* Leadership & coaching:\n  * Guide others in CI/CD best practices.\n  * Customize pipelines for team or organizational needs.\n* Thought leadership:\n  * Contribute to GitLab CI/CD templates or documentation.\n  * Evangelize CI/CD practices across teams  ",
    "hashId": "9701989e533cf1e612dc35941e022cef967ab15de7770eb0511ee6af884c9e89"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "GitHub Actions",
    "L1": "* Understand GitHub Actions basics:\n  * What workflows, jobs, steps, actions, and runners are .\n* Create simple workflows:\n  * Use .github/workflows directory.\n  * Trigger workflows on push or pull requests.\n* Use pre-built actions:\n  * Integrate basic actions from GitHub Marketplace.\n* Run basic CI tasks:\n  * Checkout code, install dependencies, run tests.\n* Configure secrets:\n  * Store and use secrets securely in workflows.",
    "L2": "* Build CI/CD pipelines:\n  * Automate testing, linting, and deployment.\n* Use matrix builds:\n   * Run jobs across multiple environments (e.g., Node.js versions).\n* Optimize workflows:\n   * Use caching for dependencies.\n   * Break workflows into reusable components.\n* Integrate with DevOps tools:\n  * Docker: Build and push images.\n  * Kubernetes: Deploy using kubectl.\n  * Terraform: Manage infrastructure as code1.\n* Prepare for certification:\n  * Author and maintain workflows and actions.\n  * Consume workflows and manage GitHub Actions for enterprise ",
    "L3": "* Design scalable workflows:\n  * Use self-hosted runners for custom environments.\n  * Implement reusable workflows across repositories.\n* Security and governance:\n   * Enforce secrets management and access controls.\n   * Audit workflows for vulnerabilities.\n* Enterprise-level integration:\n   * Manage GitHub Actions across teams and organizations.\n   * Implement CI/CD across microservices and cloud platforms.\n* Mentor and lead:\n   * Guide teams in GitHub Actions best practices.\n   * Contribute custom actions to the GitHub Marketplace",
    "hashId": "65b71633e495b8433e2df40e3f8475f44107d149fc55dfe07670966354091688"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Docker",
    "L1": "* Understand containerization vs virtualization.\n* Run containers using public Docker images.\n* Write basic Dockerfile and use docker-compose.\n* Learn Docker networking basics.\n* Apply basic security practices for Docker containers.",
    "L2": "* Write optimized Dockerfile with multi-stage builds.\n* Use Docker volumes and networks effectively.\n* Implement container security best practices.\n* Use alternative tools like BuildKit, Kaniko, Buildah .\n",
    "L3": "* Migrate from Docker to containerd or other runtimes.\n* Integrate Docker with CI/CD pipelines.\n* Secure container supply chain and image scanning.\n* Contribute to Docker tooling or open-source projects .\n",
    "hashId": "9c408958de9d1b9bdcc05bb4f54a37819ae0427fec2c4cbe88e9a15a41f0d8dd"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Kubernetes",
    "L1": "* Understand Kubernetes architecture: Pods, Deployments, Services .\n* Use managed Kubernetes services (GKE, EKS, AKS).\n* Work with basic objects: ConfigMaps, Secrets, Jobs, CronJobs.\n* Deploy simple applications using Helm charts.\n* Learn basic resource management and RBAC .",
    "L2": "* Configure advanced scheduling: affinity, anti-affinity, topology spread.\n* Implement autoscaling and node pools for cost optimization.\n* Use Service Mesh (e.g., Istio) for traffic management.\n* Apply network policies and sealed secrets.\n* Monitor clusters using Prometheus and Grafana",
    "L3": "* Design and manage complex multi-tenant clusters.\n* Implement GitOps workflows using ArgoCD or Flux.\n* Apply OPA (Open Policy Agent) for policy enforcement.\n* Manage cluster upgrades and disaster recovery.\n* Optimize performance and resource usage at scale ",
    "hashId": "2ec512118f5c4e354860faf6fc51bd8a28190f6bd99fb2689b12cb25d04da19c"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "AWS CodePipeline",
    "L1": "* Understand CI/CD concepts and AWS CodePipeline architecture.\n* Create basic pipelines using AWS Console.\n* Integrate with CodeCommit and CodeBuild for simple workflows.\n* Trigger pipelines on code changes or manual approvals.\n",
    "L2": "* Implement multi-stage pipelines with CodeDeploy and Lambda.\n* Use artifacts and manage secrets via AWS Secrets Manager.\n* Integrate automated testing within pipelines.\n* Troubleshoot pipeline failures and optimize performance .",
    "L3": "* Design complex CI/CD workflows across multiple AWS services (EC2, ECS, EKS).\n* Implement GitOps and blue/green or canary deployments.\n* Use CodeArtifact for secure artifact management.\n* Automate pipeline creation using CloudFormation or CDK .",
    "hashId": "217f2e0514381bdca8bd12959ebbede9e69d3a2fe9e6ea75eece32d2e5fddd4e"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Amazon CloudWatch",
    "L1": "* Set up basic monitoring and logging.\n* Create CloudWatch Alarms for CPU, memory, and disk usage.\n* Use CloudWatch Logs for application debugging.\n* Understand CloudWatch Events and basic dashboard creation",
    "L2": "* Monitor application health using CloudWatch and Elastic Beanstalk.\n* Use CloudWatch Agent for custom metrics.\n* Set up log filters and metric filters.\n* Integrate CloudWatch with SNS for alerting and notifications",
    "L3": "* Build advanced dashboards with cross-service metrics.\n* Implement anomaly detection and predictive monitoring.\n* Use CloudWatch Logs Insights for deep log analysis.\n* Integrate CloudWatch with third-party observability tools (e.g., Prometheus, Grafana)",
    "hashId": "ae247a985119de9200ae9cedc2fc7ea548dda2a7f5a37c71d74fdee232b687f8"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Azure DevOps",
    "L1": "* Understand DevOps principles and Azure DevOps architecture.\n* Use Azure Boards for basic project tracking.\n* Create simple CI/CD pipelines using Azure Pipelines.\n* Integrate with Git for version control.\n* Use Azure Repos for source code management .",
    "L2": "* Implement multi-stage CI/CD pipelines with approvals and gates.\n* Use YAML pipelines and templates for modular design.\n* Integrate with external tools (e.g., Jenkins, GitHub Actions).\n* Apply Infrastructure as Code using ARM templates or Terraform.\n* Manage secrets using Azure Key Vault.\n",
    "L3": "* Design enterprise-grade CI/CD workflows across multiple environments.\n* Implement GitOps and automated rollback strategies.\n* Integrate security and compliance checks into pipelines.\n* Use Azure DevOps REST APIs for automation and extensibility.\n* Mentor teams and contribute to DevOps governance.",
    "hashId": "d23388184f739cd51cb1f3f8095484940452991cc5ef031b652993978fb8617a"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "Azure Monitor",
    "L1": "* Set up basic monitoring for Azure resources.\n* Create simple alerts and dashboards.\n* Use built-in metrics and logs for troubleshooting.\n* Understand the role of Application Insights .",
    "L2": "* Configure custom metrics and log queries using Log Analytics.\n* Set up Application Insights for distributed tracing.\n* Use Azure Monitor Alerts with action groups.\n* Monitor performance and availability of applications",
    "L3": "* Build advanced dashboards with cross-resource metrics.\n* Implement predictive monitoring and anomaly detection.\n* Integrate with third-party observability tools (e.g., Grafana, Prometheus).\n* Use Azure Policy and Blueprints for governance and compliance.\n* Optimize cost and performance using insights from monitoring data",
    "hashId": "9789007eb5bfad6415c73609633ca3805cddf0bb23547b215211b236c75cad18"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Devops",
    "Tools": "GCP (Cloud Build, Stackdriver)",
    "L1": "* Cloud Build: Create basic build triggers, use pre-built builders, write simple cloudbuild.yaml.\n* Stackdriver (Cloud Monitoring & Logging): Enable monitoring/logging, view basic metrics/logs, set up simple alerts and dashboards.",
    "L2": "* Cloud Build: Build multi-step pipelines, integrate with GitHub/Bitbucket, deploy to GKE/App Engine, use custom containers.\n* Stackdriver: Create custom dashboards, log-based metrics, integrate with Cloud Trace/Profiler, monitor hybrid environments.",
    "L3": "* Cloud Build: Design enterprise-grade CI/CD workflows, implement GitOps, secure pipelines, automate with Spinnaker.\n* Stackdriver: Use anomaly detection, predictive monitoring, integrate with Prometheus/Grafana, monitor microservices/serverless apps.",
    "hashId": "b1b1a6d8a275fdc10a950d140105d6c78afb2954e76866d05a18bf40f198ec3c"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Agile",
    "L1": "* Understand Agile principles and values (from the Agile Manifesto).\n* Familiarity with Scrum and Kanban frameworks.\n* Participate in daily stand-ups, sprint planning, and retrospectives.\n* Use basic Agile tools (e.g., Jira, Azure Boards, Trello).\n* Collaborate in cross-functional teams.",
    "L2": "* Facilitate Agile ceremonies and manage sprint backlogs.\n* Apply user story mapping and estimation techniques (e.g., story points, planning poker).\n* Track velocity and burn-down charts for team performance.\n* Implement Agile metrics for continuous improvement.\n* Coordinate across multiple Agile teams (Scrum of Scrums).",
    "L3": "* Lead Agile transformations and coach teams on Agile maturity.\n* Customize Agile practices for enterprise environments (SAFe, LeSS, Nexus).\n* Align Agile delivery with DevOps and CI/CD pipelines.\n* Use Agile analytics for strategic decision-making.\n* Drive cultural change and stakeholder engagement in Agile adoption.",
    "hashId": "0d3a1e1ee03ec3f02c40b899f96be81b048b7a89555160a8b6afb1b86b7c13c4"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Kanban",
    "L1": "* Understand Kanban principles: visualizing work, limiting WIP (Work in Progress), managing flow.\n* Use basic Kanban boards (physical or digital tools like Trello, Jira).\n* Track tasks through stages (To Do, In Progress, Done).\n* Participate in daily stand-ups and basic flow discussions.\n* Recognize the importance of cycle time and lead time.",
    "L2": "* Configure digital Kanban boards with custom workflows.\n* Apply WIP limits and optimize flow efficiency.\n* Use cumulative flow diagrams to identify bottlenecks.\n* Implement service classes and prioritization strategies.\n* Coordinate Kanban across multiple teams or services.",
    "L3": "* Design and manage enterprise-level Kanban systems.\n* Integrate Kanban with DevOps and CI/CD pipelines.\n* Use advanced metrics (e.g., throughput, flow efficiency) for forecasting and planning.\n* Coach teams on Lean principles and continuous delivery.\n* Customize Kanban for portfolio-level work and strategic initiatives.",
    "hashId": "422b32b5665f916dfb72cff60d3b73dd5101c968eecb65995ae4a109dfe00d73"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "SAFe",
    "L1": "* Understand SAFe principles, Lean-Agile mindset, and the Four SAFe configurations.\n* Participate in Agile Release Trains (ARTs) and basic PI (Program Increment) planning.\n* Use SAFe terminology: Epics, Capabilities, Features, Stories.\n* Collaborate within cross-functional teams and ARTs.\n* Familiarity with SAFe roles: Scrum Master, Product Owner, RTE (Release Train Engineer).",
    "L2": "* Facilitate PI planning and manage team-level backlogs.\n* Apply SAFe metrics: velocity, predictability, flow efficiency.\n* Coordinate across teams using Scrum of Scrums and ART sync.\n* Implement Lean Portfolio Management and Agile budgeting.\n* Use SAFe tooling (e.g., Jira Align, Azure DevOps with SAFe templates).",
    "L3": "* Lead SAFe transformations across departments or enterprise.\n* Coach teams and leadership on SAFe adoption and maturity.\n* Customize SAFe practices for organizational needs (e.g., hybrid models).\n* Align strategy with execution using Portfolio Kanban and OKRs.\n* Drive continuous improvement through Inspect & Adapt workshops and DevOps integration.",
    "hashId": "95f5a6a87127f4c813ac22a24ec72ab966e6c412888be19f059e6b9aa52c2ed4"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Jenkins",
    "L1": "* Understand Jenkins fundamentals and its role in CI/CD.\n* Install and configure Jenkins locally or on a server.\n* Create basic freestyle jobs and pipelines.\n* Use Jenkins UI to trigger builds and view logs.\n* Integrate with version control systems (e.g., Git).\n",
    "L2": "* Develop scripted and declarative pipelines using Jenkinsfile.\n* Use plugins for notifications, test reports, and integrations.\n* Configure build triggers (e.g., webhook, cron).\n* Manage credentials and secrets securely.\n* Monitor build performance and job history.",
    "L3": "* Design scalable Jenkins architecture with master-agent setup.\n* Implement pipeline-as-code across multiple projects.\n* Integrate Jenkins with Docker, Kubernetes, and cloud platforms.\n* Apply security best practices and role-based access control.\n* Automate infrastructure provisioning and deployments using Jenkins with Terraform or Ansible.",
    "hashId": "412b997c898e164f9a4b421bd6cb056a6f1623fefc4fd51bdf48d3b0d9adc91a"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "GitLab CI/CD",
    "L1": "* Understand CI/CD fundamentals and GitLab\u2019s role in Agile workflows.\n* Create basic .gitlab-ci.yml files for simple pipelines.\n* Use GitLab UI to trigger builds and view job logs.\n* Integrate pipelines with Git repositories and merge requests.\n* Participate in Agile ceremonies with CI/CD feedback loops.",
    "L2": "* Build modular pipelines using include, extends, and templates.\n* Integrate automated testing, code coverage, and security scans.\n* Use GitLab CI/CD to support Agile sprints and continuous delivery.\n* Apply conditional logic and workflow rules in pipelines.\n* Collaborate across teams using GitLab issues, boards, and milestones.\n",
    "L3": "* Design enterprise-grade CI/CD workflows aligned with Agile release trains.\n* Implement GitOps, blue/green deployments, and environment strategies.\n* Use GitLab CI/CD Catalog for reusable components across Agile teams.\n* Align Agile metrics (velocity, cycle time) with pipeline performance.\n* Coach teams on Agile delivery using GitLab CI/CD best practices.\n",
    "hashId": "06ed7cc0bf485219e15acf8e27bca789fc8ac1d6f18de1481aaf61680fedc279"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Selenium",
    "L1": "* Understand the basics of automated testing and Selenium\u2019s role in Agile development.\n* Write simple test scripts using Selenium WebDriver (e.g., in Java, Python).\n* Automate browser actions like clicking, typing, and navigation.\n* Run tests locally and validate UI functionality.\n* Use Selenium IDE for record-and-playback testing.",
    "L2": "* Develop modular and reusable test frameworks using Page Object Model (POM).\n* Integrate Selenium tests into CI/CD pipelines (e.g., Jenkins, GitLab CI).\n* Handle dynamic elements, waits, and cross-browser testing.\n* Use Selenium Grid for parallel and distributed test execution.\n* Collaborate with QA teams during Agile sprints for continuous testing.",
    "L3": "* Design scalable test automation frameworks integrated with BDD tools (e.g., Cucumber, Behave).\n* Implement test reporting and analytics (e.g., Allure, ExtentReports).\n* Optimize test execution with Docker containers and cloud-based testing platforms (e.g., BrowserStack, Sauce Labs).\n* Align Selenium testing with Agile metrics (e.g., defect leakage, test coverage).\n* Mentor teams on test automation strategy and Agile quality practices.",
    "hashId": "09372d2c62de0d58f59ad60a131b7f0d90b81b4e3a329013cbffe2b53a4a819f"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Sprint Reports",
    "L1": "* Understand the purpose of sprint reports in Agile: tracking progress, identifying blockers, and improving team performance.\n* Use basic tools (e.g., Jira, Azure Boards) to generate sprint summaries.\n* Report on completed vs. committed work.\n* Participate in sprint reviews and retrospectives using report insights.",
    "L2": "* Customize sprint reports to include velocity, burn-down charts, and story point trends.\n* Analyze team performance across sprints using historical data.\n* Identify patterns in blockers, spillovers, and scope changes.\n* Share sprint insights with stakeholders for transparency and alignment.\n* Use sprint reports to support continuous improvement discussions.",
    "L3": "* Design automated sprint reporting dashboards integrated with Agile tools.\n* Align sprint metrics with business outcomes and OKRs.\n* Use predictive analytics to forecast sprint capacity and delivery risks.\n* Coach teams on interpreting and acting on sprint data.\n* Integrate sprint reports into portfolio-level Agile planning and governance.",
    "hashId": "70b4528048e86cc7ae2e36f36a0064cfb60ca62c04b8e3ec4ef78b8a1a687009"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Swagger (OpenAPI)",
    "L1": "* Understand the purpose of Swagger/OpenAPI in documenting RESTful APIs.\n* Use Swagger UI to explore and test API endpoints.\n* Write basic OpenAPI specifications in YAML or JSON.\n* Generate documentation from annotations in code (e.g., using Swagger in Spring Boot or Flask).\n* Collaborate with teams using Swagger-generated API docs during Agile sprints.",
    "L2": "* Design complete API contracts using OpenAPI 3.0 standards.\n* Use Swagger Codegen or OpenAPI Generator to scaffold client/server code.\n* Validate API specs and integrate with CI/CD pipelines.\n* Manage versioning and backward compatibility of API definitions.\n* Collaborate across Agile teams using shared API documentation and mock servers.",
    "L3": "* Implement API-first development workflows in Agile environments.\n* Integrate Swagger/OpenAPI with API gateways and service meshes.\n* Automate testing and security validation using OpenAPI specs.\n* Customize Swagger UI and documentation for enterprise needs.\n* Coach teams on best practices for scalable, maintainable API design.",
    "hashId": "f4ae49061b7a35337b1a0a6016ef8de5e3de6f4cde5ac1a7a7a91fb9dd8c3f7c"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Jira",
    "L1": "* Understand Jira\u2019s role in Agile project management.\n* Navigate Jira boards (Scrum/Kanban) and view issues.\n* Create and update basic issue types: stories, tasks, bugs.\n* Participate in sprints using Jira: view backlog, move items across statuses.\n* Use filters and basic dashboards for team visibility.",
    "L2": "* Configure workflows, custom fields, and issue types.\n* Manage sprint planning, backlog grooming, and retrospectives in Jira.\n* Use Jira Query Language (JQL) for advanced filtering.\n* Generate sprint reports: velocity, burn-down, cumulative flow.\n* Integrate Jira with development tools (e.g., GitHub, Bitbucket, CI/CD platforms).",
    "L3": "* Design and manage Jira projects across multiple teams or programs.\n* Implement scaled Agile frameworks (e.g., SAFe) using Jira Align or Advanced Roadmaps.\n* Automate workflows using Jira automation rules.\n* Customize dashboards for executive reporting and Agile metrics.\n* Coach teams on Agile best practices using Jira as a central collaboration tool.",
    "hashId": "e94abe79ef087c0fc67a33de5c8b7848881d9088e5f582d9a88dc382ddaf2041"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Confluence",
    "L1": "* Understand Confluence as a collaboration and documentation tool in Agile environments.\n* Create and edit basic pages for sprint notes, retrospectives, and meeting summaries.\n* Use templates for project documentation and team wikis.\n* Link Jira issues to Confluence pages for traceability.\n* Collaborate with team members using comments and mentions.",
    "L2": "* Organize content using spaces, labels, and hierarchical page structures.\n* Customize templates for Agile ceremonies (e.g., sprint planning, retrospectives).\n *Embed Jira reports, charts, and macros for dynamic updates.\n* Manage permissions and access controls for team spaces.\n* Use Confluence for knowledge sharing and decision tracking across Agile teams",
    "L3": "* Design Confluence spaces for scaled Agile frameworks (e.g., SAFe, LeSS).\n* Automate documentation workflows using Confluence and Jira integrations.\n* Implement governance and content lifecycle strategies.\n* Use analytics to track engagement and content effectiveness.\n* Coach teams on using Confluence for Agile transparency, alignment, and continuous improvement.",
    "hashId": "a64417cbabd86f6cf46caa4fcc76e5a32b6c1fd3dc43dc5d0f5a7a7cec8c6c97"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Software Engineering & Development",
    "Sub-Sub-Category": "Agile",
    "Tools": "Cypress",
    "L1": "* Understand Cypress as a modern JavaScript-based end-to-end testing framework.\n* Write basic test cases to validate UI elements and user interactions.\n* Run tests locally and view results in the Cypress Test Runner.\n* Use Cypress Dashboard for visual feedback and debugging.\n* Integrate Cypress tests into Agile sprint cycles for early feedback.",
    "L2": "* Build reusable test components using Page Object Model or custom commands.\n* Handle asynchronous behavior, fixtures, and API mocking.\n* Integrate Cypress with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions).\n* Use Cypress for regression testing during Agile sprint reviews.\n* Generate test reports and track test coverage.",
    "L3": "* Design scalable test automation frameworks with Cypress and BDD tools (e.g., Cucumber).\n* Run parallel tests using Cypress Cloud or Docker containers.\n* Implement visual testing and performance monitoring.\n* Align Cypress testing with Agile metrics (e.g., defect density, test reliability).\n* Coach teams on Cypress best practices and Agile quality engineering.",
    "hashId": "76c37b8a045c1afc400ece865567848af562b39be3056bba64b156bee4498dbe"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "AWS (EC2, S3, Lambda, CloudFormation, ECS, EKS, IAM, CloudWatch, CodeDeploy)",
    "L1": "* Launch EC2 instances, store data in S3, and manage IAM users/roles.\n* Create basic Lambda functions and monitor resources with CloudWatch.\n* Deploy simple applications using CodeDeploy.\n* Write basic CloudFormation templates for infrastructure provisioning.",
    "L2": "* Design scalable architectures using EC2, S3, and VPC.\n* Implement IAM policies, automate deployments with CodePipeline.\n* Deploy containerized apps using ECS and EKS.\n* Use CloudWatch Logs and Metrics for performance tracking.\n* Apply lifecycle policies and encryption in S3.",
    "L3": "* Architect fault-tolerant, cost-optimized multi-tier systems.\n* Use advanced CloudFormation features (nested stacks, macros).\n* Secure environments with fine-grained IAM, KMS, and GuardDuty.\n* Manage Kubernetes clusters with EKS and build serverless apps with Lambda.\n* Optimize monitoring and cost using CloudWatch Insights and Trusted Advisor.",
    "hashId": "1d0f5061bd8a61b3a481653ded6a8ef4a861df6918f7de818b42281d0ccfc224"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Microsoft Azure (Azure VMs, App Services, Azure Functions, Bicep, ARM Templates, AKS, Azure DevOps, Monitor)",
    "L1": "* Launch and manage Azure VMs, App Services, and Azure Functions.\n* Use Azure Portal for provisioning and monitoring.\n* Write basic ARM templates and understand Bicep syntax.\n* Set up simple Azure Monitor alerts and dashboards.\n* Use Azure DevOps for basic CI/CD and work tracking.",
    "L2": "* Design scalable apps using App Services and Functions.\n* Automate deployments with Bicep and ARM templates.\n* Deploy containers using AKS (Azure Kubernetes Service).\n* Implement multi-stage pipelines in Azure DevOps.\n* Monitor performance and logs using Azure Monitor and Log Analytics.",
    "L3": "* Architect resilient, secure, and cost-optimized cloud solutions.\n* Use advanced Bicep modules and nested ARM templates.\n* Manage production-grade AKS clusters with Helm, RBAC, and autoscaling.\n* Integrate Azure DevOps with GitHub, Terraform, and policy tools.\n* Implement predictive monitoring and custom dashboards in Azure Monitor.",
    "hashId": "ef7064f1b658c90628c879456010be38d8bc18b52165eebb3a51c5064f9b024a"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Google Cloud Platform ( Compute Engine, Cloud Functions, Cloud Run, Deployment Manager, GKE, Stackdriver)",
    "L1": "* Launch and manage virtual machines using Compute Engine.\n* Deploy basic serverless functions with Cloud Functions.\n* Run containerized apps using Cloud Run.\n* Use Deployment Manager for simple infrastructure provisioning.\n* Enable basic monitoring and logging with Stackdriver.",
    "L2": "* Design scalable apps using Cloud Run and Functions.\n* Automate deployments with Deployment Manager and Terraform.\n* Deploy and manage workloads using GKE (Google Kubernetes Engine).\n* Configure IAM roles, VPC networks, and load balancers.\n* Use Stackdriver for log analysis, alerting, and performance monitoring.",
    "L3": "* Architect resilient, secure, and cost-optimized cloud solutions.\n* Use advanced GKE features: autoscaling, multi-tenancy, Helm, RBAC.\n* Build event-driven architectures with Cloud Functions and Pub/Sub.\n* Implement full-stack observability using Cloud Trace, Profiler, and Error Reporting.\n* Apply IAM policies, resource hierarchy, and cost optimization strategies.",
    "hashId": "5988df38ba12d87f9c916960d10f013d538e74526db1cd6130cc00d948b0c889"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Docker",
    "L1": "* Understand containerization basics and Docker architecture.\n* Install Docker and run containers using CLI.\n* Build simple Docker images and use Docker Compose for multi-container apps.\n",
    "L2": "* Optimize Docker images and manage volumes and networks.\n* Implement resource limits and persistent storage.\n* Integrate Docker into CI/CD pipelines.",
    "L3": "* Secure Docker environments with best practices.\n* Automate orchestration with Docker Swarm or Kubernetes.\n* Use Docker in production-grade cloud-native architectures.",
    "hashId": "2a5690376f1607da3fd7aa61c419492211b6d49c9a1124a625e87d6136955227"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Kubernetes",
    "L1": "* Learn Kubernetes fundamentals: Pods, Deployments, Services.\n* Use kubectl to manage resources.\n* Deploy basic applications and YAML manifests ",
    "L2": "* Manage clusters and scale applications.\n* Use Helm charts for templated deployments.\n* Apply network policies and configure service discovery.\n* Deploy apps on cloud platforms like GKE",
    "L3": "* Design and manage large-scale clusters with RBAC, namespaces, and autoscaling.\n* Implement advanced security (CKS-level): PodSecurityPolicies, network policies.\n* Use service mesh (e.g., Istio) and observability tools.\n* Automate deployments with GitOps and CI/CD integrations",
    "hashId": "c7aa13d22087146ffa472fee77aad3e65ebd572f6645ef5d8ff8cf43a4bbb5f1"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Jenkins",
    "L1": "* Set up basic Jenkins jobs and builds.\n* Use Jenkins UI and integrate with Git.\n* Write simple scripts and understand Jenkinsfile structure.",
    "L2": "* Develop declarative and scripted pipelines.\n* Configure Jenkins agents and distributed builds.\n* Secure credentials and apply role-based access.\n* Optimize performance and troubleshoot pipelines.",
    "L3": "* Integrate Jenkins with Docker, Kubernetes, and cloud platforms.\n* Automate infrastructure provisioning using Terraform or Ansible.\n* Implement GitOps and advanced CI/CD strategies.\n* Design scalable Jenkins architecture and mentor teams.",
    "hashId": "9c7aae89f7318dd5d38f8b71cd102134bb9b53edb5c6093cfa26362ca68ec5ac"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Gitlab CI /CD",
    "L1": "* Understand CI/CD principles and GitLab architecture.\n* Create basic .gitlab-ci.yml files with simple jobs and stages.\n* Use GitLab Runners and configure build triggers.\n* Automate builds and tests with version control integration.\n* Deploy simple applications and manage environments via GitLab UI",
    "L2": "* Develop modular pipelines using include, extends, and templates.\n* Implement unit, integration, and end-to-end testing with code coverage.\n* Use caching, artifacts, and conditional logic to optimize pipelines.\n* Integrate Docker and Kubernetes for containerized deployments.\n* Monitor pipelines using Prometheus and Grafana; apply security scans ",
    "L3": "* Architect multi-project pipelines and merge trains.\n* Implement advanced deployment strategies: blue/green, canary, feature flags.\n* Scale runners across zones/regions and configure Kubernetes executors.\n* Optimize pipelines with parallelism, fail-fast testing, and reference tags.\n* Troubleshoot complex CI/CD scenarios and mentor teams on GitLab best practices",
    "hashId": "c322447c659ff5df470389582eaf499053cb5c2667048a72f68df119b015e073"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "GitHub Actions",
    "L1": "* Understand CI/CD basics and GitHub Actions workflow syntax.\n* Learn cloud service models and basic deployment types.\n* Use GitHub-hosted runners and manage secrets simply",
    "L2": "* Build reusable workflows and integrate secure cloud access (OIDC).\n* Use Infrastructure as Code (Terraform, CloudFormation).\n* Configure networking, monitoring, and enforce security practices.",
    "L3": "* Design scalable, secure cloud architectures with self-hosted runners.\n* Implement governance, compliance, and audit mechanisms.\n* Lead DevOps integrations and align roles with SFIA for strategic planning",
    "hashId": "c57bea4a7599bccaaa874b9cb0706df8bd31faf08f2e46ca55f5d31bcd9b9de3"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Cloud architecture and deployment",
    "Tools": "Cloudflare",
    "L1": "* Cloudflare Basics: Understand DNS, CDN, SSL setup, and basic firewall rules.\n* Cloud Concepts: Learn IaaS, PaaS, SaaS models and virtualization fundamentals.\n* GitHub Actions: Create simple CI/CD workflows for deploying static sites to Cloudflare Pages.\n* Linux & Networking: Basic Linux commands and virtual network setup ",
    "L2": "* Cloudflare Integration: Automate deployments using Cloudflare Workers and Pages via GitHub Actions.\n* Infrastructure as Code (IaC): Use Terraform or Cloudflare API for resource provisioning .\n* Security & Compliance: Implement IAM, encryption, and firewall rules; understand GDPR/HIPAA compliance.\n* DevOps Practices: Build CI/CD pipelines with GitHub Actions; use Docker and Kubernetes for container orchestration .",
    "L3": "* Cloud Architecture Design: Architect scalable, secure, and resilient systems using Cloudflare\u2019s edge network .\n* Automation & Monitoring: Implement advanced IaC, logging, and performance monitoring using Cloudflare analytics and GitHub workflows .\n* Governance & Optimization: Enforce deployment policies, optimize cost and resource usage, and manage multi-cloud strategies .\n* Strategic Leadership: Lead cloud transformation initiatives, align roles with SFIA levels, and mentor cross-functional teams",
    "hashId": "e3c831745c185e0ad056e91c11b04dd1cbd56c3331bc8891ac4aff731d060d72"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Monitoring and performance optimization",
    "Tools": "AWS CloudWatch",
    "L1": "* AWS CloudWatch Basics: Understand metrics, dashboards, logs, and alarms.\n* Monitoring Setup: Configure basic monitoring for EC2, S3, and Lambda.\n* Linux Familiarity: Use basic Linux commands for system performance checks.\n* Cloud Concepts: Learn IaaS, PaaS, SaaS models and AWS service overview ",
    "L2": "* CloudWatch Insights: Use CloudWatch Container Insights for Docker/Kubernetes monitoring.\n* Performance Engineering: Identify bottlenecks in microservices and infrastructure.\n* Automation: Integrate CloudWatch with CI/CD pipelines using Jenkins and GitHub Actions.\n* Alerting & Troubleshooting: Set up alarms, logs, and use AWS X-Ray for tracing issues ",
    "L3": "* Architecture Optimization: Design resilient, fault-tolerant systems with performance monitoring.\n* Advanced Metrics & Dashboards: Customize CloudWatch dashboards for multi-service environments.\n* DevOps Integration: Implement continuous performance testing and monitoring in CI/CD workflows.\n* Strategic Leadership: Align performance engineering with SFIA levels for cloud transformation",
    "hashId": "1956c044952ae8fa59c32987de82bfea16f4d5ca9b1cfcb036cb8a156d398970"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Monitoring and performance optimization",
    "Tools": "Azure Monitor",
    "L1": "* Azure Monitor Basics: Understand metrics, logs, and alerts.\n* Monitoring Setup: Configure basic monitoring for Azure VMs, storage, and web apps.\n* Dashboards: Create simple dashboards to visualize performance data.\n* Cloud Fundamentals: Learn IaaS, PaaS models and basic Azure services ",
    "L2": "* Log Analytics & Insights: Use Azure Log Analytics and Application Insights for deeper diagnostics.\n* Automation: Set up automated alerts and remediation using Azure Monitor and Logic Apps.\n* Performance Tuning: Identify and resolve performance bottlenecks in Azure-hosted applications .\n* Security & Compliance Monitoring: Monitor security posture using Azure Security Center integration",
    "L3": "* End-to-End Observability: Design comprehensive monitoring solutions across hybrid and multi-cloud environments.\n* Custom Metrics & Workbooks: Build advanced dashboards and workbooks tailored to business KPIs.\n* DevOps Integration: Integrate Azure Monitor with CI/CD pipelines for continuous performance feedback .\n* Strategic Optimization: Lead performance engineering initiatives aligned with SFIA levels and business goals ",
    "hashId": "624d5f30302c5f0f8a37884f50c21ecc091648e1a6055ff46d138d3c9208edab"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Monitoring and performance optimization",
    "Tools": "\nDatadog (APM, Infrastructure Monitoring, Log Management)",
    "L1": "* Metric Collection: Understand how to collect basic metrics using Datadog agents and integrations .\n* Dashboards & Alerts: Create simple dashboards and set up basic alerts for infrastructure health.\n* Log Management Basics: Learn to ingest and view logs for troubleshooting.\n* Application Monitoring: Set up basic APM for single-service applications",
    "L2": "* Distributed Tracing: Implement tracing across microservices to monitor request flows .\n* Anomaly Detection: Configure anomaly detection to identify unusual patterns in metrics and logs.\n* Log Pipelines: Define parsing rules and use logs for deeper diagnostics.\n* CI/CD Integration: Embed Datadog monitoring into CI/CD pipelines for real-time feedback",
    "L3": "* Performance Engineering: Optimize application and infrastructure performance using Datadog analytics .\n* Custom Dashboards & SLOs: Build advanced dashboards and define Service Level Objectives (SLOs) for reliability tracking .\n* Security & Access Control: Manage permissions, roles, and data privacy across teams.\n* Strategic Monitoring: Lead observability initiatives across hybrid/multi-cloud environments, aligning with SFIA levels",
    "hashId": "235cf64a968cc0b79700c2996a6844773f400e3ab5bb783796a77b6c2857e319"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Monitoring and performance optimization",
    "Tools": "Plugins",
    "L1": "* Plugin Basics: Understand what monitoring plugins are and how they extend observability tools (e.g., Nagios, Prometheus, Datadog).\n* Installation & Configuration: Learn to install and configure basic plugins for system metrics (CPU, memory, disk).\n* Alert Setup: Configure simple alerts using default plugin settings.\n* Tool Familiarity: Use built-in plugins in tools like Zabbix or Grafana for basic infrastructure monitoring .",
    "L2": "* Custom Plugin Use: Integrate third-party or custom plugins for application-specific monitoring.\n* Log & Metric Enrichment: Use plugins to enrich logs and metrics with contextual data.\n* Performance Tuning: Apply plugin-based insights to optimize resource usage and application performance .\n* Automation Integration: Connect plugins with CI/CD pipelines for automated performance checks.",
    "L3": "* Plugin Development: Build custom plugins tailored to enterprise monitoring needs.\n* Multi-Tool Integration: Use plugins to unify data across platforms (e.g., Datadog, Azure Monitor, AWS CloudWatch).\n* Advanced Observability: Implement plugins for distributed tracing, anomaly detection, and predictive analytics .\n* Strategic Optimization: Lead plugin-based observability initiatives aligned with SFIA levels and business KPIs.",
    "hashId": "eca031752cffadb8d7746f220aec32eb7624051f1a380379a214c2d4c0ab4bd8"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Cloud & Infrastructure Engineering",
    "Sub-Sub-Category": "Monitoring and performance optimization",
    "Tools": "Linux CLI",
    "L1": "* Use basic commands (top, df, free, ps) to monitor system health.\n* View logs with cat, tail, grep.\n* Understand file permissions and process management.",
    "L2": "* Use advanced tools (htop, vmstat, netstat) for deeper insights.\n* Automate log analysis with awk, sed, and shell scripts.\n* Monitor security and system events using journalctl",
    "L3": "* Tune system performance and kernel parameters.\n* Build custom dashboards integrating CLI outputs.\n* Automate monitoring pipelines and lead CLI-based diagnostics.",
    "hashId": "c14c247d12e2b6ebcae19b83eb573a0eb628ab713ac2f0bbe25ad9be9b82a67b"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Data Engineering & Analytics",
    "Sub-Sub-Category": "See Data Engineering",
    "Tools": "Apache Spark, Kafka, Snowflake, dbt, Airflow, SQL, Python",
    "L1": null,
    "L2": null,
    "L3": null,
    "hashId": "5c4166b798a6c0f4decb47b033f874ea6ce7913f11bdc6ef03a7ed5cb2d61dd9"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Slack, Microsoft Teams,",
    "L1": "* Basic use of Slack and Microsoft Teams for messaging, file sharing, and group chats.\n* Communication Etiquette: Understand norms for digital communication (e.g., tagging, status updates).\n* Team Awareness: Participate in team channels and respond to updates and requests.",
    "L2": "* Use shared channels, threaded conversations, and integrations (e.g., calendars, task boards).\n* Agile Coordination: Support sprint planning, stand-ups, and retrospectives via Slack/Teams.\n* Cross-functional Engagement: Collaborate across departments using structured communication and shared goals",
    "L3": "* Lead cross-functional initiatives using Slack/Teams as central collaboration hubs.\n* Workflow Automation: Implement bots, workflows, and integrations (e.g., Jira, Trello, GitHub) to streamline team operations.\n* Cultural Enablement: Foster inclusive, transparent, and high-performing team culture across tools and departments",
    "hashId": "2b85fd9f6d02b56b27d534b6a1a18884362ada371ba4fad977bdad93f527144f"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Jira",
    "L1": "* Navigate Jira boards, create and assign issues, and update statuses.\n* Basic Agile Practices: Participate in sprint planning, daily stand-ups, and retrospectives using Jira.\n* Team Communication: Use comments and mentions to collaborate within tasks ",
    "L2": "* Customize workflows, use labels, filters, and dashboards for team visibility.\n* Cross-functional Coordination: Collaborate across roles (e.g., dev, QA, design) using shared boards and epics.\n* Reporting & Metrics: Generate sprint reports, burndown charts, and velocity tracking",
    "L3": "* Align Jira projects with OKRs and business goals.\n* Automation & Integration: Use Jira automation rules and integrate with tools like Slack, GitHub, Confluence.\n* Leadership & Enablement: Drive cross-functional collaboration, resolve blockers, and foster team ownership ",
    "hashId": "43b6583b3973f13d3bae57440712b65f3f183ae5cd2f212bb2957cb6a13c4741"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Confluence",
    "L1": "* Navigate Confluence spaces, pages, and basic editing features.\n* Documentation Basics: Create and format meeting notes, project outlines, and team updates.\n* Collaboration: Comment on pages, tag team members, and share documents for feedback",
    "L2": "* Use templates for retrospectives, sprint planning, and decision logs.\n* Integration: Link Confluence with Jira for traceability between documentation and tasks.\n* Knowledge Management: Organize content using labels, macros, and page hierarchies",
    "L3": "* Design collaborative workspaces for cross-functional initiatives.\n* Governance & Standards: Implement documentation standards and workflows across teams.\n* Leadership & Culture: Foster transparency, psychological safety, and continuous learning through shared knowledge hubs",
    "hashId": "c6b9dd322b797ae593259d48790a02ff86e63ec4f74ec9c8fdf470678cd227e9"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Google Docs",
    "L1": "* Use Google Docs for basic document creation, editing, and sharing.\n* Real-time Collaboration: Co-edit documents with team members and use comments for feedback.\n* Version Control: Understand revision history and restore previous versions",
    "L2": "* Use templates for meeting notes, project plans, and retrospectives.\n* Cross-functional Input: Collaborate across departments by tagging, assigning tasks, and linking related documents.\n* Integration: Connect Google Docs with other tools like Google Sheets, Slides, and Drive for unified workflows.",
    "L3": "* Create centralized knowledge hubs for cross-functional teams.\n* Governance & Standards: Implement documentation standards, access controls, and approval workflows.\n* Leadership Enablement: Use Docs to drive alignment, facilitate decision-making, and support Agile rituals like sprint reviews and planning",
    "hashId": "12cc81616c530b30517df0710755ca7e2ae482648b59580be1bb2d09758539cb"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "MURAL",
    "L1": "* Navigate MURAL boards, use sticky notes, templates, and basic visual elements.\n* Participation: Contribute to brainstorming sessions and team retrospectives.\n* Communication: Use MURAL for visual collaboration during meetings and workshops",
    "L2": "* Use timers, voting, and private mode to guide structured team discussions.\n* Framework Application: Apply collaboration frameworks like shared vision, role clarity, and open communication using MURAL templates.\n* Cross-functional Engagement: Collaborate asynchronously across departments using visual workflows and shared boards ",
    "L3": "* Build and lead cross-functional collaboration frameworks tailored to team dynamics.\n* Innovation Enablement: Use MURAL to foster psychological safety, trust-building, and inclusive ideation .\n* Agile Integration: Align MURAL boards with Agile rituals (e.g., sprint planning, retrospectives) and integrate with tools like Jira and Confluence",
    "hashId": "5fd1e5d6685620d33a187dd5fc838672ce70193f455ccd048d9118874d16373d"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Jenkins",
    "L1": "* Understand Jenkins interface, job creation, and basic pipeline setup.\n* Team Communication: Use Jenkins to share build status and logs with team members.\n* Basic CI/CD Awareness: Recognize Jenkins\u2019 role in automating builds and tests within Agile workflows.",
    "L2": "* Configure multi-stage pipelines for cross-functional tasks (e.g., dev, QA, ops).\n* Integration: Connect Jenkins with tools like Jira, GitHub, Slack, and Confluence for collaborative workflows.\n* Monitoring & Feedback: Use Jenkins dashboards and notifications to support Agile ceremonies like sprint reviews and retrospectives.",
    "L3": "* Lead cross-functional initiatives using Jenkins as a central automation and feedback tool.\n* Governance & Optimization: Implement role-based access, audit trails, and performance tuning across Jenkins jobs.\n* Agile Enablement: Align Jenkins pipelines with Agile delivery goals, enabling continuous integration, testing, and deployment across teams.",
    "hashId": "d6328e2889b88e551f06fe2987102731e9ee0e5ea40bb1ef7d38845db4483c8a"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Adobe XD",
    "L1": "* Navigate Adobe XD interface, use basic design elements, and share prototypes.\n* Team Communication: Collaborate with team members using comments and shared links.\n* Design Basics: Create simple wireframes and mockups for team feedback.",
    "L2": "* Build clickable prototypes and use component states for dynamic interactions.\n* Cross-functional Collaboration: Work with developers, product managers, and UX researchers to align design with user needs and technical feasibility.\n* Integration: Sync Adobe XD with tools like Jira, Slack, and Confluence for seamless workflow",
    "L3": "* Create and manage reusable design systems and libraries for consistency across teams.\n* Strategic Enablement: Lead design workshops and co-creation sessions to drive innovation and alignment.\n* Agile Integration: Embed Adobe XD into Agile rituals (e.g., sprint planning, reviews) and support iterative design delivery",
    "hashId": "8c759d6c7cb857dd0df9321ac9d7364e07cfce24e39e11e1661a5f5f3456ac93"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Git (GitHub, GitLab, Bitbucket)",
    "L1": "* Understand basic Git commands (clone, commit, push, pull) and UI navigation in GitHub, GitLab, or Bitbucket.\n* Collaboration Basics: Participate in code reviews, comment on pull requests, and resolve merge conflicts with guidance.\n* Agile Integration: Link commits to Jira issues or tasks and follow team branching strategies.",
    "L2": "* Use feature branches, forks, and pull request workflows effectively.\n* Cross-functional Coordination: Collaborate with developers, testers, and designers using Git-integrated tools (e.g., CI/CD pipelines, issue tracking).\n* Automation & Integration: Set up basic CI/CD pipelines and integrate Git with tools like Jenkins, Slack, and Confluence ",
    "L3": "* Lead Git-based workflows across teams, enforce branching models, and manage repository governance.\n* DevOps Enablement: Architect and maintain advanced CI/CD pipelines, automate testing and deployment across environments.\n* Team Enablement: Mentor team members on Git best practices, resolve complex merge scenarios, and align Git workflows with Agile delivery goals",
    "hashId": "7e1be53721d76ffe5ff73ade59b08b176faa7397541ac3178b6f2a07ac97cef1"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Microsoft Power Automate",
    "L1": "* Understand the basics of Power Automate\u2014creating simple flows for notifications, approvals, and reminders.\n* Collaboration Basics: Use templates to automate repetitive tasks across Microsoft Teams, Outlook, and SharePoint.\n* Agile Awareness: Support team productivity by automating routine Agile tasks like meeting reminders or status updates.",
    "L2": "* Build custom flows that integrate multiple services (e.g., Jira, Excel, Teams) to streamline cross-functional collaboration.\n* Process Automation: Automate document approvals, task assignments, and feedback loops across departments.\n* Integration: Connect Power Automate with other tools like GitHub, Trello, or Confluence to support Agile delivery pipelines",
    "L3": "* Design scalable, enterprise-grade automation solutions that support end-to-end Agile workflows.\n* Governance & Optimization: Implement role-based access, error handling, and performance monitoring in flows.\n* Leadership Enablement: Lead cross-functional automation initiatives, aligning Power Automate usage with business goals and SFIA levels",
    "hashId": "28ee0fc7a7ffb682e2a7f00736357a78b40956c8f63770619c7683689d7798c2"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Azure AD",
    "L1": "* Understand Azure AD basics\u2014user management, group creation, and role assignments.\n* Collaboration Setup: Enable access to Microsoft 365 tools (Teams, SharePoint) for cross-functional teams.\n* Security Awareness: Learn basic identity and access management principles like single sign-on (SSO) and MFA.",
    "L2": "* Implement RBAC for secure collaboration across departments.\n* Integration: Connect Azure AD with third-party tools (e.g., Jira, GitHub, Confluence) to streamline workflows.\n* Automation: Use Power Automate and Azure AD connectors to automate onboarding, approvals, and access provisioning ",
    "L3": "* Design scalable identity governance models for large cross-functional teams.\n* Conditional Access & Compliance: Implement policies for secure, compliant collaboration across hybrid environments.\n* Leadership Enablement: Lead cross-functional initiatives by aligning Azure AD configurations with Agile delivery goals and SFIA levels",
    "hashId": "429320c5648a5614cf7bd3808e2d91b40248f6a913b1cd68d80abaff06aba1e2"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Cross-functional team collaboration",
    "Tools": "Google IAM",
    "L1": "* Understand basic concepts of Google IAM\u2014roles, permissions, and identity management.\n* Access Control: Assign predefined roles to users for Google Workspace and cloud services.\n* Collaboration Setup: Enable secure access to shared resources like Google Docs, Drive, and Meet for cross-functional teams.",
    "L2": "* Create custom IAM roles tailored to team needs and project scopes.\n* Audit & Monitoring: Use audit logs to track access and changes across collaborative environments.\n* Integration: Connect Google IAM with third-party tools (e.g., Jira, Slack, GitHub) to manage access across platforms.",
    "L3": "* Design scalable IAM strategies for large cross-functional teams with complex access needs.\n* Security & Compliance: Implement conditional access, least privilege principles, and compliance controls.\n* Strategic Enablement: Lead identity and access initiatives aligned with Agile delivery goals and SFIA levels, ensuring secure and efficient collaboration across departments.",
    "hashId": "c46befe13aefe876d09edeb354e1be92fe4a561e2ab06100f35097385789d49d"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Microsoft Teams",
    "L1": "* Use Microsoft Teams for scheduling and attending Agile ceremonies (e.g., sprint planning, stand-ups).\n* Basic Participation: Join meetings, share updates, and use chat or reactions to engage.\n* Documentation: Record meeting notes and decisions using Teams-integrated tools like OneNote or Wiki.",
    "L2": "* Assist in organizing Agile ceremonies using Teams features like breakout rooms, polls, and shared files.\n* Integration: Connect Teams with tools like Jira, Planner, and Outlook to streamline sprint planning and task tracking.\n* Collaboration: Use threaded conversations and @mentions to maintain clarity and alignment across cross-functional teams",
    "L3": "* Facilitate sprint planning, reviews, and retrospectives using Teams as a central collaboration hub.\n* Workflow Automation: Implement Power Automate flows to manage ceremony reminders, task updates, and feedback loops.\n* Strategic Enablement: Lead Agile transformation initiatives by aligning Teams usage with SFIA levels and organizational goals ",
    "hashId": "8322834c40b6c6e7b7d8a81a016d34c8f4609a41accfe98b89eebeb1fe326c99"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Zoom",
    "L1": "* Use Zoom for joining Agile ceremonies like sprint planning, stand-ups, and retrospectives.\n* Basic Participation: Share updates, use chat and reactions, and follow meeting etiquette.\n* Documentation Support: Record meetings and take notes using Zoom\u2019s built-in features or integrations.",
    "L2": "* Use breakout rooms, polls, and screen sharing to support Agile ceremonies.\n* Integration: Connect Zoom with tools like Jira, Confluence, and MURAL for collaborative planning and documentation.\n* Time Zone Coordination: Schedule ceremonies effectively for distributed teams using Zoom\u2019s calendar integrations",
    "L3": "* Facilitate sprint planning, reviews, and retrospectives with structured agendas and engagement techniques.\n* Workflow Optimization: Use Zoom recordings, whiteboards, and integrations to streamline feedback and decision-making.\n*Strategic Enablement: Lead Agile transformation initiatives by aligning Zoom usage with SFIA levels and organizational collaboration goals",
    "hashId": "f1db2819003b06f4806ea54b4996017d5e9fecceed34f160eb76580c475b2089"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Google Meet",
    "L1": "* Use Google Meet to join Agile ceremonies like sprint planning, stand-ups, and retrospectives.\n* Basic Participation: Share updates, use chat and reactions, and follow virtual meeting etiquette.\n* Documentation Support: Record meetings and take notes using Google Docs or Keep during sessions.\n",
    "L2": "* Use features like breakout rooms, polls, and screen sharing to support Agile ceremonies.\n* Integration: Connect Google Meet with Google Calendar, Jira, and Google Workspace tools for streamlined planning and collaboration.\n* Cross-functional Engagement: Coordinate ceremonies across distributed teams using shared agendas and collaborative documents.",
    "L3": "* Facilitate sprint planning, reviews, and retrospectives with structured agendas and engagement techniques.\n* Workflow Optimization: Use Google Meet recordings, collaborative whiteboards (e.g., Jamboard), and integrations to streamline feedback and decision-making.\n* Strategic Enablement: Lead Agile transformation initiatives by aligning Google Meet usage with SFIA levels and organizational collaboration goals ",
    "hashId": "d82e78c75114d397a828c95b357a5a80a835353c652f67fa357e5f2d96eaa808"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "WebEx",
    "L1": "DigitalEng\tCollaboration & Agile Delivery\tAgile ceremonies and sprint planning\tWebEx",
    "L2": "* Use breakout rooms, screen sharing, and polling features to support Agile ceremonies.\n* Integration: Connect WebEx with tools like Jira, Confluence, and MURAL to enhance planning and collaboration.\n* Cross-functional Engagement: Coordinate ceremonies across distributed teams using shared agendas and collaborative workspaces ",
    "L3": "* Facilitate sprint planning, reviews, and retrospectives with structured agendas and engagement techniques.\n* Workflow Optimization: Use WebEx recordings, whiteboards, and integrations to streamline feedback and decision-making.\n* Strategic Enablement: Lead Agile transformation initiatives by aligning WebEx usage with SFIA levels and organizational collaboration goals",
    "hashId": "7c85beb980c324526013907de7e2e497fa080f58ee96c581938af01f939f244f"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Jira (Scrum boards, sprint planning, Product Discovery)",
    "L1": "* Navigate Jira Scrum boards, create and update issues, and understand basic sprint workflows.\n* Ceremony Participation: Join sprint planning, daily stand-ups, and retrospectives using Jira boards.\n* Backlog Awareness: View and understand product backlog items and sprint goals",
    "L2": "* Use Jira to estimate story points, prioritize backlog items, and commit to sprint goals.\n* Product Discovery Support: Collaborate with product owners to refine backlog items and link epics to business outcomes.\n* Agile Metrics: Track sprint progress using burndown charts, velocity reports, and cumulative flow diagrams",
    "L3": "* Lead sprint planning sessions using Jira\u2019s advanced features like backlog refinement, dependency mapping, and capacity planning.\n* Cross-functional Enablement: Align Jira workflows with Agile ceremonies across distributed teams, integrating with tools like Confluence and Slack.\n* Continuous Improvement: Use Jira analytics and retrospective insights to optimize team performance and delivery cadence",
    "hashId": "fb694215c575b743e83abc08b8c7042ee4381b7c2bba1f76e325cf990e996be5"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Azure Baords",
    "L1": "* Navigate Azure Boards, understand work item types (e.g., user stories, tasks), and view sprint backlogs.\n* Basic Participation: Join sprint planning and stand-ups using Azure Boards to track assigned work.\n* Sprint Setup: Understand iteration paths and how sprints are defined and assigned",
    "L2": "* Use Azure Boards to estimate effort, assign tasks, and commit to sprint goals.\n* Team Coordination: Collaborate across teams using shared boards, rollups, and delivery plans.\n* Agile Metrics: Track progress using burndown charts, velocity, and forecast tools ",
    "L3": "* Lead sprint planning using advanced features like portfolio backlogs, analytics, and release trains.\n* Cross-functional Enablement: Configure multiple teams and hierarchies to support scaled Agile delivery.\n* Continuous Improvement: Use delivery plans and retrospective insights to optimize team performance and cadence",
    "hashId": "e6f508f2f3f310e0d4fe99fd9f13a3de44547da561b7ae1ab8742daa5b93b600"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "GitHub Actions, GitLab CI/CD",
    "L1": "* Understand basic CI/CD concepts and how GitHub Actions or GitLab CI/CD support Agile workflows.\n* Trigger simple pipelines for build, test, and deploy during sprint cycles.\n* Agile Participation:\n  * Use pipelines to support sprint goals (e.g., automated testing before sprint review).\n  * Collaborate with team members by linking commits and pipeline status to sprint tasks ",
    "L2": "* Configure multi-stage pipelines aligned with Agile ceremonies (e.g., deploy to staging before sprint review).\n* Use GitHub Actions/GitLab CI/CD to automate feedback loops and reduce manual effort.\n* Cross-functional Collaboration:\n   * Integrate pipelines with Jira, Slack, or Confluence to enhance visibility and coordination.\n   * Support sprint retrospectives with pipeline performance data (e.g., build failures, test coverage)",
    "L3": "* Design CI/CD workflows that align with Agile principles\u2014continuous delivery, iterative feedback, and sprint goals.\n* Use pipeline analytics to inform sprint planning and retrospectives (e.g., deployment frequency, lead time).\n* Strategic Leadership:\n   * Lead Agile transformation by embedding GitHub Actions/GitLab CI/CD into team rituals.\n   * Align CI/CD practices with SFIA levels and organizational delivery goals",
    "hashId": "4e2b2c07e347105fa2ff74687594ad69889526840f90b6278b09889b7efd4dcb"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Google Calendar",
    "L1": "* Use Google Calendar to schedule Agile ceremonies like sprint planning, stand-ups, reviews, and retrospectives.\n* Basic Coordination: Set recurring events, invite team members, and add basic agenda notes.\n* Time Management: Understand time-boxing for ceremonies and ensure timely participation ",
    "L2": "* Coordinate across time zones and teams using shared calendars and availability insights.\n* Integration: Link Google Calendar with tools like Google Meet, Jira, and Confluence to streamline Agile workflows.\n* Agenda Planning: Embed detailed agendas, links to boards, and documents to support ceremony facilitation ",
    "L3": "* Design calendar workflows that align Agile ceremonies with sprint goals and delivery timelines.\n* Automation: Use Google Workspace automation (e.g., reminders, follow-ups, task assignments) to support Agile rituals.\n* Leadership & Governance: Lead Agile planning across distributed teams, ensuring calendar-driven visibility and accountability ",
    "hashId": "6fee874e6727bdd1b5d186e761da0f03908738c4292dcf16ccabf372e06027d1"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Agile ceremonies and sprint planning",
    "Tools": "Outlook Calendar",
    "L1": "* Use Outlook Calendar to schedule Agile ceremonies like sprint planning, stand-ups, reviews, and retrospectives.\n* Basic Coordination: Set recurring events, invite participants, and add basic agendas or notes.\n* Time Management: Understand time-boxing and ensure timely participation in Agile meetings",
    "L2": "* Coordinate across teams and time zones using shared calendars and availability insights.\n* Integration: Link Outlook Calendar with Microsoft Teams, Jira, and OneNote to streamline Agile workflows.\n* Agenda Planning: Embed detailed agendas, links to boards, and documents to support ceremony facilitation",
    "L3": "* Design calendar workflows that align Agile ceremonies with sprint goals and delivery timelines.\n* Automation: Use Microsoft Power Automate with Outlook Calendar to manage reminders, follow-ups, and task assignments.\n* Leadership & Governance: Lead Agile planning across distributed teams, ensuring calendar-driven visibility and accountability",
    "hashId": "ed0750cd92941448faca9ab6f778c00c21c4c47a64bfde5b2e8a79cbf1b9acf8"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "Java / Groovy (Jenkins, Gradle, Maven, Jenkins Pipelines)",
    "L1": "* Understands basic CI/CD concepts.\n* Can run and modify simple Jenkins jobs.\n* Familiar with Gradle/Maven for basic builds.\n* Able to use version control with Jenkins.\n* Writes basic Groovy scripts and reads pipeline logs.",
    "L2": "* Builds custom Jenkins Pipelines with multiple stages.\n* Configures Gradle/Maven for builds and dependencies.\n* Automates testing and deployment processes.\n* Sets up build environments and uses Docker.\n* Integrates security and quality checks into pipelines.",
    "L3": "* Designs scalable, reusable pipeline architectures.\n* Masters Jenkins DSL and custom plugin development.\n* Implements advanced deployment strategies (e.g., Blue/Green).\n* Integrates pipelines with cloud platforms and IaC tools.\n* Leads CI/CD governance, monitoring, and mentoring.",
    "hashId": "d1780a29e7769a49ebb262bef10a49bed1f4097fb6b61d4e59962a1d081ff435"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "GitHub Actions, GitLab CI/CD",
    "L1": "* Understands basic CI/CD principles and workflows.\n* Can configure simple pipelines using GitHub Actions or GitLab CI/CD templates.\n* Familiar with YAML syntax for defining jobs and stages.\n* Able to trigger builds on code commits and monitor job status.\n* Uses predefined actions or runners without customization.",
    "L2": "* Develops custom workflows with multiple jobs and dependencies.\n* Integrates CI/CD with version control, testing, and deployment tools.\n* Implements environment-specific configurations and secrets management.\n* Uses caching, artifacts, and conditional steps to optimize pipelines.\n* Troubleshoots pipeline failures and improves reliability.",
    "L3": "* Designs scalable, reusable CI/CD architectures across multiple projects.\n* Implements dynamic workflows with matrix builds, reusable components, and advanced triggers.\n* Integrates security scans, compliance checks, and infrastructure provisioning (e.g., Terraform).\n* Manages self-hosted runners and optimizes resource usage.\n* Leads CI/CD strategy, governance, and cross-functional collaboration for DevOps maturity.",
    "hashId": "d8be9107b990ef1c5b70cd02eb11fc86f5a7a74df6d9191913abb14dc8c7ac5b"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "Python (Ansible, Fabric, Invoke, Pytest (for test automation)",
    "L1": "* Understands basic CI/CD concepts and Python scripting.\n* Can write simple automation scripts using Fabric or Invoke.\n* Uses Pytest for basic unit testing and test execution.\n* Familiar with Ansible playbooks for basic configuration tasks.\n* Able to run and monitor simple CI/CD workflows.",
    "L2": "* Develops modular Python automation scripts for deployment and orchestration.\n* Uses Ansible roles and inventories for multi-environment setups.\n* Implements Pytest fixtures, mocks, and parametrized tests.\n* Integrates Python scripts into CI/CD pipelines (e.g., GitHub Actions, GitLab CI).\n* Manages secrets, environment variables, and error handling in automation flows.",
    "L3": "* Designs scalable automation frameworks using Fabric/Invoke with custom CLI tools.\n* Implements Ansible Tower/AWX for enterprise-grade orchestration.\n* Develops advanced Pytest strategies including test coverage, parallel execution, and reporting.\n* Integrates Python-based CI/CD workflows with cloud platforms and container orchestration (e.g., Kubernetes).\n* Leads CI/CD strategy, promotes best practices, and mentors teams on Python automation.",
    "hashId": "6872cc47d0926b7b5894c2f51cd94bd5024b5e4f164b8724fe16d89795d366db"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "Docker, Kubernetes",
    "L1": "* Understands containerization basics and the role of Docker in CI/CD.\n* Can build and run simple Docker containers.\n* Familiar with basic Kubernetes concepts (pods, deployments, services).\n* Uses Docker in local development and testing environments.\n* Can deploy basic applications using Kubernetes manifests.",
    "L2": "* Creates multi-stage Dockerfiles and optimizes container builds.\n* Manages container orchestration using Kubernetes (e.g., Helm charts, namespaces).\n* Integrates Docker/Kubernetes into CI/CD pipelines for automated deployments.\n* Implements health checks, resource limits, and rolling updates.\n* Uses Kubernetes for staging and production environments with monitoring tools.",
    "L3": "* Creates multi-stage Dockerfiles and optimizes container builds.\n* Manages container orchestration using Kubernetes (e.g., Helm charts, namespaces).\n* Integrates Docker/Kubernetes into CI/CD pipelines for automated deployments.\n* Implements health checks, resource limits, and rolling updates.\n* Uses Kubernetes for staging and production environments with monitoring tools.",
    "hashId": "4a151178d4c632fe43c2cb9577969194fbb826800cfc1cae606122d85f935dc4"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "AWS CloudFormation",
    "L1": "* Understands basic concepts of Infrastructure as Code (IaC) and AWS CloudFormation.\n* Can deploy simple stacks using predefined templates.\n* Familiar with YAML/JSON syntax used in CloudFormation templates.\n* Uses AWS Console or CLI to launch and monitor CloudFormation stacks.\n* Understands basic AWS services like EC2, S3, IAM, and how they are defined in templates.",
    "L2": "* Writes custom CloudFormation templates for multi-resource deployments.\n* Implements parameterization, mappings, and conditions for reusable templates.\n* Uses nested stacks and stack sets for modular infrastructure management.\n* Integrates CloudFormation into CI/CD pipelines for automated provisioning.\n* Manages change sets and rollback strategies during deployments.\n",
    "L3": "* Designs scalable, secure, and compliant infrastructure using CloudFormation.\n* Implements drift detection, stack policies, and advanced lifecycle hooks.\n* Integrates CloudFormation with AWS CodePipeline, CodeBuild, and other DevOps tools.\n* Uses macros and transforms (e.g., AWS::Serverless) for complex deployments.\n* Leads IaC governance, template standardization, and cross-team enablement.",
    "hashId": "10a8faf89a7ad40ba95d54b4c9cd1be4c39b035258df94d8d4cf38d2b148f295"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "Azure Bicep",
    "L1": "* Understands basic Infrastructure as Code (IaC) principles and Azure resource provisioning.\n* Can write simple Bicep templates to deploy single Azure resources.\n* Familiar with Azure CLI or Azure Portal for deploying Bicep files.\n* Uses parameters and variables in basic templates.\n* Able to validate and preview deployments using az deployment.",
    "L2": "* Develops modular Bicep templates using modules and reusable components.\n* Implements conditional logic, loops, and advanced parameterization.\n* Integrates Bicep deployments into CI/CD pipelines (e.g., Azure DevOps, GitHub Actions).\n* Manages secrets and secure parameters using Azure Key Vault.\n* Uses template specs and deployment scopes (resource group, subscription).",
    "L3": "* Designs scalable, secure, and compliant infrastructure using Bicep across environments.\n* Implements full-stack deployments with dependencies and orchestration.\n* Integrates Bicep with automated testing, policy enforcement, and governance tools.\n* Leads IaC strategy, template standardization, and cross-team enablement.\n* Uses Bicep in multi-cloud or hybrid scenarios with advanced DevOps workflows.\n",
    "hashId": "47fddfdd975a1e4863b85fd3221972c349006d5992610989b325129eaf37b267"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "Terraform",
    "L1": "* Understands basic Infrastructure as Code (IaC) principles and * Terraform\u2019s role in provisioning cloud infrastructure.\n* Can write simple Terraform configurations to deploy single resources.\n* Familiar with Terraform CLI commands (init, plan, apply, destroy).\n* Uses Terraform with local state files and basic variables.\n* Able to deploy infrastructure manually using Terraform scripts.",
    "L2": "* Develops modular Terraform code using reusable modules and workspaces.\n* Implements remote state management using backends like S3 or Azure Blob.\n* Uses data sources, outputs, and complex variable types.\n* Integrates Terraform into CI/CD pipelines for automated provisioning.\n* Applies security best practices (e.g., secrets management, IAM roles).",
    "L3": "* Designs scalable, multi-environment infrastructure using Terraform across cloud platforms.\n* Implements policy enforcement using tools like Sentinel or Open Policy Agent (OPA).\n* Manages infrastructure lifecycle with advanced orchestration and drift detection.\n* Integrates Terraform with cloud-native CI/CD tools (e.g., AWS CodePipeline, Azure DevOps).\n* Leads IaC strategy, governance, and cross-team enablement for infrastructure automation.",
    "hashId": "c50800ae583affa6c0594305c8513ab514d595796bcc968f92419b2eb0b36e25"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "GCP Cloud Build",
    "L1": "* Understands basic CI/CD concepts and GCP services.\n* Can create simple Cloud Build configurations using cloudbuild.yaml.\n* Familiar with deploying basic applications to GCP services like Cloud Run.\n* Uses Cloud Console or CLI to trigger builds and monitor status.\n* Able to integrate source repositories (e.g., GitHub) with Cloud Build.",
    "L2": "* Develops multi-step build pipelines using Cloud Build triggers.\n* Uses Artifact Registry for storing and managing container images.\n* Implements automated testing and vulnerability scanning in pipelines.\n* Deploys applications to GKE (Google Kubernetes Engine) using Cloud Build.\n* Manages secrets and environment variables securely in CI/CD workflows.",
    "L3": "* Designs scalable, secure CI/CD architectures using Cloud Build and Cloud Deploy.\n* Implements GitOps workflows with automated rollbacks and approvals.\n* Integrates Cloud Build with Terraform, Helm, and other IaC tools.\n* Manages complex release strategies across multiple environments.\n* Leads CI/CD governance, performance optimization, and cross-team enablement.",
    "hashId": "9665cd7cba55146933ead3da12de55ecadba1943fc5b81c9076bf754709d2534"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "AWS CodePipeline",
    "L1": "* Understands basic CI/CD concepts and AWS CodePipeline architecture.\n* Can create simple pipelines using the AWS Console with source, build, and deploy stages.\n* Familiar with integrating AWS services like CodeCommit, CodeBuild, and CodeDeploy.\n* Able to monitor pipeline executions and troubleshoot basic errors.\nUses default configurations and templates for pipeline setup.",
    "L2": "* Builds custom pipelines using YAML or CloudFormation templates.\n* Integrates third-party tools (e.g., GitHub, Jenkins) with AWS CodePipeline.\n* Implements approval actions, notifications, and environment-specific deployments.\n* Manages IAM roles and permissions for secure pipeline execution.\n* Uses CodePipeline with automated testing and rollback strategies.\n",
    "L3": "* Designs scalable, multi-environment CI/CD architectures using CodePipeline.\n* Implements dynamic pipeline triggers and cross-account deployments.\n* Integrates CodePipeline with Terraform, container orchestration (ECS/EKS), and serverless workflows.\n* Applies governance, auditing, and compliance controls across pipelines.\n* Leads DevOps strategy, pipeline standardization, and team enablement.",
    "hashId": "4ae82939846210c6467f42ab160f9f6dfa6f194fefb1807cbca6d27635943ccb"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "Azure DevOps Pipelines",
    "L1": "* Understands CI/CD fundamentals and Azure DevOps architecture.\n* Can create basic pipelines using the visual designer or YAML templates.\n* Familiar with integrating source control (e.g., Azure Repos, GitHub).\n* Able to configure simple build and release stages.\n* Uses predefined tasks for build, test, and deployment.",
    "L2": "* Develops custom multi-stage YAML pipelines with conditional logic.\n* Implements environment-specific deployments and approvals.\n* Integrates testing frameworks and security scans into pipelines.\n* Manages pipeline variables, secrets, and service connections.\n* Uses templates and reusable components for pipeline standardization.",
    "L3": "* Designs scalable, secure CI/CD architectures across multiple projects and environments.\n* Implements dynamic triggers, rollback strategies, and parallel execution.\n* Integrates Azure DevOps with external tools (e.g., Terraform, Kubernetes, GitHub Actions).\n* Applies governance, auditing, and compliance controls.\n* Leads DevOps strategy, pipeline optimization, and cross-team enablement.",
    "hashId": "e891030d3d91a68821e88d96a1019a815c5c72c13e95f79dd588f59728d0e322"
  },
  {
    "Category": "Digital Engineering",
    "Sub-Category": "Collaboration & Agile Delivery",
    "Sub-Sub-Category": "Continuous integration and delivery",
    "Tools": "JavaScript / Node.js (npm scripts, Serverless Framework, Mocha, Jest)",
    "L1": "* Understands basic CI/CD concepts and Node.js project structure.\n* Can run and modify simple npm scripts for build and test tasks.\n* Uses Mocha or Jest for basic unit testing.\n* Familiar with Serverless Framework for deploying simple functions.\n* Able to integrate basic CI workflows using GitHub Actions or similar tools.",
    "L2": "* Writes custom npm scripts for automation (e.g., linting, testing, deployment).\n* Implements structured test suites using Mocha/Jest with mocks and coverage reports.\n* Uses Serverless Framework with environment variables, plugins, and multiple services.\n* Integrates Node.js applications into CI/CD pipelines with automated testing and deployment.\n* Manages secrets, error handling, and rollback strategies in CI/CD flows.",
    "L3": "* Designs scalable CI/CD pipelines for microservices and serverless architectures.\n* Implements advanced testing strategies (e.g., snapshot testing, parallel execution).\n* Customizes Serverless Framework for multi-cloud deployments and complex workflows.\n* Integrates Node.js CI/CD with cloud-native tools (e.g., AWS Lambda, Azure Functions).\n* Leads DevOps strategy, pipeline governance, and cross-team enablement for JavaScript-based systems.",
    "hashId": "3359f829e6bad2e1ca75c1a28c67e68f26dc5fd61f3879438dc5de3d57773e2c"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": " HTML / CSS (Bootstrap, Tailwind CSS, Bulma, Sass, LESS)",
    "L1": "* Understands basic HTML and CSS syntax and structure.\n* Can build static web pages with semantic HTML tags and basic styling.\n* Uses frameworks like Bootstrap or Tailwind CSS for layout and responsiveness.\n* Familiar with basic concepts of responsive design and media queries.\n* Can apply simple styles using Sass or LESS variables and nesting.",
    "L2": "* Builds responsive, cross-browser compatible UIs using CSS frameworks.\n* Customizes components in Bootstrap, Tailwind, or Bulma.\n* Uses Sass/LESS features like mixins, functions, and imports for modular styling.\n* Implements accessibility best practices and semantic markup.\n* Integrates UI components with JavaScript for interactivity.\n* Optimizes CSS for performance and maintainability.\n",
    "L3": "* Designs scalable, reusable UI systems using atomic design principles.\n* Creates custom themes and utility classes in Tailwind or Bootstrap.\n* Builds complex layouts using Flexbox, Grid, and advanced media queries.\n* Implements advanced animations and transitions using CSS and preprocessors.\n* Leads UI architecture decisions and enforces design consistency across teams.\n* Integrates UI development into CI/CD workflows and design systems.",
    "hashId": "010ed200a6099d5ad36335d5ac29b2f8a386dece4867129771fa0db08f476236"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "Angular CLI, RxJS, NgRx, Angular Material",
    "L1": "* Understands basic Angular architecture: components, modules, and services.\n* Uses Angular CLI to scaffold projects and generate components.\n* Familiar with RxJS for basic asynchronous operations (e.g., observables).\n* Can apply Angular Material components for UI design.\n* Writes simple reactive forms and handles basic routing.",
    "L2": "* Builds modular applications using Angular CLI and custom configurations.\n* Implements RxJS operators for complex data streams and error handling.\n* Uses NgRx for state management in medium-complexity applications.\n* Customizes Angular Material themes and components.\n* Integrates unit testing using Jasmine/Karma and applies best practices.",
    "L3": "* Designs scalable Angular applications with advanced CLI configurations and lazy loading.\n* Masters RxJS for reactive programming, including subjects, multicasting, and higher-order observables.\n* Implements NgRx with effects, selectors, and entity adapters for large-scale state management.\n* Builds custom Angular Material components and integrates accessibility features.\n* Leads architecture decisions, performance optimization, and team mentoring.",
    "hashId": "206b78ebe0d290cc257ae29191d55a502b6edcce45ffe8b90dc62e6b971ed3c6"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "ReactJS ( Redux, React Router, Next.js, Material UI, Styled Components)",
    "L1": "* Understands React fundamentals: components, props, state, and JSX.\n* Uses React CLI or create-react-app to scaffold basic projects.\n* Can implement routing using React Router for simple navigation.\n* Applies Material UI components for basic UI design.\n* Uses Styled Components for scoped styling in components.\n* Familiar with basic Redux concepts and store setup.",
    "L2": "* Builds modular applications with reusable components and hooks.\n* Implements Redux for state management with actions, reducers, and middleware .\n* Uses React Router for nested routes, dynamic routing, and route guards.\n* Customizes Material UI themes and integrates accessibility features.\n* Applies Styled Components with props-based styling and theming.\n* Uses Next.js for server-side rendering and static site generation.",
    "L3": "* Designs scalable React architectures with code splitting and lazy loading.\n* Implements advanced Redux patterns (e.g., selectors, Redux Toolkit, async flows).\n* Uses Next.js for full-stack capabilities, API routes, and performance optimization.\n* Builds custom Material UI components and integrates with design systems.\n* Applies advanced styling strategies using Styled Components (e.g., global styles, animations).\n* Leads UI performance optimization, testing (Jest, React Testing Library), and CI/CD integration",
    "hashId": "91d0bf35f2ef8f3294710d628f9e8505cc78f5a507bff2971b15d8168a8bc070"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "Cypress",
    "L1": "* Understands basic concepts of end-to-end (E2E) testing and Cypress architecture.\n* Can write simple Cypress tests to validate UI elements and user flows.\n* Uses Cypress Test Runner to execute and debug tests.\n* Familiar with basic assertions and selectors.\n* Able to integrate Cypress into local development workflows.",
    "L2": "* Writes modular and reusable Cypress test suites.\n* Implements custom commands and fixtures for test data management.\n* Uses Cypress for testing complex user interactions and asynchronous behavior.\n* Integrates Cypress into CI/CD pipelines for automated testing.\n* Applies best practices for test organization, performance, and reliability.",
    "L3": "* Designs scalable Cypress testing frameworks for large applications.\n* Implements cross-browser testing and visual regression testing.\n* Integrates Cypress with tools like Cucumber, Allure, or Percy for enhanced reporting.\n* Leads test strategy, coverage analysis, and continuous testing practices.\n* Mentors teams on Cypress usage and drives quality engineering initiatives.",
    "hashId": "5b69bd45d4ba70f31bde3f85fef6356eb5cd24435a78317b2f049866ffbb60d4"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "Playwright",
    "L1": "* Understands the basics of end-to-end testing and browser automation.\n* Can write simple Playwright tests to validate UI elements and user flows.\n* Familiar with running tests across multiple browsers (Chromium, Firefox, WebKit).\n* Uses Playwright Test Runner to execute and debug tests.\n* Able to integrate Playwright with basic CI workflows.",
    "L2": "* Writes modular and reusable test suites using Playwright.\n* Implements advanced selectors, assertions, and test hooks.\n* Uses Playwright for testing asynchronous behavior and dynamic content.\n* Integrates Playwright with CI/CD pipelines and reporting tools.\n* Applies parallel test execution and handles flaky tests effectively.",
    "L3": "* Designs scalable Playwright testing frameworks for large applications.\n* Implements cross-browser and cross-device testing strategies.\n* Integrates Playwright with tools like Cucumber, Allure, or custom dashboards.\n* Leads test strategy, performance optimization, and continuous testing practices.\n* Mentors teams on Playwright usage and drives quality engineering initiatives.",
    "hashId": "86e319d8f251c83955732ee84720cfd907368f2e0e44ccbd3db2dcb633285851"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "Adobe XD",
    "L1": "Understands the basics of UI/UX design and Adobe XD interface.\n* Can create simple wireframes and screen layouts.\n* Uses basic prototyping features to link screens and simulate navigation.\n* Familiar with design elements like typography, color, and spacing.\n* Able to export designs and share prototypes for feedback.",
    "L2": "* Designs responsive layouts and multi-device interfaces.\n* Uses components, design systems, and libraries for consistency.\n* Implements interactive prototypes with transitions and animations.\n* Collaborates using shared documents and version control.\n* Applies accessibility principles and user testing feedback to improve designs.",
    "L3": "* Builds scalable design systems and reusable components in Adobe XD.\n* Integrates Adobe XD workflows with development handoff tools (e.g., Zeplin, Avocode).\n* Leads UX strategy, user research, and iterative design processes.\n* Implements advanced prototyping with conditional logic and voice interactions.\n* Mentors teams and drives design governance across projects.",
    "hashId": "05be9b0738427a53a38ce576e6c4ec842b6f868daec691d17384c2ddf172f737"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "Sketch",
    "L1": "* Understands the Sketch interface and basic design tools.\n* Can create simple wireframes, mockups, and UI layouts.\n* Uses artboards, layers, and symbols for basic design organization.\n* Familiar with exporting assets and sharing designs for feedback.\n* Applies basic typography, color, and spacing principles.",
    "L2": "* Builds reusable components and design systems using symbols and shared styles.\n* Uses plugins and integrations to enhance design workflows.\n* Implements responsive design principles across multiple screen sizes.\n* Collaborates with developers using handoff tools (e.g., Zeplin, Abstract).\n* Applies accessibility and usability principles in design.",
    "L3": "* Designs scalable UI systems with nested symbols and advanced overrides.\n* Leads design documentation and version control using Sketch libraries.\n* Integrates Sketch into cross-functional workflows with prototyping and testing tools.\n* Customizes plugins and automates repetitive tasks for efficiency.\n* Mentors team members and drives design consistency across products.",
    "hashId": "5899a5198c0c9203ebaec6e9a06ccd718c0b686ad13fce074f9bd9b8069259c0"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": "ReactJs",
    "L1": "* Understands component-based architecture and JSX syntax.\n* Can build simple functional components and manage local state.\n* Uses create-react-app for project setup.\n* Applies basic routing with React Router.",
    "L2": "* Builds modular applications using hooks (useState, useEffect, etc.).\n* Implements Redux for state management.\n* Uses React Router for nested and dynamic routes.\n* Applies Styled Components or Material UI for custom styling.\n* Integrates testing tools like Jest and React Testing Library.\n",
    "L3": "* Designs scalable architectures with code splitting and SSR using Next.js.\n* Masters Redux Toolkit and advanced async flows.\n* Builds custom Material UI components and design systems.\n* Leads performance optimization and CI/CD integration.",
    "hashId": "fa200bbcc24752adab8829563017790d891d83317e31124982d19c40a5471663"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "UI development",
    "Tools": " Angular ",
    "L1": "* Understands Angular CLI and basic project structure.\n* Can create components, services, and modules.\n* Uses Angular Material for UI components.\n* Familiar with basic data binding and routing.",
    "L2": "* Implements reactive forms and RxJS observables.\n* Uses NgRx for state management.\n* Customizes Angular Material themes and components.\n* Applies route guards and lazy loading.\n* Integrates unit testing with Jasmine and Karma.",
    "L3": "* Implements advanced RxJS patterns and NgRx effects/selectors.\n* Designs enterprise-grade Angular applications with dynamic module loading.\n* Builds custom Angular Material components and accessibility features.\n* Leads architecture decisions, performance tuning, and team mentoring.",
    "hashId": "2b6cc9f6b94010a9c5ea74b9ec38cac80880298f6e28abe3be8d1df5dcc846c3"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Figma",
    "L1": "* Understands Figma\u2019s interface and basic tools (frames, shapes, layers).\n* Can create simple wireframes and mockups.\n* Uses basic components and styles for consistency.\n* Shares designs for feedback and collaboration.\n* Familiar with basic design principles like alignment, spacing, and typography",
    "L2": "* Builds responsive layouts using auto layout and constraints.\n* Creates reusable components and design systems.\n* Implements interactive prototypes with transitions and animations.\n* Collaborates with developers using handoff features (e.g., inspect mode).\n* Applies accessibility and usability principles in design",
    "L3": "* Designs scalable, multi-platform UI systems with nested components and variants.\n* Integrates Figma with plugins and external tools for enhanced workflows.\n* Leads design strategy, documentation, and governance across teams.\n* Conducts user research and testing to inform design decisions.\n* Mentors team members and drives innovation in UI/UX design ",
    "hashId": "46c8e4de667a72110e9e838a476a7c2274f44728eb52b8e83c107dd6880d8fbf"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Adobe XD",
    "L1": "* Understands Adobe XD interface and basic tools (artboards, shapes, layers).\n* Can create simple wireframes and mockups.\n* Uses basic components and UI kits.\n* Applies basic visual design principles (typography, color, spacing).\n* Shares prototypes and receives feedback using built-in collaboration tools",
    "L2": "* Builds responsive layouts and interactive prototypes with transitions and animations.\n* Uses design systems, components, and libraries for consistency.\n* Applies user-centered design principles and accessibility standards.\n* Collaborates with developers using design handoff features.\n* Conducts basic user testing and iterates based on feedback ",
    "L3": "* Designs scalable, multi-device UI systems with nested components and variants.\n* Integrates Adobe XD with external tools (e.g., Zeplin, Slack, Notion).\n* Leads design strategy, documentation, and governance across teams.\n* Conducts in-depth user research and testing to inform design decisions.\n* Mentors team members and drives innovation in UI/UX design workflows",
    "hashId": "1689a790c806a435bf480d4dc9b6bc971774591a06397ff8c2dc35e0109cad94"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Sketch",
    "L1": "* Understands Sketch\u2019s interface and basic tools (artboards, layers, shapes).\n* Can create simple wireframes and mockups.\n* Uses basic symbols and styles for consistency.\n* Familiar with exporting assets and sharing designs.\n* Applies basic design principles like alignment, spacing, and typography",
    "L2": "* Builds responsive layouts using constraints and resizing rules.\n* Creates reusable components and shared libraries.\n* Implements interactive prototypes and design systems.\n* Collaborates with developers using handoff tools (e.g., Zeplin).\n* Applies accessibility and usability standards in design",
    "L3": "* Designs scalable, multi-platform UI systems with nested symbols and overrides.\n* Integrates Sketch with plugins and external tools for enhanced workflows.\n* Leads design strategy, documentation, and governance across teams.\n* Conducts user research and testing to inform design decisions.\n* Mentors team members and drives innovation in UI/UX design",
    "hashId": "5a239f07c653dd84697d57e19469c7f681ad5c3004b7a8fd5980a9cd1718b2bf"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Fluent UI",
    "L1": "* Understands Fluent UI\u2019s purpose and basic design principles.\n* Can use pre-built Fluent UI components (e.g., buttons, inputs) in simple layouts.\n* Familiar with Fluent UI documentation and basic customization options.\n* Applies basic styling using Fluent UI themes and tokens.\n* Integrates Fluent UI into basic React projects.",
    "L2": "* Builds responsive and accessible interfaces using Fluent UI components.\n* Customizes Fluent UI themes and tokens for brand alignment.\n* Uses Fluent UI\u2019s styling system (e.g., makeStyles, mergeStyles) for component-level control.\n* Implements Fluent UI in enterprise-grade applications with consistent design patterns.\n* Collaborates with developers and designers to maintain design consistency.",
    "L3": "* Designs scalable design systems using Fluent UI across multiple platforms.\n* Creates custom Fluent UI components and extends existing ones.\n* Leads integration of Fluent UI with accessibility standards and performance optimization.\n* Implements Fluent UI in complex applications with dynamic theming and localization.\n* Mentors teams on Fluent UI best practices and contributes to design governance.",
    "hashId": "cb560c68eec4eb05e062826df1f87f0a4318b1ee8ef27e3e85a38d3f3c99c996"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Canva",
    "L1": "* Understands Canva\u2019s interface and basic design tools (drag-and-drop, templates, elements).\n* Can create simple designs like posters, social media graphics, and presentations.\n* Applies basic design principles such as layout, color, and typography.\n* Uses pre-built templates and assets for quick design creation.\n* Shares and exports designs in various formats ",
    "L2": "* Customizes templates and builds original designs using grids, frames, and layers.\n* Uses Canva Pro features like brand kits, content planner, and background remover.\n* Applies consistent branding across multiple assets.\n* Creates interactive presentations and animated visuals.\n* Collaborates with teams using shared folders and comments",
    "L3": "* Designs professional-level assets including logos, pitch decks, and marketing kits.\n* Uses advanced tools like Magic Resize, AI-powered design suggestions, and whiteboards.\n* Builds scalable design systems and templates for team-wide use.\n* Integrates Canva with external platforms (e.g., social media, websites).\n* Leads design strategy and trains others on Canva best practices ",
    "hashId": "298f472a6b3d32f41ad41a9a14ad878f9c4c359f17323bcf7301cffe9d3e457b"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Swift UI",
    "L1": "* Understands SwiftUI basics: views, modifiers, and layout structures.\n* Can build simple user interfaces using built-in components.\n* Uses Xcode for previewing and debugging UI.\n* Applies basic styling and navigation patterns.\n* Familiar with Swift syntax and reactive data binding using @State.",
    "L2": "* Builds modular and reusable components using @Binding, @Environment, and custom views.\n* Implements navigation stacks, tab views, and dynamic lists.\n* Integrates API calls, local storage, and animations into UI flows.\n* Applies accessibility features and responsive design principles.\n* Uses Combine or async/await for reactive data handling",
    "L3": "* Designs scalable UI architectures using MVVM and advanced state management.\n* Implements complex features like push notifications, offline data, and dynamic theming.\n* Integrates SwiftUI with backend services (e.g., Firebase, GraphQL) and native modules.\n* Uses Xcode Cloud for CI/CD and automated testing.\n* Leads UI/UX strategy, performance optimization, and team mentoring",
    "hashId": "2a2b9aa9c7127e9cdb1c4b78ec276bf9bc46d090877b4dbade48dcba91674846"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Bootstrap",
    "L1": "* Understands basic HTML and CSS structure.\n* Can use Bootstrap\u2019s grid system and pre-built components (e.g., buttons, forms, navbars).\n* Applies basic styling and layout using Bootstrap classes.\n* Familiar with responsive design principles using Bootstrap utilities.\n* Able to build simple static pages with consistent design.",
    "L2": "* Customizes Bootstrap themes using Sass variables and mixins.\n* Builds responsive, multi-device layouts using advanced grid and flex utilities.\n* Integrates Bootstrap with JavaScript plugins (e.g., modals, carousels).\n* Uses Bootstrap in conjunction with design tools (e.g., Figma, Adobe XD) for prototyping.\n* Applies accessibility standards and semantic HTML practices.",
    "L3": "* Designs scalable UI systems using Bootstrap with custom components and overrides.\n* Integrates Bootstrap into full-stack frameworks (e.g., React, Angular) for dynamic applications.\n* Leads UI consistency across projects using Bootstrap design tokens and utility-first strategies.\n* Implements performance optimization and cross-browser compatibility.\n* Mentors teams on Bootstrap best practices and contributes to design system governance.",
    "hashId": "5b3fe83502ec735cc97defaf10392f9d3de7dc18029cd76a81c12895cf7c885b"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "Sass",
    "L1": "* Understands the basics of CSS and how Sass extends it.\n* Can use variables, nesting, and partials to organize styles.\n* Familiar with compiling Sass to CSS using tools like the Sass CLI or build tools.\n* Applies basic styling to components using Sass syntax.\n* Uses simple mixins and imports for modularity.",
    "L2": "* Builds scalable stylesheets using advanced features like mixins, functions, and inheritance.\n* Organizes styles using a structured architecture (e.g., 7-1 pattern).\n* Implements responsive design and theming using Sass maps and conditionals.\n* Integrates Sass into frontend frameworks (e.g., React, Angular).\n* Applies performance optimization techniques like minification and modular imports.",
    "L3": "* Designs and maintains large-scale design systems using Sass.\n* Creates reusable utility libraries and custom functions for styling logic.\n* Leads integration of Sass with CI/CD pipelines and design tokens.\n* Implements dynamic theming and cross-platform styling strategies.\n* Mentors teams on Sass best practices and contributes to style governance.",
    "hashId": "6397fd51c7819c0601b0873c71c29ae83200c40bbb8a6f05c3e1a31a8c3166d4"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": "Design ",
    "Tools": "JetPack Compose",
    "L1": "* Understands the basics of Jetpack Compose: Composable functions, layout elements (Column, Row, Box), and modifiers.\n* Can build simple static UIs using Material Design components.\n* Familiar with basic state management using remember and mutableStateOf.\n* Uses Android Studio and previews to test UI changes.\n* Able to create basic navigation flows and dialogs",
    "L2": "* Builds dynamic and responsive UIs using advanced layout techniques (ConstraintLayout, LazyColumn).\n* Implements animations, gestures, and transitions.\n* Uses ViewModel, LiveData, and Navigation Compose for state and navigation management.\n* Integrates third-party libraries and APIs (e.g., Retrofit, Hilt).\n* Applies accessibility and theming best practices",
    "L3": "* Designs scalable UI architectures using MVVM or MVI patterns.\n* Implements complex UI features like paginated lists, shimmer effects, and custom components.\n* Integrates Jetpack Compose with backend services and cloud APIs.\n* Leads performance optimization, testing strategies, and CI/CD integration.\n* Mentors teams and contributes to design system governance using Compose ",
    "hashId": "71694da55ce07e073f7cdf72c05ed2ddcca3012244e8d5160815559906e05c3b"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "C# / .NET (ASP.NET Core, Blazor, Razor Pages, Windows Forms, WPF (Windows Presentation Foundation)",
    "L1": "* Understands basic C# syntax and object-oriented programming concepts.\n* Can create simple UI applications using:\n* Windows Forms (basic controls like buttons, textboxes).\n* WPF (basic layout panels and controls).\n* Familiar with Visual Studio and basic debugging.\n* Can build simple web pages using Razor Pages.\n* Understands the structure of an ASP.NET Core project.\n* Can perform basic data binding in WPF and Razor Pages.\n* Knows how to run and deploy a basic Blazor Server app.",
    "L2": "* Builds moderately complex UI applications with:\n  * WPF (MVVM pattern, data templates, styles).\n  * Windows Forms (custom controls, event handling).\n* Develops web apps using:\n  * ASP.NET Core MVC (controllers, views, routing).\n  * Blazor Server/WebAssembly (components, lifecycle methods).\n* Implements form validation and user input handling.\n* Uses Entity Framework Core for data access in UI apps.\n* Applies responsive design principles in Razor Pages and Blazor.\n* Integrates third-party UI libraries (e.g., Telerik, Syncfusion).\n* Understands state management in Blazor apps.",
    "L3": "* Designs scalable and maintainable UI architectures using:\n  * MVVM in WPF with advanced bindings and commands.\n  * Component-based architecture in Blazor.\n* Creates custom controls and reusable components.\n* Optimizes performance and memory usage in UI applications.\n* Implements advanced routing, authorization, and authentication in ASP.NET Core and Blazor.\n* Integrates REST APIs and SignalR for real-time communication.\n* Applies advanced styling and theming techniques.\n* Conducts unit and UI testing using tools like xUnit, MSTest, and Selenium.\n* Uses dependency injection and service lifetimes effectively in UI projects.",
    "hashId": "b36acd61ae7bd4b9db818a10a7f670b8bcf096e1924ab8401249b764b188a82c"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "Seleium",
    "L1": "* Understands basic C# programming and .NET project structure.\n* Can create simple UI applications using:\n  * Windows Forms or WPF with basic controls.\n  * ASP.NET Core with Razor Pages or Blazor.\n* Knows basic HTML/CSS for UI layout.\n* Understands what Selenium is and its role in UI automation.\n* Can write simple Selenium scripts to open a browser and navigate to a webpage.\n* Uses basic locators (ID, Name, Class) to interact with UI elements.",
    "L2": "* Builds moderately complex UI apps using:\n   * WPF with MVVM pattern.\n   * Blazor with component-based architecture.\n   * ASP.NET Core MVC with routing and views.\n* Implements form validation, user input handling, and responsive design.\n* Uses Selenium WebDriver to automate UI testing:\n* Handles dynamic elements and waits.\n* Performs form submissions, dropdown selections, and file uploads.\n* Uses Page Object Model (POM) for test structure.\n* Integrates Selenium tests into CI/CD pipelines.\n* Understands cross-browser testing and basic test reporting.",
    "L3": "* Designs scalable UI systems with reusable components and custom controls.\n* Implements advanced UI features like real-time updates (SignalR), animations, and accessibility.\n* Uses Selenium Grid for distributed test execution.\n* Writes robust test suites with:\n   * Advanced locators (XPath, CSS selectors).\n   * Exception handling and retry logic.\n   * Parallel test execution.\n* Integrates Selenium with test frameworks like xUnit, NUnit, or SpecFlow.\n* Performs performance testing and UI profiling.\n* Applies best practices for UI automation in enterprise-level applications.",
    "hashId": "f60bb79ed85f160037dfde98ec5f6df1ba69e266c37d9f74fd9ad342af730398"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "CSS",
    "L1": "* Understands basic C# and .NET project setup for UI development.\n* Can build simple UI layouts using:\n   * ASP.NET Core Razor Pages or Blazor.\n   * Windows Forms or WPF with basic controls.\n* Applies basic CSS styles:\n   * Colors, fonts, margins, padding.\n   * Inline and internal styles.\n* Understands the role of CSS in web UI design.\n* Can link external CSS files in ASP.NET Core projects.",
    "L2": "* Builds responsive and interactive UI using:\n   * Blazor components with scoped CSS.\n   * ASP.NET Core MVC with layout pages and partial views.\n* Uses CSS classes and selectors effectively.\n* Applies layout techniques:\n   * Flexbox and Grid.\n   * Media queries for responsiveness.\n* Integrates CSS frameworks (e.g., Bootstrap, Tailwind) into .NET projects.\n* Styles form elements, navigation bars, and modals.\n* Understands CSS specificity and inheritance.",
    "L3": "* Designs scalable UI systems with modular CSS architecture (e.g., BEM, SMACSS).\n* Implements advanced animations and transitions using CSS.\n* Uses CSS variables and custom properties for theming.\n* Optimizes CSS for performance and maintainability.\n* Applies advanced responsive design techniques.\n* Integrates CSS with Blazor WebAssembly for dynamic styling.\n* Troubleshoots complex styling issues across browsers and devices.\n* Collaborates with designers using design systems and style guides.",
    "hashId": "9dbb8e76c5633a020dff0044f510f44b218550c67630de9f36956a6791cc0ed6"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "MVVM Pattern",
    "L1": "* Understands basic C# and .NET UI frameworks (especially WPF).\n* Knows the purpose of MVVM (Model-View-ViewModel) pattern.\n* Can separate UI (View) from logic (ViewModel) using basic data binding.\n* Implements simple ViewModel classes with properties and INotifyPropertyChanged.\n* Uses basic commands (e.g., RelayCommand) for button actions.\n* Can bind UI controls to ViewModel properties in XAML.",
    "L2": "* Applies MVVM pattern consistently across WPF or Blazor projects.\n* Uses advanced data binding techniques (e.g., two-way binding, converters).\n* Implements command binding with parameter passing.\n* Structures projects with clear separation of concerns (Model, ViewModel, View).\n* Uses frameworks like MVVM Light, Prism, or CommunityToolkit.MVVM.\n* Handles navigation and messaging between ViewModels.\n* Applies dependency injection for ViewModel instantiation.",
    "L3": "* Designs scalable MVVM architectures for enterprise applications.\n* Implements complex UI interactions using behaviors and triggers.\n* Uses advanced features like:\n   * Asynchronous commands\n   * Validation in ViewModels\n   * Dynamic ViewModel creation\n* Integrates MVVM with services (e.g., REST APIs, SignalR).\n* Applies unit testing and mocking for ViewModels.\n* Optimizes performance with virtualization and lazy loading.\n* Customizes MVVM frameworks for specific application needs.",
    "hashId": "a0709a448cd0546f4a363766ce616c7a4b5e850f32401252ddfd0db27ce7be09"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "XAML",
    "L1": "* Understands what XAML is and its role in .NET UI development (especially WPF and UWP).\n* Can create basic UI layouts using XAML:\n  * Buttons, TextBoxes, Labels, Grids, StackPanels.\n* Uses simple property settings (e.g., Width, Height, Margin, Text).\n* Applies basic data binding to UI elements.\n* Understands the structure of a XAML file and its connection to code-behind.",
    "L2": "* Builds responsive and structured UI using:\n* Layout containers (Grid, DockPanel, WrapPanel).\n* Styles and resources (StaticResource, DynamicResource).\n* Implements data binding with converters and INotifyPropertyChanged.\n* Uses control templates and data templates for customization.\n* Applies triggers and animations for interactive UI.\n* Understands and uses MVVM pattern with XAML effectively.\n* Works with user controls and custom controls in XAML.",
    "L3": "* Designs scalable and maintainable XAML-based UI architectures.\n* Implements advanced styling and theming using resource dictionaries and merged resources.\n* Uses visual states and behaviors for complex UI logic.\n* Optimizes performance with virtualization (e.g., VirtualizingStackPanel).\n* Creates reusable templates and controls with high customization.\n* Integrates XAML with services and APIs for dynamic UI updates.\n* Applies accessibility and localization best practices in XAML.",
    "hashId": "cf26833ddb531085c7b3a517ca304c5ebbec33dfc155fab617daaeec1a1bf5eb"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "ASP.Net MVC",
    "L1": "* Understands the MVC (Model-View-Controller) architecture.\n* Can create a basic ASP.NET MVC project in Visual Studio.\n* Builds simple views using Razor syntax.\n* Creates basic controllers and actions to handle requests.\n* Uses models to pass data between controller and view.\n* Applies basic routing and URL mapping.\n* Can perform simple form submissions and validations.",
    "L2": "* Implements strongly-typed views and partial views.\n* Uses layout pages and view components for reusable UI.\n* Applies client-side and server-side validation using Data Annotations and jQuery.\n* Integrates Entity Framework Core for data access.\n* Handles complex routing scenarios and attribute routing.\n* Uses TempData, ViewBag, and ViewData effectively.\n* Applies role-based authorization and authentication.\n* Implements AJAX calls using jQuery and fetch API.",
    "L3": "* Designs scalable MVC applications with clean separation of concerns.\n* Implements custom model binders and filters (action, result, exception).\n* Uses dependency injection for services and repositories.\n* Applies advanced Razor techniques (tag helpers, custom helpers).\n* Integrates RESTful APIs and third-party services.\n* Optimizes performance with bundling, minification, and caching.\n* Implements unit testing for controllers and services using xUnit/NUnit.\n* Applies security best practices (anti-forgery tokens, HTTPS, input sanitization).",
    "hashId": "ef9da15a809d1b9daf91eb696d130cbb5b537e57fc691c4b565ea3f90d9aa6af"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": ".NET MAUI",
    "L1": "* Understands the purpose of .NET MAUI for cross-platform UI development.\n* Can set up a basic .NET MAUI project in Visual Studio.\n* Creates simple UI layouts using XAML (e.g., StackLayout, Grid, Label, Button).\n* Handles basic user interactions (e.g., button clicks).\n* Understands the structure of a MAUI app (MainPage.xaml, code-behind).\n* Can run apps on Android, iOS, Windows, and macOS simulators/emulators.",
    "L2": "* Applies MVVM pattern in .NET MAUI apps using INotifyPropertyChanged and Command.\n* Uses data binding and converters effectively.\n* Implements navigation between pages (Shell navigation).\n* Integrates local storage and SQLite for data persistence.\n* Applies platform-specific customizations using dependency services.\n* Uses styles, themes, and resource dictionaries for consistent UI.\n* Handles lifecycle events and platform permissions.",
    "L3": "* Designs scalable and modular MAUI applications with clean architecture.\n* Implements advanced UI features:\n  * Custom controls and renderers.\n  * Animations and transitions.\n* Responsive layouts for different screen sizes.\n* Integrates REST APIs and real-time data (e.g., SignalR).\n* Applies dependency injection and service registration.\n* Optimizes performance and memory usage across platforms.\n* Implements unit testing and UI testing using tools like xUnit and Appium.\n* Publishes apps to app stores (Google Play, Apple App Store) with platform-specific configurations.",
    "hashId": "455531950321712aad004870647163bc3526b3e1f9eb0583cabd1a19bc910ff9"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "Blazor",
    "L1": "* Understands what Blazor is and its types: Blazor Server and Blazor WebAssembly.\n* Can set up a basic Blazor project in Visual Studio.\n* Creates simple components using Razor syntax (.razor files).\n* Uses basic HTML and C# code within Blazor components.\n* Handles simple events like button clicks (@onclick).\n* Understands component structure and lifecycle basics.",
    "L2": "* Builds modular applications using reusable components.\n* Implements data binding (@bind) and event callbacks.\n* Uses dependency injection for services.\n* Applies routing and navigation between pages.\n* Integrates form validation using EditForm, DataAnnotationsValidator, and custom validators.\n* Connects to APIs using HttpClient for data fetching.\n* Applies scoped CSS and uses layout components for consistent UI.",
    "L3": "* Designs scalable Blazor applications with clean architecture.\n* Implements state management using cascading parameters, local storage, or third-party libraries.\n* Uses JavaScript interop for advanced UI interactions.\n* Applies authentication and authorization using ASP.NET Identity or external providers.\n* Optimizes performance (lazy loading, virtualization, pre-rendering).\n* Creates custom components and libraries for reuse across projects.\n* Integrates SignalR for real-time communication.\n* Implements unit and integration testing for Blazor components.",
    "hashId": "b1267226459d9b6de56775c6ea38472e75c699f1ec4b245c70c239596fa067b9"
  },
  {
    "Category": "Programming",
    "Sub-Category": "UI",
    "Sub-Sub-Category": ",Net",
    "Tools": "WinForms (Visual Studio Designer, GDI+, Event-driven UI)",
    "L1": "* Understands the basics of Windows Forms and its role in desktop application development.\n* Uses Visual Studio Designer to drag and drop controls (e.g., Button, Label, TextBox).\n* Handles basic events like Click, TextChanged, and Load.\n* Writes simple event handlers in code-behind.\n* Understands the form lifecycle and basic control properties.\n* Can build simple forms with basic layout and input validation.",
    "L2": "* Designs multi-form applications with navigation and data passing.\n* Uses GDI+ for basic custom drawing (e.g., shapes, lines, text).\n* Implements custom controls and user controls.\n* Applies layout management using containers like Panel, TableLayoutPanel, and FlowLayoutPanel.\n* Handles complex event interactions and delegates.\n* Applies error handling and input validation logic.\n* Integrates external libraries or APIs into WinForms applications.",
    "L3": "* Builds scalable and maintainable WinForms applications with layered architecture.\n* Uses GDI+ for advanced graphics rendering (e.g., charts, dynamic drawing, image manipulation).\n* Implements high-performance UI with double buffering and optimized rendering.\n* Applies design patterns (e.g., MVP, MVVM-lite) in WinForms.\n* Customizes control behavior and appearance deeply.\n* Integrates WinForms with backend services and databases.\n* Implements localization, accessibility, and theming.\n* Applies unit testing and UI automation (e.g., using White or WinAppDriver).",
    "hashId": "60c1c7947afe3831767b88917502fbc7f6f42a4ae13b436b243d4c3d9217c7ef"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "Progres OpenEdge",
    "L1": "* Understands the basics of Progress OpenEdge architecture and development environment.\n* Can write simple programs using Progress 4GL/ABL (Advanced Business Language).\n* Familiar with OpenEdge Studio and basic debugging tools.\n* Understands relational database concepts and basic queries.\n* Can perform basic CRUD operations using OpenEdge.\n* Knows how to run and test applications locally.",
    "L2": "* Develops multi-tier applications using OpenEdge AppServer.\n* Applies modular programming and structured error handling.\n* Uses ProDataSets and temp-tables for complex data manipulation.\n* Integrates OpenEdge applications with external systems via REST or SOAP APIs.\n* Implements business logic in middle-tier services.\n* Understands performance tuning and optimization techniques.\n* Applies version control and collaborates in team environments.",
    "L3": "* Designs scalable and maintainable enterprise applications using OpenEdge.\n* Implements service-oriented architecture (SOA) and microservices with OpenEdge.\n* Uses advanced features like dynamic queries, custom error handling frameworks, and multi-threading.\n* Integrates OpenEdge with modern front-end technologies and cloud platforms.\n* Applies security best practices (authentication, authorization, data encryption).\n* Leads modernization efforts including migration to OpenEdge 12+, containerization, and DevOps integration.\n* Mentors junior developers and contributes to architectural decisions.",
    "hashId": "f1cfd4b2bd2c3065bc8f93c91fda66d19056845d19d1db4c3dad3dc6b8ecea32"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "Python (Flask, Django, SQLAlchemy, PyTest)",
    "L1": "* Understands Python syntax and basic programming concepts.\n* Can set up simple web applications using:\n* Flask with basic routing and templates.\n* Django with models, views, and templates.\n* Uses SQLAlchemy for basic ORM operations (CRUD).\n* Writes simple unit tests using PyTest.\n* Understands the role of middle-tier logic in web applications.\n* Can connect to a database and perform basic queries.",
    "L2": "* Builds RESTful APIs using Flask or Django REST Framework.\n* Applies middleware and decorators for request handling.\n* Uses SQLAlchemy for complex queries, relationships, and migrations.\n* Implements authentication and authorization (JWT, OAuth).\n* Applies PyTest for test-driven development (TDD) with fixtures and mocks.\n* Optimizes middle-tier performance with caching and async operations.\n* Integrates third-party services and APIs.",
    "L3": "* Designs scalable and modular middle-tier architectures.\n* Implements advanced ORM features (e.g., polymorphic models, custom types).\n* Uses Celery or Django Channels for background tasks and real-time features.\n* Applies advanced testing strategies:\n   * Integration tests\n   * Coverage analysis\n* Continuous testing in CI/CD pipelines\n* Secures applications with best practices (rate limiting, input sanitization, secure headers).\n* Applies profiling and performance tuning techniques.\n* Leads architectural decisions and mentors junior developers.",
    "hashId": "b2801bff31e7b330f12034c03d0275a3dfb132c31633de3d589c231d7b8ffe74"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "Node.js (JavaScript)",
    "L1": "* Understands the basics of JavaScript and Node.js runtime.\n* Can set up a simple Node.js project using npm and basic modules.\n* Creates basic HTTP servers using Express.js.\n* Handles simple routing and middleware.\n* Connects to a database (e.g., MongoDB or MySQL) using basic drivers.\n* Understands the role of middle-tier logic in web applications.",
    "L2": "* Builds RESTful APIs using Express.js with routing, controllers, and services.\n* Applies middleware for logging, error handling, and authentication.\n* Uses environment variables and configuration management.\n* Integrates ORMs/ODMs like Sequelize or Mongoose for data modeling.\n* Implements JWT-based authentication and role-based access control.\n* Applies asynchronous programming using Promises and async/await.\n* Writes unit and integration tests using Mocha, Chai, or Jest.",
    "L3": "* Designs scalable and modular middle-tier architectures using MVC or Clean Architecture.\n* Implements GraphQL APIs and real-time features using Socket.IO.\n* Applies microservices architecture with inter-service communication (e.g., gRPC, message queues).\n* Uses TypeScript for type safety and maintainability.\n* Applies performance optimization techniques (e.g., caching, clustering, load balancing).\n* Integrates CI/CD pipelines and containerization (Docker, Kubernetes).\n* Implements advanced security practices (rate limiting, input sanitization, OAuth2).\n* Leads architectural decisions and mentors junior developers.",
    "hashId": "6f965af90d1d5eee18016318833de2e7783515c18fefabc3571f39dea3004f6c"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "ASP.NET Core, Entity Framework, NUnit",
    "L1": "* Understands the basics of ASP.NET Core project structure and middleware pipeline.\n* Can create simple controllers and endpoints using MVC or minimal APIs.\n* Uses Entity Framework Core for basic CRUD operations with code-first or database-first approaches.\n* Sets up simple models and DbContext.\n* Writes basic unit tests using NUnit for service or controller logic.\n* Understands dependency injection and basic configuration.",
    "L2": "* Builds RESTful APIs with proper routing, model binding, and validation.\n* Applies Entity Framework Core features:\n   * Relationships (one-to-many, many-to-many)\n   * LINQ queries\n   * Migrations and seeding\n* Implements layered architecture (Controller \u2192 Service \u2192 Repository).\n* Uses NUnit with mocking frameworks (e.g., Moq) for isolated testing.\n* Applies middleware for logging, error handling, and authentication.\n* Integrates third-party services and APIs.\n* Uses configuration management and environment-based settings.",
    "L3": "* Designs scalable and maintainable middle-tier architectures using Clean Architecture or DDD.\n* Optimizes EF Core performance (e.g., tracking behavior, compiled queries, batching).\n* Implements advanced features:\n  * Custom middleware\n  * Background services\n  * CQRS and MediatR\n* Applies unit, integration, and functional testing using NUnit with test coverage tools.\n* Secures APIs with OAuth2, JWT, and role-based access control.\n* Integrates CI/CD pipelines and containerization (Docker, Kubernetes).\n* Leads architectural decisions and mentors teams on best practices.",
    "hashId": "8ec69300760cafc44cad6c4f4cd1575efc462f23322f6f4b1eb0a67376de81cd"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "JSON, XML, YAML, ProtoBuf",
    "L1": "* Understands the purpose of data serialization formats in middle-tier communication.\n* Can read and write basic JSON, XML, and YAML structures.\n* Uses built-in libraries in languages like C#, Python, or JavaScript to parse and generate these formats.\n* Understands the difference between human-readable (JSON, XML, YAML) and binary formats (ProtoBuf).\n* Can consume and produce simple API responses in JSON/XML.",
    "L2": "* Works with nested and complex data structures in JSON, XML, and YAML.\n* Validates data using schemas:\n   * JSON Schema\n   * XML Schema (XSD)\n* Uses ProtoBuf with tools like protoc and integrates it into services for efficient serialization.\n* Converts between formats (e.g., JSON \u2194 XML, YAML \u2194 JSON).\n* Applies serialization and deserialization in middle-tier services (e.g., ASP.NET Core, Flask, Node.js).\n* Handles encoding, escaping, and formatting issues.",
    "L3": "* Designs robust APIs and services using appropriate serialization formats based on performance and interoperability.\n* Implements streaming and chunked data transfer using JSON/XML.\n* Uses ProtoBuf for high-performance microservices and gRPC communication.\n* Applies advanced schema validation and transformation techniques (e.g., XSLT for XML).\n* Secures data formats against injection and parsing vulnerabilities.\n* Optimizes serialization for large datasets and real-time systems.\n* Leads architectural decisions on format selection and integration strategies.",
    "hashId": "6272f5ecb218951182caac038ae31b3d488b7a348b450b3c8ce1b0cae579574e"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "MongoDB",
    "L1": "* Understands the basics of NoSQL databases and MongoDB\u2019s document-oriented model.\n* Can install and set up MongoDB locally or use cloud services like MongoDB Atlas.\n* Performs basic CRUD operations using MongoDB shell or drivers (e.g., for Node.js, Python, C#).\n* Understands BSON format and basic document structure.\n* Uses simple queries and filters to retrieve data.",
    "L2": "* Designs schemas using embedded documents and references.\n* Applies indexes to improve query performance.\n* Uses MongoDB drivers in middle-tier applications (e.g., with Express.js, Flask, ASP.NET Core).\n* Implements aggregation pipelines for data transformation and analytics.\n* Applies validation rules and schema enforcement using MongoDB\u2019s built-in features.\n* Integrates MongoDB with ORMs/ODMs like Mongoose (Node.js) or MongoEngine (Python).\n* Handles connection pooling and error handling in production apps.",
    "L3": "* Designs scalable MongoDB architectures with sharding and replication.\n* Optimizes performance using compound indexes, TTL indexes, and caching strategies.\n* Implements real-time data streaming using Change Streams.\n* Applies role-based access control (RBAC) and encryption for secure data access.\n* Integrates MongoDB into microservices and event-driven architectures.\n* Uses MongoDB Atlas features like triggers, functions, and automated backups.\n* Performs advanced monitoring and profiling using MongoDB Compass and Atlas dashboards.",
    "hashId": "eece813f68671a146f8f0a9a15c07782831a7f35002aeb1146470e12792a523a"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Progress",
    "Tools": "PostgreSQL",
    "L1": "* Understands relational database concepts and PostgreSQL architecture.\n* Can install and configure PostgreSQL locally or on cloud platforms.\n* Performs basic SQL operations:\n  * SELECT, INSERT, UPDATE, DELETE\n  * Simple joins and filters\n* Uses tools like pgAdmin or command-line interface for database management.\n* Connects PostgreSQL to middle-tier applications (e.g., Python, Node.js, ASP.NET Core).",
    "L2": "* Designs normalized schemas with primary/foreign keys and constraints.\n* Uses advanced SQL features:\n  * Subqueries, CTEs, window functions\n  * Indexing for performance\n* Applies stored procedures, functions, and triggers.\n* Integrates PostgreSQL with ORMs like SQLAlchemy, Entity Framework, or Sequelize.\n* Implements role-based access control and basic security measures.\n* Performs backup and restore operations.\n* Optimizes queries and monitors performance using EXPLAIN and ANALYZE.\n",
    "L3": "* Designs scalable and high-performance PostgreSQL databases.\n* Implements partitioning, replication, and logical decoding.\n* Applies advanced security practices (SSL, encryption, auditing).\n* Uses PostGIS for geospatial data and full-text search capabilities.\n* Integrates PostgreSQL into microservices and distributed systems.\n* Automates database operations using CI/CD pipelines and migration tools.\n* Leads database architecture decisions and mentors teams on best practices.",
    "hashId": "904ba0ca49644a1ce01d41b6bb7d26457f77530f7fef4ccbf3f91108e41bc6e6"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Middle Tier ",
    "Tools": "Java (Spring Boot, Hibernate, JPA, Maven, JUnit)",
    "L1": "* Understands basic Java syntax and object-oriented programming principles.\n* Can set up a simple Spring Boot application using Maven.\n* Creates basic REST endpoints using @RestController.\n* Uses JPA annotations for simple entity mapping.\n* Performs basic CRUD operations with Hibernate and Spring Data JPA.\n* Writes simple unit tests using JUnit.\n",
    "L2": "* Builds layered architecture (Controller \u2192 Service \u2192 Repository).\n* Applies Spring Boot features like configuration, profiles, and dependency injection.\n* Uses Hibernate for complex relationships (OneToMany, ManyToMany).\n* Implements pagination, sorting, and custom queries using JPA.\n* Manages dependencies and build lifecycle with Maven.\n* Applies JUnit with mocking frameworks (e.g., Mockito) for service and repository testing.\n* Integrates external APIs and handles exceptions gracefully.\n",
    "L3": "* Designs scalable and maintainable middle-tier applications using Spring Boot with microservices architecture.\n* Applies advanced Hibernate features:\n  * Lazy/eager loading\n  * Caching\n  * Custom naming strategies\n* Uses JPA Criteria API and native queries for dynamic data access.\n* Implements CI/CD pipelines and containerization (Docker, Kubernetes).\n* Applies advanced testing strategies:\n  * Integration tests\n  * Test coverage analysis\n  * Continuous testing in pipelines\n* Secures APIs with OAuth2, JWT, and Spring Security.\n* Leads architectural decisions and mentors teams on best practices.",
    "hashId": "cf2a77d855dfcf4de1301d4b1fa5d14491ed76816d159ea673d14cd4899d8a55"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Middle Tier ",
    "Tools": "C# (.NET) ASP.NET Core, Entity Framework, LINQ, xUnit",
    "L1": "* Understands the basics of C# and the structure of an ASP.NET Core project.\n* Can create simple REST APIs using controllers and routing.\n* Uses Entity Framework Core for basic CRUD operations with code-first approach.\n* Writes simple LINQ queries for data filtering and projection.\n* Sets up basic unit tests using xUnit.\n* Understands dependency injection and basic middleware configuration.",
    "L2": "* Builds layered architecture (Controller \u2192 Service \u2192 Repository).\n* Applies Entity Framework Core features:\n   * Relationships (One-to-Many, Many-to-Many)\n   * Migrations and seeding\n   * Query optimization with AsNoTracking, Include, etc.\n* Uses LINQ for complex queries and joins.\n* Implements validation, error handling, and custom middleware.\n* Applies xUnit with mocking frameworks (e.g., Moq) for service and repository testing.\n* Integrates external APIs and handles configuration management.",
    "L3": "* Designs scalable and maintainable middle-tier applications using Clean Architecture or DDD.\n* Implements advanced EF Core features:\n  * Raw SQL queries\n  * Value converters\n  * Query filters\n* Uses LINQ Expressions and dynamic query generation.\n* Applies advanced testing strategies:\n  * Integration tests\n  * Test coverage analysis\n  * Continuous testing in CI/CD pipelines\n* Secures APIs with OAuth2, JWT, and role-based access control.\n* Optimizes performance with caching, batching, and profiling.\n* Leads architectural decisions and mentors teams on best practices.",
    "hashId": "afc6ccce5935336d3a282ad0e3a3ae3d95d76a6fb7417f72f98320d53aeab0c3"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Middle Tier ",
    "Tools": "Python (Django, Flask, SQLAlchemy, Marshmallow, PyTest)",
    "L1": "* Understands Python syntax and basic programming concepts.\n* Can set up simple web applications using:\n* Flask with basic routing and templates.\n* Django with models, views, and templates.\n* Uses SQLAlchemy for basic ORM operations (CRUD).\n* Applies Marshmallow for basic data serialization and validation.\n* Writes simple unit tests using PyTest.\n* Connects to a database and performs basic queries.\n",
    "L2": "* Builds RESTful APIs using Flask or Django REST Framework.\n* Applies middleware and decorators for request handling.\n* Uses SQLAlchemy for complex queries, relationships, and migrations.\n* Implements Marshmallow schemas with nested fields and custom validators.\n* Applies PyTest for test-driven development (TDD) with fixtures and mocks.\n* Optimizes middle-tier performance with caching and async operations.\n* Integrates third-party services and APIs.",
    "L3": "* Designs scalable and modular middle-tier architectures using Flask or Django.\n* Implements advanced ORM features (e.g., polymorphic models, custom types).\n* Uses Celery or Django Channels for background tasks and real-time features.\n* Applies advanced testing strategies:\n  * Integration tests\n  * Coverage analysis\n  * Continuous testing in CI/CD pipelines\n* Secures applications with best practices (rate limiting, input sanitization, secure headers).\n* Applies profiling and performance tuning techniques.\n* Leads architectural decisions and mentors junior developers.",
    "hashId": "799a4fad4c6a95d47e19837643c859a1e208540a68254fdfe5a406d7c570ba59"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Middle Tier ",
    "Tools": "Node.js (JavaScript) (Express.js, NestJS, Sequelize, Mongoose, Jest)",
    "L1": "* Understands the basics of JavaScript and Node.js runtime.\n* Can set up a simple Express.js server with basic routing.\n* Uses Mongoose or Sequelize for basic CRUD operations with MongoDB or SQL databases.\n* Writes simple unit tests using Jest.\n* Understands RESTful API principles and JSON data exchange.\n* Can connect to a database and handle basic erro",
    "L2": "* Builds modular applications using Express.js or NestJS with controllers, services, and middleware.\n* Applies Sequelize for relational data modeling, migrations, and associations.\n* Uses Mongoose for schema design, validation, and population in MongoDB.\n* Implements authentication and authorization (e.g., JWT).\n* Applies Jest with mocking and test coverage tools.\n* Handles environment configuration and error logging.\n* Integrates third-party APIs and services.\n",
    "L3": "* Designs scalable and maintainable middle-tier architectures using NestJS with dependency injection and decorators.\n* Implements advanced ORM/ODM features:\n    * Sequelize hooks, scopes, and raw queries.\n    * Mongoose plugins and virtuals.\n* Applies microservices architecture and inter-service communication (e.g., gRPC, message queues).\n* Uses Jest for integration testing, snapshot testing, and CI/CD integration.\n* Secures APIs with OAuth2, rate limiting, and input sanitization.\n* Optimizes performance with caching, clustering, and load balancing.\n* Leads architectural decisions and mentors teams on best practices.\n",
    "hashId": "effff06c3c9eab09350e222807164f106f3eaf500aef8648296bd1e212c68540"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Messaging",
    "Tools": "Java (Apache Kafka, RabbitMQ (via Spring AMQP), ActiveMQ, JMS)",
    "L1": "* Understands basic messaging concepts: producer, consumer, queue, topic.\n* Can set up and configure messaging brokers like RabbitMQ, ActiveMQ, or Kafka locally.\n* Uses Spring Boot with Spring AMQP to send and receive simple messages via RabbitMQ.\n* Writes basic JMS producers and consumers using Java EE or Spring.\n* Understands message formats (JSON, XML) and basic serialization.",
    "L2": "* Implements asynchronous communication between services using messaging.\n* Uses Kafka for event streaming with multiple partitions and consumer groups.\n* Applies Spring Boot integration with RabbitMQ and ActiveMQ using annotations and configuration.\n* Handles message acknowledgment, retries, and dead-letter queues.\n* Applies JMS features like durable subscriptions and message selectors.\n* Secures messaging channels with authentication and encryption.\n* Monitors message flow and broker health using tools like Kafka Manager or RabbitMQ Management UI.",
    "L3": "* Designs scalable and fault-tolerant messaging architectures using Kafka, RabbitMQ, or ActiveMQ.\n* Implements Kafka Streams or Kafka Connect for real-time data processing and integration.\n* Applies advanced routing, exchange types, and message patterns (e.g., fanout, topic, direct) in RabbitMQ.\n* Uses JMS with advanced features like XA transactions and message-driven beans.\n* Integrates messaging into microservices and event-driven systems.\n* Applies performance tuning, partitioning strategies, and message batching.\n* Leads architectural decisions and mentors teams on messaging best practices.",
    "hashId": "b00b8b309d09dd674e06b2b0406de0b92f893ba923b1bd1d1d7fe7729672eb67"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Messaging",
    "Tools": "C# (.NET) Azure Service Bus, RabbitMQ (EasyNetQ), NServiceBus, MassTransit",
    "L1": "* Understands basic messaging concepts: queues, topics, publishers, subscribers.\n* Can set up and send/receive messages using:\n* Azure Service Bus with basic queue operations.\n* RabbitMQ using EasyNetQ for simplified messaging.\n* Uses basic configuration in .NET for connecting to messaging services.\n* Understands message formats (JSON/XML) and serialization.\n* Can write simple message handlers and consumers.",
    "L2": "* Implements asynchronous communication between services using messaging.\n* Uses MassTransit or NServiceBus for structured message handling and routing.\n* Applies message retry policies, error handling, and dead-letter queues.\n* Uses Azure Service Bus topics and subscriptions with filters.\n* Configures RabbitMQ exchanges and bindings for routing messages.\n* Applies dependency injection and middleware in message consumers.\n* Monitors message flow and broker health using built-in or third-party tools.",
    "L3": "* Designs scalable and fault-tolerant messaging architectures using MassTransit, NServiceBus, or Azure Service Bus.\n* Implements Saga patterns, state machines, and event-driven workflows.\n* Applies advanced routing strategies, message batching, and throttling.\n* Secures messaging with encryption, authentication, and RBAC.\n* Integrates messaging into microservices and distributed systems.\n* Uses MassTransit with RabbitMQ or Azure Service Bus for advanced orchestration.\n* Leads architectural decisions and mentors teams on messaging best practices.",
    "hashId": "022bfd9a659f985eec2e5312a3e610d28aa4e697623aaf83e0004cb7b93a52d5"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Messaging",
    "Tools": "Node.js (JavaScript)",
    "L1": "* Understands basic messaging concepts: producer, consumer, queue, topic.\n* Can set up and use simple messaging with Node.js using libraries like:\n  * amqplib for RabbitMQ\n  * kafkajs for Apache Kafka\n* Sends and receives basic messages using JSON format.\n* Handles simple message events and callbacks.\n* Understands the role of messaging in asynchronous and decoupled systems.",
    "L2": "* Implements messaging in microservices using RabbitMQ, Kafka, or Redis Pub/Sub.\n* Applies message acknowledgment, retries, and dead-letter queues.\n* Uses KafkaJS or node-rdkafka for partitioned and replicated Kafka messaging.\n* Applies routing and exchange types in RabbitMQ (direct, topic, fanout).\n* Handles message serialization and deserialization.\n* Integrates messaging with Express.js or NestJS applications.\n* Implements logging and monitoring for message flow.\n",
    "L3": "* Designs scalable and fault-tolerant messaging architectures using Kafka, RabbitMQ, or NATS.\n* Implements event-driven architecture and CQRS patterns.\n* Applies advanced features:\n  * Kafka consumer groups and offset management\n  * RabbitMQ clustering and high availability\n* Secures messaging with TLS, authentication, and authorization.\n* Uses message brokers in containerized environments (Docker, Kubernetes).\n* Applies performance tuning, batching, and backpressure handling.\n* Leads architectural decisions and mentors teams on messaging best practices.",
    "hashId": "f3d125a2d1578627618ad63bce5a86c00b488e033f73936c010a9390a0b2c9a9"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Messaging",
    "Tools": "Python (Celery (with RabbitMQ/Redis), Kombu, Pika (RabbitMQ), aiokafka)",
    "L1": "* Understands basic messaging concepts: producer, consumer, queue, broker.\n* Can set up Celery with RabbitMQ or Redis for simple task queues.\n* Uses Pika to send and receive basic messages with RabbitMQ.\n* Understands JSON message formats and basic serialization.\n* Can install and configure messaging libraries and brokers locally.",
    "L2": "* Implements asynchronous task processing using Celery with retries, timeouts, and periodic tasks.\n* Uses Kombu for custom messaging workflows and broker abstraction.\n* Applies Pika for advanced RabbitMQ features like exchanges, routing keys, and acknowledgments.\n* Integrates aiokafka for asynchronous Kafka producers and consumers.\n* Applies message serialization/deserialization with JSON, YAML, or custom formats.\n* Monitors task queues and broker health using tools like Flower (for Celery) or RabbitMQ Management UI.",
    "L3": "* Designs scalable and fault-tolerant messaging architectures using Celery, RabbitMQ, and Kafka.\n* Implements chained, grouped, and workflow-based tasks in Celery.\n* Applies advanced Kombu features like custom transports, routing, and compression.\n* Uses aiokafka with consumer groups, offset management, and partitioning strategies.\n* Secures messaging with TLS, authentication, and role-based access control.\n* Integrates messaging into microservices and event-driven systems.\n* Applies performance tuning, backpressure handling, and message batching.\n* Leads architectural decisions and mentors teams on messaging best practices.",
    "hashId": "03021247308d32ac8b81808ab416d24ac7a8649813b1b28752aec2690039d434"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Messaging",
    "Tools": "JSON",
    "L1": "* Understands what JSON (JavaScript Object Notation) is and its role in data exchange.\n* Can read and write basic JSON structures (objects, arrays, key-value pairs).\n* Uses built-in libraries in languages like Python (json), JavaScript (JSON.parse, JSON.stringify), Java (Jackson, Gson), and C# (System.Text.Json) to serialize and deserialize data.\n* Sends and receives JSON payloads in simple REST APIs.\n* Understands basic formatting rules and data types in JSON.\n",
    "L2": "* Works with nested and complex JSON structures.\n* Validates JSON using JSON Schema.\n* Applies JSON in messaging systems (e.g., RabbitMQ, Kafka) for structured communication.\n* Converts between JSON and other formats (e.g., XML, YAML).\n* Handles encoding, escaping, and formatting issues.\n* Uses JSON for configuration files and structured logging.\n* Applies JSON in asynchronous messaging workflows and microservices.",
    "L3": "* Designs robust messaging systems using JSON for event-driven architectures.\n* Implements streaming JSON and chunked data transfer for large payloads.\n* Applies advanced schema validation and transformation techniques.\n* Secures JSON data against injection and parsing vulnerabilities.\n* Optimizes JSON serialization for performance and bandwidth efficiency.\n* Integrates JSON with message brokers and protocols (e.g., gRPC with JSON fallback).\n* Leads architectural decisions on format selection and messaging strategies.",
    "hashId": "a1b4e55cfadad412897f4f6c1de1bc12c81796ee2384ddd70444e6c586332981"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Messaging",
    "Tools": "Sneakers (RabbitMQ)",
    "L1": "* Understands basic messaging concepts: producer, consumer, queue, broker.\n* Can install and configure RabbitMQ locally or via Docker.\n* Sets up Sneakers (a Python task queue library) to send and receive simple messages.\n* Uses JSON for message payloads and basic serialization.\n* Understands the role of Sneakers as a lightweight alternative to Celery.",
    "L2": "* Implements asynchronous task processing using Sneakers with RabbitMQ.\n* Applies message acknowledgment, retries, and error handling.\n* Configures multiple queues and routing keys for task segregation.\n* Integrates Sneakers into Flask or Django applications for background processing.\n* Monitors RabbitMQ queues and message flow using the management UI.\n* Applies structured logging and exception tracking for message consumers.",
    "L3": "* Designs scalable and fault-tolerant messaging systems using Sneakers and RabbitMQ.\n* Implements advanced routing strategies and exchange types (direct, topic, fanout).\n* Secures messaging with TLS, authentication, and access control.\n* Applies performance tuning, batching, and backpressure handling.\n* Integrates Sneakers into microservices and event-driven architectures.\n* Leads architectural decisions on messaging strategy and mentors teams on best practices.",
    "hashId": "fcdfd5ff4b79ecfe41100d5d033d2ccaab867202024dca074bf90f768467b08d"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Scheduling",
    "Tools": "Java (Quartz Scheduler, Spring Scheduler (@Scheduled), Cron4j)",
    "L1": "* Understands basic scheduling concepts: jobs, triggers, cron expressions.\n* Can use Spring\u2019s @Scheduled annotation for simple periodic tasks.\n* Writes basic cron expressions for fixed-rate and fixed-delay scheduling.\n* Sets up Quartz Scheduler with simple job and trigger definitions.\n* Uses Cron4j for lightweight scheduling in standalone Java apps.",
    "L2": "* Implements dynamic scheduling using Quartz with job stores and calendars.\n* Uses Spring Scheduler with externalized cron expressions and conditional execution.\n* Applies Cron4j for multiple task scheduling and error handling.\n* Integrates scheduling with business logic and external services (e.g., email, reports).\n* Handles exceptions, retries, and logging in scheduled tasks.\n* Manages thread pools and concurrency in scheduled jobs.",
    "L3": "* Designs scalable and distributed scheduling systems using Quartz clustering.\n* Implements advanced scheduling features:\n  * Job chaining\n  * Misfire instructions\n  * Custom triggers\n* Applies Spring TaskExecutor and Async for concurrent scheduling.\n* Secures scheduled tasks with role-based access and audit logging.\n* Integrates scheduling into microservices and cloud-native environments.\n* Monitors and manages scheduled jobs using dashboards or admin tools.\n* Leads architectural decisions and mentors teams on scheduling best practices.",
    "hashId": "16b6ad313feb601dfe5c438f7f5abde5a7760be072105bac1658e572b368e90e"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Scheduling",
    "Tools": "C# (.NET) (Hangfire, Quartz.NET, FluentScheduler)",
    "L1": "* Understands basic scheduling concepts: jobs, triggers, cron expressions.\n* Can set up Hangfire for background job processing with simple fire-and-forget jobs.\n* Uses Quartz.NET for basic job scheduling with simple triggers.\n* Implements basic recurring tasks using FluentScheduler.\n* Understands how to configure job intervals and delays.\n* Can monitor Hangfire jobs using the built-in dashboard.",
    "L2": "* Implements recurring, delayed, and continuation jobs using Hangfire.\n* Uses Quartz.NET with cron expressions and job stores (RAM or database).\n* Applies FluentScheduler for grouped and conditional job execution.\n* Handles job retries, error handling, and logging.\n* Integrates scheduled jobs with ASP.NET Core services and dependency injection.\n* Applies scheduling for business logic (e.g., report generation, notifications).\n* Manages concurrency and thread safety in scheduled tasks.",
    "L3": "* Designs scalable and distributed scheduling systems using Hangfire with SQL Server or Redis, or Quartz.NET clustering.\n* Implements advanced job orchestration:\n  * Chained jobs\n  * Batch jobs\n  * Parallel execution\n* Applies custom triggers and calendars in Quartz.NET.\n* Secures scheduled jobs with role-based access and audit logging.\n* Integrates scheduling into microservices and cloud-native environments (e.g., Azure Functions, Kubernetes).\n* Monitors and manages jobs using dashboards, metrics, and alerts.\n* Leads architectural decisions and mentors teams on scheduling best practices.",
    "hashId": "b9432f99d5c43ce9f4f55309520bd5ba0def98546e372b1380f7c7b09a9ebaa0"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Scheduling",
    "Tools": "Python (Celery (with periodic tasks), APScheduler, schedule)",
    "L1": "* Understands basic scheduling concepts: tasks, intervals, cron expressions.\n* Can set up Celery with RabbitMQ or Redis for asynchronous task execution.\n* Uses Celery beat for simple periodic tasks.\n* Implements basic job scheduling using APScheduler and schedule library.\n* Understands time-based triggers and simple cron syntax.",
    "L2": "* Configures Celery beat with custom schedules and periodic task definitions.\n* Uses APScheduler with different trigger types (interval, date, cron).\n* Applies schedule for lightweight job scheduling in standalone scripts.\n* Handles retries, error logging, and task chaining in Celery.\n* Integrates scheduled tasks with Flask or Django applications.\n* Manages concurrency and thread safety in scheduled jobs.",
    "L3": "* Designs scalable and distributed scheduling systems using Celery with multiple workers and queues.\n* Implements advanced scheduling workflows:\n  * Chained and grouped tasks\n  * Dynamic scheduling\n  * Task expiration and revocation\n* Applies APScheduler with persistent job stores (e.g., SQLAlchemy, Redis).\n* Secures scheduled tasks with access control and audit logging.\n* Integrates scheduling into microservices and cloud-native environments (e.g., Docker, Kubernetes).\n* Monitors and manages scheduled jobs using tools like Flower or custom dashboards.\n* Leads architectural decisions and mentors teams on scheduling best practices.",
    "hashId": "4b9a0031297b513fe83bf46507147271e365c0c6cb37fc074084da64da556fbe"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Scheduling",
    "Tools": "Elixir",
    "L1": "* Basic Understanding of Elixir: Familiar with syntax, basic data types, and simple pattern matching.\n* Scheduling Concepts: Understands what scheduling means in a concurrent system; can use basic scheduling functions like Process.send_after/3.\n* Middle Tier Awareness: Can work with basic business logic and understands the role of middle tier in a web application.\n* Tool Usage: Uses IDEs for coding and basic debugging; relies on documentation for APIs.\n* Testing: May write basic unit tests but relies on manual testing.\n* Error Handling: Handles only the \u201chappy path\u201d; limited understanding of exceptions or process crashes",
    "L2": "* Concurrency & Scheduling: Understands Elixir\u2019s concurrency model (BEAM VM, lightweight processes); uses GenServer, Task, and Process modules effectively.\n* Middle Tier Logic: Can design and implement moderately complex business logic; integrates with databases and external APIs.\n* Code Organization: Applies modular design principles; organizes code across files and folders logically.\n* Testing & Automation: Writes automated unit tests; familiar with TDD and uses tools like ExUnit.\n* Error Handling: Implements basic fault tolerance using try/rescue, Supervisor trees.\n* Performance Awareness: Begins to consider performance implications of scheduling and process spawning.",
    "L3": "* Advanced Scheduling: Designs custom scheduling mechanisms using GenStage, Flow, or Oban for job processing.\n* Middle Tier Architecture: Architects scalable and maintainable middle tier systems; applies Domain-Driven Design (DDD) and service boundaries.\n* Concurrency Mastery: Deep understanding of BEAM internals, message passing, and process lifecycle; optimizes for throughput and fault tolerance.\n* Testing Strategy: Implements full test suites including integration, performance, and fault injection tests.\n* Error Resilience: Designs robust supervision trees; uses Registry, DynamicSupervisor, and custom recovery strategies.\n* Tooling & Frameworks: Contributes to or builds frameworks; writes macros and metaprogramming constructs in Elixir.",
    "hashId": "755a97aacbd6ca89c680613ad3d499c731645e9f0332d3de827f2c7ceec9679d"
  },
  {
    "Category": "Programming",
    "Sub-Category": "Middle Tier",
    "Sub-Sub-Category": "Scheduling",
    "Tools": "Node.js (JavaScript) node-cron, Agenda, Bree.js, Bull ",
    "L1": "* Understands basic JavaScript and asynchronous programming.\n* Uses simple scheduling tools like node-cron for time-based tasks.\n* Can set up basic recurring jobs (e.g., daily reminders, log cleanup).\n* Relies on tutorials and documentation for setup.\n* Minimal error handling (basic try/catch).\n* Manual or minimal testing of scheduled tasks.",
    "L2": "* Familiar with job queue concepts and event loop behavior.\n* Uses Agenda or Bree.js for more flexible and persistent scheduling.\n* Configures job priorities, delays, and recurring patterns.\n* Implements basic retry logic and failure notifications.\n* Writes unit tests for job handlers and uses logging for monitoring.\n* Handles moderate complexity tasks like email campaigns or report generation.",
    "L3": "* Designs scalable job scheduling systems using Bull with Redis.\n* Implements distributed job queues and concurrency control.\n* Uses advanced features like job lifecycle events, backoff strategies, and dead-letter queues.\n* Integrates dashboards (e.g., Bull Board) for job monitoring.\n* Writes comprehensive test suites including integration and performance tests.\n* Suitable for high-scale systems (e.g., payment processing, video encoding, real-time alerts).",
    "hashId": "6dc4587c8d9037d941721388df46ebf7a9b7cee46fa6ce1be11fcaa6ad8f3509"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "Progress DB",
    "Tools": "Progress  DB",
    "L1": "* Understands basic SQL syntax: SELECT, WHERE, GROUP BY, ORDER BY.\n* Can write simple queries to retrieve and filter data.\n* Familiar with basic joins (INNER JOIN, LEFT JOIN) to combine tables.\n* Uses Progress DB tools to explore tables and run queries.\n* Limited understanding of indexing, performance, or query optimization.\n* Relies on documentation or templates for query writing.",
    "L2": "* conditional logic (CASE, HAVING) and pattern matching (LIKE, IN) in queries.\n* Understands and applies JOIN types effectively across multiple tables.\n* Familiar with Progress DB-specific features like buffer management and transaction control.\n* Can write moderately complex queries involving subqueries and unions (UNION, UNION ALL).\n* Begins to optimize queries for performance using LIMIT, indexing, and query plans.\n* Applies basic data modeling concepts and normalization.",
    "L3": "* Designs and manages complex schemas in Progress DB.\n* Uses advanced SQL features like WITH (CTE), window functions (OVER, PARTITION BY, LAG, LEAD).\n* Implements stored procedures, triggers, and custom functions.\n* Optimizes database performance through indexing strategies, query tuning, and caching.\n* Manages data integrity and concurrency using transactions and locking mechanisms.\n* Performs ETL operations and integrates Progress DB with other systems and APIs.",
    "hashId": "5d39dee6fa7233706f3ac9ab42d7b53c94742add7a467d046d15e6b14e90a68a"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "Progress DB",
    "Tools": "Progress ABL",
    "L1": "* Understands basic Progress ABL syntax and structure (e.g., FOR EACH, FIND, DISPLAY, ASSIGN).\n* Can write simple queries and procedural code to retrieve and display data.\n* Familiar with basic database concepts like tables, fields, and indexes.\n* Uses Progress Developer Studio or similar IDEs for basic development tasks.\n* Relies on documentation and examples to build simple programs.\n* Limited understanding of transactions, error handling, or performance tuning.",
    "L2": "* Writes modular ABL code using procedures and functions.\n* Understands and applies concepts like buffers, temp-tables, and dynamic queries.\n* Implements basic business logic and validations within Progress DB applications.\n* Uses transactions (DO TRANSACTION) and error handling (ON ERROR UNDO, THROW) effectively.\n* Familiar with database normalization and schema design.\n* Can debug and optimize queries for better performance.\n* Integrates Progress ABL with external systems using REST/SOAP APIs or file I/O.",
    "L3": "* Designs and maintains complex Progress DB applications with layered architecture.\n* Uses advanced ABL features like dynamic objects, persistent procedures, and super procedures.\n* Implements robust error handling, logging, and auditing mechanisms.\n* Optimizes performance using indexing strategies, query tuning, and memory management.\n* Develops reusable frameworks and libraries in ABL.\n* Integrates Progress DB with enterprise systems and services (e.g., ERP, CRM).\n* Leads code reviews, mentors junior developers, and contributes to architectural decisions.",
    "hashId": "a4b7a12680afe153ff534de15130a884fc4418b44667f84913c3547d9b851fe1"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "Progress DB",
    "Tools": "OpenEdge Database",
    "L1": "* Understands basic database concepts: tables, fields, records, and indexes.\n* Can perform simple data operations using Progress ABL (FIND, FOR EACH, DISPLAY, ASSIGN).\n* Uses Progress Developer Studio or similar tools for basic development.\n* Reads and writes basic queries to retrieve and update data.\n* Limited knowledge of transactions, error handling, and performance tuning.\n* Relies on documentation and examples for development.",
    "L2": "* Designs normalized schemas and understands relationships between tables.\n* Uses temp-tables, buffers, and dynamic queries in ABL.\n* Implements basic business logic and validations within the database.\n* Applies structured error handling (ON ERROR UNDO, THROW) and uses transactions (DO TRANSACTION).\n* Begins optimizing queries using indexes and query plans.\n* Integrates OpenEdge DB with external systems via REST/SOAP APIs.",
    "L3": "* Architects and maintains complex OpenEdge DB applications.\n* Uses advanced ABL features: persistent procedures, super procedures, dynamic objects.\n* Manages multi-tenant databases, replication, and disaster recovery.\n* Implements performance tuning strategies: indexing, caching, memory management.\n* Applies security features like Transparent Data Encryption and role-based access.\n* Develops reusable frameworks and leads code reviews and architectural decisions.",
    "hashId": "7558d0f8e187dafc3c2fbbac115b7298286b5aa1797a6fc22688036a89a598ff"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "Progress DB",
    "Tools": "OpenEdge AppServer",
    "L1": "* Understands the basic purpose of the OpenEdge AppServer and its role in application architecture.\n* Can deploy and run simple applications using the Progress Application Server for OpenEdge (PAS for OpenEdge).\n* Familiar with basic configuration tasks such as setting up instances and managing services.\n* Uses Progress Developer Studio to connect applications to AppServer.\n* Relies on guided tutorials and documentation for setup and troubleshooting.",
    "L2": "* Configures application-specific settings for AppServer instances.\n* Implements basic load balancing and fault tolerance mechanisms.\n* Understands session models (state-reset, stateless, state-aware) and their implications.\n* Manages AppServer security settings including authentication and authorization.\n* Monitors AppServer performance using built-in tools and logs.\n* Integrates AppServer with business logic and database layers using ABL and datasets.",
    "L3": "* Designs and administers scalable, secure, and high-performance AppServer environments.\n* Implements advanced features like multi-tenancy, replication, and disaster recovery.\n* Optimizes AppServer performance through tuning, caching, and resource management.\n* Develops and deploys enterprise-grade applications using object-oriented ABL and OERA principles.\n* Implements AppServer monitoring, alerting, and automated recovery strategies.\n* Leads architecture decisions and mentors teams on AppServer best practices.",
    "hashId": "245b3fe760c76e1f27b6c64d5ac0552f67a592fe6c0632c740619f4d8b285d16"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "Progress DB",
    "Tools": "OpenEdge Management",
    "L1": "* Understands the purpose of OpenEdge Management in monitoring and administering OpenEdge databases.\n* Can navigate the OpenEdge Management interface to view basic system metrics.\n* Learns to set up simple monitoring plans and log file monitors.\n* Familiar with basic database administration tasks like starting/stopping databases and viewing logs.\n* Relies on guided tutorials and documentation for setup and usage.\n",
    "L2": "* Configures and customizes monitoring plans for specific database activities.\n* Uses OpenEdge Management to track performance metrics, resource usage, and alerts.\n* Understands database sizing and migration concepts (e.g., binary dump/load).\n* Detects and resolves common issues like index corruption and table inconsistencies.\n* Begins automating routine monitoring and maintenance tasks.\n* Integrates OpenEdge Management with broader system monitoring tools.",
    "L3": "* Designs and implements comprehensive monitoring strategies using OpenEdge Management.\n* Diagnoses and resolves complex database corruption scenarios.\n* Optimizes database performance through proactive monitoring and tuning.\n* Implements automated alerting, logging, and recovery workflows.\n* Manages multi-tenant environments and disaster recovery using replication and partitioning.\n* Leads database health audits and contributes to enterprise-level data governance.",
    "hashId": "b2f0b591618fef318a3d8689ad03f2b727ac76b4d1d774d0cdf95f15d4e78034"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "SQL",
    "Tools": "SQL ",
    "L1": "* Understands basic SQL syntax: SELECT, WHERE, ORDER BY, GROUP BY.\n* Can retrieve and filter data from single tables.\n* Uses simple joins (INNER JOIN, LEFT JOIN) to combine data.\n* Applies basic aggregate functions: COUNT, SUM, AVG.\n* Performs simple data manipulations using INSERT, UPDATE, DELETE.\n* Relies on query builders or guided tools for writing SQL.",
    "L2": "* Writes complex queries using subqueries, CASE, and HAVING.\n* Understands relational schema design and normalization.\n* Uses stored procedures, views, and triggers.\n* Applies indexing and query optimization techniques.\n* Performs data transformations and joins across multiple tables.\n* Uses EXPLAIN plans to analyze query performance.",
    "L3": "* Designs and manages large-scale relational databases.\n* Uses advanced SQL features: window functions (RANK, ROW_NUMBER, PARTITION BY), CTEs (WITH), recursive queries.\n* Implements robust transaction control and error handling.\n* Optimizes performance through indexing strategies, partitioning, and caching.\n* Integrates SQL with BI tools, ETL pipelines, and APIs.\n* Leads database architecture decisions and enforces data governance policies.",
    "hashId": "e393b051cac01af7bed4b6e00ff12f416a762d0055a917395ae92ea9a4669641"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "SQL",
    "Tools": "PostgreSQL",
    "L1": "* Understands basic SQL syntax: SELECT, WHERE, ORDER BY, GROUP BY.\n* Can retrieve and filter data from single tables.\n* Uses simple joins (INNER JOIN, LEFT JOIN) to combine data.\n* Familiar with PostgreSQL data types and basic constraints (e.g., PRIMARY KEY, NOT NULL).\n* Uses tools like psql or pgAdmin for basic query execution.\n* Performs basic data manipulation: INSERT, UPDATE, DELETE.",
    "L2": "* Writes complex queries using subqueries, CASE, HAVING, and UNION.\n* Designs normalized schemas and understands relational integrity.\n* Creates and manages views, functions, and triggers.\n* Applies indexing and uses EXPLAIN to analyze query performance.\n* Implements stored procedures using PL/pgSQL.\n* Manages roles and permissions for secure access control.\n",
    "L3": "* Uses advanced SQL features: window functions (RANK, ROW_NUMBER, LAG, LEAD), CTEs (WITH), and recursive queries.\n* Designs and optimizes large-scale PostgreSQL databases for performance and scalability.\n* Implements partitioning, replication, and high availability strategies.\n* Manages complex transactions and concurrency control.\n* Integrates PostgreSQL with ETL pipelines, APIs, and enterprise systems.\n* Leads database architecture decisions and enforces data governance policies.",
    "hashId": "51642112762cc23678af4d8573a1f8adacc80aa7da7193ca771b1eb7c934740c"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "SQL",
    "Tools": "MySQL",
    "L1": "* Uses basic SQL commands: SELECT, INSERT, UPDATE, DELETE.\n* Creates simple tables with basic constraints (PRIMARY KEY, NOT NULL).\n* Performs simple joins (INNER JOIN, LEFT JOIN).\n* Familiar with MySQL Workbench or CLI tools.\n* Relies on tutorials and documentation for query writing.",
    "L2": "* Writes complex queries using subqueries, CASE, UNION.\n* Understands normalization and relational schema design.\n* Creates stored procedures, views, and triggers.\n* Begins using EXPLAIN for query optimization.\n* Integrates MySQL with applications (e.g., PHP, Python).",
    "L3": "* Uses advanced SQL features: window functions, recursive queries.\n* Manages replication, clustering, and backup strategies.\n* Applies indexing, caching, and performance tuning.\n* Implements role-based access control and encryption.\n* Designs scalable, secure MySQL environments for enterprise use.",
    "hashId": "823462c683170eda9b9bb376ef7fa24bc89beab4d2688f75cbda41b23f38f81c"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "SQL",
    "Tools": "Oracle SQL",
    "L1": "* Uses basic SQL commands: SELECT, INSERT, UPDATE, DELETE.\n* Creates tables with constraints like PRIMARY KEY, NOT NULL, UNIQUE.\n* Performs simple joins and filtering operations.\n* Familiar with Oracle SQL Developer or command-line tools.\n* Understands basic relational database concepts and data types.",
    "L2": "* Writes complex queries using subqueries, CASE, HAVING, UNION.\n* Designs normalized schemas and applies indexing.\n* Creates stored procedures, functions, views, and triggers using PL/SQL.\n* Begins using EXPLAIN PLAN for query performance analysis.\n* Manages user roles and access control.",
    "L3": "* Uses advanced SQL features: window functions, CTEs, recursive queries.\n* Develops robust PL/SQL packages, triggers, and exception handling.\n* Optimizes performance with partitioning, indexing, and query tuning.\n* Implements replication, backup strategies, and high availability.\n* Integrates Oracle SQL with enterprise systems and enforces data governance.",
    "hashId": "ae63c2b3c0834feb175f89dc6ecc4a0d6d8cd34dc012b856ef927ffa2ea62e6d"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "SQL",
    "Tools": "C# (.NET) (ADO.NET, Entity Framework, Dapper)",
    "L1": "* Understands basic SQL operations (SELECT, INSERT, UPDATE, DELETE) and how to execute them via C#.\n* Uses ADO.NET for direct database access with SqlConnection, SqlCommand, and DataReader.\n* Familiar with basic concepts of Entity Framework (EF) like DbContext and simple LINQ queries.\n* Can perform basic CRUD operations using EF or Dapper.\n* Relies on tutorials and documentation for setup and syntax.",
    "L2": "* Writes complex SQL queries and integrates them using Dapper for lightweight data access.\n* Uses Entity Framework for relational mapping, navigation properties, and LINQ-to-Entities.\n* Understands transactions, parameterized queries, and connection pooling in ADO.NET.\n* Applies code-first and database-first approaches in EF.\n* Implements basic performance tuning (e.g., avoiding N+1 queries, using compiled queries).\n* Familiar with exception handling and logging in data access layers.\n",
    "L3": "* Designs scalable data access layers using repository and unit of work patterns.\n* Optimizes performance using Dapper for high-throughput scenarios and EF for complex object graphs.\n* Implements asynchronous data access, caching strategies, and connection resiliency.\n* Uses advanced EF features like migrations, shadow properties, and global query filters.\n* Integrates with enterprise systems, APIs, and handles multi-database environments.\n* Leads architectural decisions on ORM usage, data modeling, and database interaction strategies.",
    "hashId": "fd21adbda4fc2a6876cbc8d2b81fb3cded0f2846fd0e5db6b8236aa0c1132530"
  },
  {
    "Category": "Programming",
    "Sub-Category": "DB",
    "Sub-Sub-Category": "SQL",
    "Tools": "Node.js (JavaScript)",
    "L1": "* Uses basic SQL queries (SELECT, INSERT, UPDATE, DELETE) within Node.js applications.\n* Connects to SQL databases using simple drivers like mysql2 or pg.\n* Executes queries using callbacks or promises.\n* Understands basic schema structure and table relationships.\n* Relies on tutorials and ORM tools like Sequelize for basic CRUD operations.",
    "L2": "* Implements parameterized queries to prevent SQL injection.\n* Uses query builders like knex.js for dynamic SQL generation.\n* Applies relational design principles and indexing for performance.\n* Handles transactions and error management using async/await.\n* Integrates SQL queries with RESTful APIs and middleware.\n* Begins using Sequelize or TypeORM for model relationships and migrations.",
    "L3": "* Designs scalable data access layers using repository patterns.\n* Optimizes performance with connection pooling, caching, and query tuning.\n* Uses advanced SQL features (e.g., window functions, CTEs) within Node.js.\n* Implements multi-database support and replication strategies.\n* Integrates SQL with real-time systems (e.g., WebSockets, GraphQL).\n* Leads architectural decisions on database interaction and ORM usage.",
    "hashId": "aabd7d68b9455a4654f893348b2e6fc424f4c044b0fe329d831dc9e64ebc398c"
  }
]